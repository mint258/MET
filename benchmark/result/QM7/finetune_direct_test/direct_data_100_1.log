Using device: cuda
Total samples: 100, Training: 80, Validation: 20
Total trainable parameters: 4577537
Epoch 1/200
Train Loss: 1522.80432098, Train R²: -65.146698, Val Loss: 1530.75994199, Val R²: -38.991032
Saved best model with validation R2 -38.991032 to best_finetuned_model.pth
Epoch 2/200
Train Loss: 1522.45996663, Train R²: -65.116791, Val Loss: 1530.58591722, Val R²: -38.981945
Saved best model with validation R2 -38.981945 to best_finetuned_model.pth
Epoch 3/200
Train Loss: 1522.19077320, Train R²: -65.093414, Val Loss: 1530.40060115, Val R²: -38.972267
Saved best model with validation R2 -38.972267 to best_finetuned_model.pth
Epoch 4/200
Train Loss: 1521.92549095, Train R²: -65.070374, Val Loss: 1530.19124622, Val R²: -38.961330
Saved best model with validation R2 -38.961330 to best_finetuned_model.pth
Epoch 5/200
Train Loss: 1521.63799900, Train R²: -65.045410, Val Loss: 1529.93815234, Val R²: -38.948112
Saved best model with validation R2 -38.948112 to best_finetuned_model.pth
Epoch 6/200
Train Loss: 1521.32532681, Train R²: -65.018280, Val Loss: 1529.62822280, Val R²: -38.931931
Saved best model with validation R2 -38.931931 to best_finetuned_model.pth
Epoch 7/200
Train Loss: 1520.97751463, Train R²: -64.988098, Val Loss: 1529.26346651, Val R²: -38.912880
Saved best model with validation R2 -38.912880 to best_finetuned_model.pth
Epoch 8/200
Train Loss: 1520.58163214, Train R²: -64.953743, Val Loss: 1528.83239762, Val R²: -38.890388
Saved best model with validation R2 -38.890388 to best_finetuned_model.pth
Epoch 9/200
Train Loss: 1520.13583275, Train R²: -64.915085, Val Loss: 1528.31631216, Val R²: -38.863468
Saved best model with validation R2 -38.863468 to best_finetuned_model.pth
Epoch 10/200
Train Loss: 1519.61857056, Train R²: -64.870232, Val Loss: 1527.71463304, Val R²: -38.832081
Saved best model with validation R2 -38.832081 to best_finetuned_model.pth
Epoch 11/200
Train Loss: 1519.03615823, Train R²: -64.819748, Val Loss: 1527.02758652, Val R²: -38.796261
Saved best model with validation R2 -38.796261 to best_finetuned_model.pth
Epoch 12/200
Train Loss: 1518.36184752, Train R²: -64.761322, Val Loss: 1526.25595822, Val R²: -38.756058
Saved best model with validation R2 -38.756058 to best_finetuned_model.pth
Epoch 13/200
Train Loss: 1517.59484053, Train R²: -64.694901, Val Loss: 1525.39855448, Val R²: -38.711399
Saved best model with validation R2 -38.711399 to best_finetuned_model.pth
Epoch 14/200
Train Loss: 1516.74454013, Train R²: -64.621307, Val Loss: 1524.46441415, Val R²: -38.662769
Saved best model with validation R2 -38.662769 to best_finetuned_model.pth
Epoch 15/200
Train Loss: 1515.80120728, Train R²: -64.539703, Val Loss: 1523.46307799, Val R²: -38.610687
Saved best model with validation R2 -38.610687 to best_finetuned_model.pth
Epoch 16/200
Train Loss: 1514.77622110, Train R²: -64.451096, Val Loss: 1522.39523449, Val R²: -38.555172
Saved best model with validation R2 -38.555172 to best_finetuned_model.pth
Epoch 17/200
Train Loss: 1513.66427916, Train R²: -64.355049, Val Loss: 1521.24546014, Val R²: -38.495457
Saved best model with validation R2 -38.495457 to best_finetuned_model.pth
Epoch 18/200
Train Loss: 1512.46305740, Train R²: -64.251350, Val Loss: 1520.00000000, Val R²: -38.430809
Saved best model with validation R2 -38.430809 to best_finetuned_model.pth
Epoch 19/200
Train Loss: 1511.14800731, Train R²: -64.137932, Val Loss: 1518.63771190, Val R²: -38.360165
Saved best model with validation R2 -38.360165 to best_finetuned_model.pth
Epoch 20/200
Train Loss: 1509.70772006, Train R²: -64.013824, Val Loss: 1517.13718892, Val R²: -38.282417
Saved best model with validation R2 -38.282417 to best_finetuned_model.pth
Epoch 21/200
Train Loss: 1508.14148872, Train R²: -63.879005, Val Loss: 1515.47805989, Val R²: -38.196552
Saved best model with validation R2 -38.196552 to best_finetuned_model.pth
Epoch 22/200
Train Loss: 1506.43173592, Train R²: -63.731987, Val Loss: 1513.67260331, Val R²: -38.103210
Saved best model with validation R2 -38.103210 to best_finetuned_model.pth
Epoch 23/200
Train Loss: 1504.54800522, Train R²: -63.570198, Val Loss: 1511.70359198, Val R²: -38.001545
Saved best model with validation R2 -38.001545 to best_finetuned_model.pth
Epoch 24/200
Train Loss: 1502.49174707, Train R²: -63.393822, Val Loss: 1509.55978683, Val R²: -37.891010
Saved best model with validation R2 -37.891010 to best_finetuned_model.pth
Epoch 25/200
Train Loss: 1500.25647807, Train R²: -63.202362, Val Loss: 1507.25619919, Val R²: -37.772400
Saved best model with validation R2 -37.772400 to best_finetuned_model.pth
Epoch 26/200
Train Loss: 1497.83325173, Train R²: -62.995129, Val Loss: 1504.76235665, Val R²: -37.644207
Saved best model with validation R2 -37.644207 to best_finetuned_model.pth
Epoch 27/200
Train Loss: 1495.20249465, Train R²: -62.770527, Val Loss: 1502.06033501, Val R²: -37.505543
Saved best model with validation R2 -37.505543 to best_finetuned_model.pth
Epoch 28/200
Train Loss: 1492.36431544, Train R²: -62.528664, Val Loss: 1499.10765124, Val R²: -37.354309
Saved best model with validation R2 -37.354309 to best_finetuned_model.pth
Epoch 29/200
Train Loss: 1489.29867723, Train R²: -62.267933, Val Loss: 1495.92939339, Val R²: -37.191849
Saved best model with validation R2 -37.191849 to best_finetuned_model.pth
Epoch 30/200
Train Loss: 1485.98270515, Train R²: -61.986496, Val Loss: 1492.53433796, Val R²: -37.018692
Saved best model with validation R2 -37.018692 to best_finetuned_model.pth
Epoch 31/200
Train Loss: 1482.43772213, Train R²: -61.686337, Val Loss: 1488.84065635, Val R²: -36.830750
Saved best model with validation R2 -36.830750 to best_finetuned_model.pth
Epoch 32/200
Train Loss: 1478.62966966, Train R²: -61.364693, Val Loss: 1484.89822884, Val R²: -36.630665
Saved best model with validation R2 -36.630665 to best_finetuned_model.pth
Epoch 33/200
Train Loss: 1474.50691758, Train R²: -61.017406, Val Loss: 1480.65483824, Val R²: -36.415901
Saved best model with validation R2 -36.415901 to best_finetuned_model.pth
Epoch 34/200
Train Loss: 1470.05510101, Train R²: -60.643494, Val Loss: 1476.00940038, Val R²: -36.181484
Saved best model with validation R2 -36.181484 to best_finetuned_model.pth
Epoch 35/200
Train Loss: 1465.33514938, Train R²: -60.248287, Val Loss: 1471.04036314, Val R²: -35.931561
Saved best model with validation R2 -35.931561 to best_finetuned_model.pth
Epoch 36/200
Train Loss: 1460.22282546, Train R²: -59.821659, Val Loss: 1465.79585891, Val R²: -35.668705
Saved best model with validation R2 -35.668705 to best_finetuned_model.pth
Epoch 37/200
Train Loss: 1454.71179276, Train R²: -59.363430, Val Loss: 1460.15470071, Val R²: -35.386993
Saved best model with validation R2 -35.386993 to best_finetuned_model.pth
Epoch 38/200
Train Loss: 1448.85748091, Train R²: -58.878555, Val Loss: 1454.03894377, Val R²: -35.082829
Saved best model with validation R2 -35.082829 to best_finetuned_model.pth
Epoch 39/200
Train Loss: 1442.55606477, Train R²: -58.358837, Val Loss: 1447.54378172, Val R²: -34.761189
Saved best model with validation R2 -34.761189 to best_finetuned_model.pth
Epoch 40/200
Train Loss: 1435.77865634, Train R²: -57.802391, Val Loss: 1440.63666481, Val R²: -34.420727
Saved best model with validation R2 -34.420727 to best_finetuned_model.pth
Epoch 41/200
Train Loss: 1428.56494427, Train R²: -57.213005, Val Loss: 1433.16428751, Val R²: -34.054234
Saved best model with validation R2 -34.054234 to best_finetuned_model.pth
Epoch 42/200
Train Loss: 1420.85546415, Train R²: -56.586391, Val Loss: 1425.16806027, Val R²: -33.664162
Saved best model with validation R2 -33.664162 to best_finetuned_model.pth
Epoch 43/200
Train Loss: 1412.63179031, Train R²: -55.921703, Val Loss: 1416.68517145, Val R²: -33.252731
Saved best model with validation R2 -33.252731 to best_finetuned_model.pth
Epoch 44/200
Train Loss: 1403.77510663, Train R²: -55.210186, Val Loss: 1407.62916814, Val R²: -32.816216
Saved best model with validation R2 -32.816216 to best_finetuned_model.pth
Epoch 45/200
Train Loss: 1394.38365416, Train R²: -54.460594, Val Loss: 1397.96973322, Val R²: -32.353699
Saved best model with validation R2 -32.353699 to best_finetuned_model.pth
Epoch 46/200
Train Loss: 1384.35425379, Train R²: -53.665646, Val Loss: 1387.65472110, Val R²: -31.863316
Saved best model with validation R2 -31.863316 to best_finetuned_model.pth
Epoch 47/200
Train Loss: 1373.68737346, Train R²: -52.826462, Val Loss: 1376.99487109, Val R²: -31.360348
Saved best model with validation R2 -31.360348 to best_finetuned_model.pth
Epoch 48/200
Train Loss: 1362.38268302, Train R²: -51.944183, Val Loss: 1365.44511973, Val R²: -30.819769
Saved best model with validation R2 -30.819769 to best_finetuned_model.pth
Epoch 49/200
Train Loss: 1350.25716069, Train R²: -51.005943, Val Loss: 1352.86653998, Val R²: -30.236212
Saved best model with validation R2 -30.236212 to best_finetuned_model.pth
Epoch 50/200
Train Loss: 1337.46207423, Train R²: -50.024990, Val Loss: 1339.97327399, Val R²: -29.643675
Saved best model with validation R2 -29.643675 to best_finetuned_model.pth
Epoch 51/200
Train Loss: 1324.00068920, Train R²: -49.003040, Val Loss: 1326.08870367, Val R²: -29.011908
Saved best model with validation R2 -29.011908 to best_finetuned_model.pth
Epoch 52/200
Train Loss: 1309.53362882, Train R²: -47.916267, Val Loss: 1311.77394203, Val R²: -28.367464
Saved best model with validation R2 -28.367464 to best_finetuned_model.pth
Epoch 53/200
Train Loss: 1294.41811251, Train R²: -46.793537, Val Loss: 1296.30585704, Val R²: -27.678961
Saved best model with validation R2 -27.678961 to best_finetuned_model.pth
Epoch 54/200
Train Loss: 1278.36868704, Train R²: -45.615704, Val Loss: 1279.97119108, Val R²: -26.960752
Saved best model with validation R2 -26.960752 to best_finetuned_model.pth
Epoch 55/200
Train Loss: 1261.27998478, Train R²: -44.377750, Val Loss: 1263.14325197, Val R²: -26.230379
Saved best model with validation R2 -26.230379 to best_finetuned_model.pth
Epoch 56/200
Train Loss: 1243.55452233, Train R²: -43.111282, Val Loss: 1245.51048771, Val R²: -25.475445
Saved best model with validation R2 -25.475445 to best_finetuned_model.pth
Epoch 57/200
Train Loss: 1224.70380092, Train R²: -41.784069, Val Loss: 1227.13910173, Val R²: -24.700176
Saved best model with validation R2 -24.700176 to best_finetuned_model.pth
Epoch 58/200
Train Loss: 1205.09682184, Train R²: -40.425125, Val Loss: 1206.34427093, Val R²: -23.836538
Saved best model with validation R2 -23.836538 to best_finetuned_model.pth
Epoch 59/200
Train Loss: 1184.36783771, Train R²: -39.012268, Val Loss: 1185.83715155, Val R²: -22.999302
Saved best model with validation R2 -22.999302 to best_finetuned_model.pth
Epoch 60/200
Train Loss: 1162.47646213, Train R²: -37.546799, Val Loss: 1162.41150201, Val R²: -22.060478
Saved best model with validation R2 -22.060478 to best_finetuned_model.pth
Epoch 61/200
Train Loss: 1140.13483852, Train R²: -36.079376, Val Loss: 1139.99424341, Val R²: -21.179605
Saved best model with validation R2 -21.179605 to best_finetuned_model.pth
Epoch 62/200
Train Loss: 1115.87642685, Train R²: -34.518299, Val Loss: 1117.95158661, Val R²: -20.330173
Saved best model with validation R2 -20.330173 to best_finetuned_model.pth
Epoch 63/200
Train Loss: 1091.05284015, Train R²: -32.955612, Val Loss: 1093.70654199, Val R²: -19.415035
Saved best model with validation R2 -19.415035 to best_finetuned_model.pth
Epoch 64/200
Train Loss: 1065.09073557, Train R²: -31.358856, Val Loss: 1069.75213251, Val R²: -18.530565
Saved best model with validation R2 -18.530565 to best_finetuned_model.pth
Epoch 65/200
Train Loss: 1037.74833173, Train R²: -29.718790, Val Loss: 1041.68163323, Val R²: -17.519043
Saved best model with validation R2 -17.519043 to best_finetuned_model.pth
Epoch 66/200
Train Loss: 1009.82003347, Train R²: -28.087605, Val Loss: 1016.28490100, Val R²: -16.627043
Saved best model with validation R2 -16.627043 to best_finetuned_model.pth
Epoch 67/200
Train Loss: 980.31406320, Train R²: -26.412613, Val Loss: 985.38187395, Val R²: -15.571341
Saved best model with validation R2 -15.571341 to best_finetuned_model.pth
Epoch 68/200
Train Loss: 950.67057254, Train R²: -24.779835, Val Loss: 956.15198844, Val R²: -14.602794
Saved best model with validation R2 -14.602794 to best_finetuned_model.pth
Epoch 69/200
Train Loss: 918.75030612, Train R²: -23.077702, Val Loss: 927.55185165, Val R²: -13.683341
Saved best model with validation R2 -13.683341 to best_finetuned_model.pth
Epoch 70/200
Train Loss: 886.50504793, Train R²: -21.417253, Val Loss: 890.83321250, Val R²: -12.543822
Saved best model with validation R2 -12.543822 to best_finetuned_model.pth
Epoch 71/200
Train Loss: 853.54008400, Train R²: -19.781067, Val Loss: 858.95859912, Val R²: -11.591949
Saved best model with validation R2 -11.591949 to best_finetuned_model.pth
Epoch 72/200
Train Loss: 818.43120511, Train R²: -18.106640, Val Loss: 825.14275280, Val R²: -10.620014
Saved best model with validation R2 -10.620014 to best_finetuned_model.pth
Epoch 73/200
Train Loss: 784.04685766, Train R²: -16.534929, Val Loss: 791.21611302, Val R²: -9.684119
Saved best model with validation R2 -9.684119 to best_finetuned_model.pth
Epoch 74/200
Train Loss: 747.95301490, Train R²: -14.957638, Val Loss: 760.62322309, Val R²: -8.873876
Saved best model with validation R2 -8.873876 to best_finetuned_model.pth
Epoch 75/200
Train Loss: 711.27798715, Train R²: -13.431077, Val Loss: 723.15171731, Val R²: -7.924982
Saved best model with validation R2 -7.924982 to best_finetuned_model.pth
Epoch 76/200
Train Loss: 674.57453258, Train R²: -11.980155, Val Loss: 688.36815640, Val R²: -7.087049
Saved best model with validation R2 -7.087049 to best_finetuned_model.pth
Epoch 77/200
Train Loss: 637.01513424, Train R²: -10.574960, Val Loss: 655.32670764, Val R²: -6.329330
Saved best model with validation R2 -6.329330 to best_finetuned_model.pth
Epoch 78/200
Train Loss: 598.92690497, Train R²: -9.232167, Val Loss: 619.76797574, Val R²: -5.555515
Saved best model with validation R2 -5.555515 to best_finetuned_model.pth
Epoch 79/200
Train Loss: 561.53391483, Train R²: -7.994395, Val Loss: 584.24978605, Val R²: -4.825666
Saved best model with validation R2 -4.825666 to best_finetuned_model.pth
Epoch 80/200
Train Loss: 523.56879920, Train R²: -6.819293, Val Loss: 547.92135613, Val R²: -4.123714
Saved best model with validation R2 -4.123714 to best_finetuned_model.pth
Epoch 81/200
Train Loss: 486.25321014, Train R²: -5.744426, Val Loss: 516.99223278, Val R²: -3.561593
Saved best model with validation R2 -3.561593 to best_finetuned_model.pth
Epoch 82/200
Train Loss: 449.94853872, Train R²: -4.774917, Val Loss: 485.13772207, Val R²: -3.016786
Saved best model with validation R2 -3.016786 to best_finetuned_model.pth
Epoch 83/200
Train Loss: 413.80144695, Train R²: -3.884319, Val Loss: 447.24886459, Val R²: -2.413870
Saved best model with validation R2 -2.413870 to best_finetuned_model.pth
Epoch 84/200
Train Loss: 380.19479999, Train R²: -3.123180, Val Loss: 415.49723600, Val R²: -1.946353
Saved best model with validation R2 -1.946353 to best_finetuned_model.pth
Epoch 85/200
Train Loss: 347.58097977, Train R²: -2.446132, Val Loss: 387.53126890, Val R²: -1.563079
Saved best model with validation R2 -1.563079 to best_finetuned_model.pth
Epoch 86/200
Train Loss: 316.52458929, Train R²: -1.857820, Val Loss: 350.45545143, Val R²: -1.096111
Saved best model with validation R2 -1.096111 to best_finetuned_model.pth
Epoch 87/200
Train Loss: 290.04032047, Train R²: -1.399588, Val Loss: 324.91187507, Val R²: -0.801688
Saved best model with validation R2 -0.801688 to best_finetuned_model.pth
Epoch 88/200
Train Loss: 263.69590085, Train R²: -0.983475, Val Loss: 313.92542407, Val R²: -0.681905
Saved best model with validation R2 -0.681905 to best_finetuned_model.pth
Epoch 89/200
Train Loss: 242.18628064, Train R²: -0.673089, Val Loss: 281.91655736, Val R²: -0.356406
Saved best model with validation R2 -0.356406 to best_finetuned_model.pth
Epoch 90/200
Train Loss: 222.35796379, Train R²: -0.410345, Val Loss: 263.68718150, Val R²: -0.186661
Saved best model with validation R2 -0.186661 to best_finetuned_model.pth
Epoch 91/200
Train Loss: 204.74462562, Train R²: -0.195763, Val Loss: 253.06131915, Val R²: -0.092949
Saved best model with validation R2 -0.092949 to best_finetuned_model.pth
Epoch 92/200
Train Loss: 189.07908398, Train R²: -0.019781, Val Loss: 236.94709490, Val R²: 0.041811
Saved best model with validation R2 0.041811 to best_finetuned_model.pth
Epoch 93/200
Train Loss: 176.89108142, Train R²: 0.107451, Val Loss: 206.81807185, Val R²: 0.269996
Saved best model with validation R2 0.269996 to best_finetuned_model.pth
Epoch 94/200
Train Loss: 162.39077700, Train R²: 0.247784, Val Loss: 194.45306913, Val R²: 0.354675
Saved best model with validation R2 0.354675 to best_finetuned_model.pth
Epoch 95/200
Train Loss: 153.16497119, Train R²: 0.330826, Val Loss: 181.33443077, Val R²: 0.438811
Saved best model with validation R2 0.438811 to best_finetuned_model.pth
Epoch 96/200
Train Loss: 145.36947203, Train R²: 0.397209, Val Loss: 175.24180839, Val R²: 0.475888
Saved best model with validation R2 0.475888 to best_finetuned_model.pth
Epoch 97/200
Train Loss: 133.85245638, Train R²: 0.488939, Val Loss: 172.33249907, Val R²: 0.493146
Saved best model with validation R2 0.493146 to best_finetuned_model.pth
Epoch 98/200
Train Loss: 126.46465971, Train R²: 0.543797, Val Loss: 166.41578318, Val R²: 0.527352
Saved best model with validation R2 0.527352 to best_finetuned_model.pth
Epoch 99/200
Train Loss: 116.82722160, Train R²: 0.610679, Val Loss: 158.07682690, Val R²: 0.573533
Saved best model with validation R2 0.573533 to best_finetuned_model.pth
Epoch 100/200
Train Loss: 108.69031228, Train R²: 0.663022, Val Loss: 154.32336561, Val R²: 0.593546
Saved best model with validation R2 0.593546 to best_finetuned_model.pth
Epoch 101/200
Train Loss: 101.31751896, Train R²: 0.707188, Val Loss: 153.37265050, Val R²: 0.598538
Saved best model with validation R2 0.598538 to best_finetuned_model.pth
Epoch 102/200
Train Loss: 93.91204745, Train R²: 0.748428, Val Loss: 153.79074028, Val R²: 0.596346
Epoch 103/200
Train Loss: 88.03388685, Train R²: 0.778935, Val Loss: 157.26712695, Val R²: 0.577891
Epoch 104/200
Train Loss: 83.25092201, Train R²: 0.802304, Val Loss: 161.56065642, Val R²: 0.554529
Epoch 105/200
Train Loss: 79.49348440, Train R²: 0.819747, Val Loss: 162.86351768, Val R²: 0.547315
Epoch 106/200
Train Loss: 74.67230756, Train R²: 0.840948, Val Loss: 160.10193896, Val R²: 0.562537
Epoch 107/200
Train Loss: 70.00496216, Train R²: 0.860210, Val Loss: 154.88209351, Val R²: 0.590597
Epoch 108/200
Train Loss: 68.31822369, Train R²: 0.866865, Val Loss: 153.78962904, Val R²: 0.596352
Epoch 109/200
Train Loss: 66.04190342, Train R²: 0.875589, Val Loss: 157.30888082, Val R²: 0.577667
Epoch 110/200
Train Loss: 63.25456459, Train R²: 0.885869, Val Loss: 159.07687189, Val R²: 0.568120
Epoch 111/200
Train Loss: 61.13433874, Train R²: 0.893392, Val Loss: 156.07633474, Val R²: 0.584259
Epoch 112/200
Train Loss: 58.57734948, Train R²: 0.902123, Val Loss: 157.85029869, Val R²: 0.574755
Epoch 00112: reducing learning rate of group 0 to 5.0000e-05.
Epoch 113/200
Train Loss: 56.75493530, Train R²: 0.908119, Val Loss: 159.70355228, Val R²: 0.564711
Epoch 114/200
Train Loss: 55.94976297, Train R²: 0.910707, Val Loss: 160.28059502, Val R²: 0.561560
Epoch 115/200
Train Loss: 54.85629175, Train R²: 0.914163, Val Loss: 158.25196854, Val R²: 0.572588
Epoch 116/200
Train Loss: 53.24488312, Train R²: 0.919132, Val Loss: 155.80658332, Val R²: 0.585695
Epoch 117/200
Train Loss: 52.27632224, Train R²: 0.922047, Val Loss: 155.07255736, Val R²: 0.589590
Epoch 118/200
Train Loss: 51.30088071, Train R²: 0.924929, Val Loss: 155.75605049, Val R²: 0.585964
Epoch 119/200
Train Loss: 50.67638319, Train R²: 0.926746, Val Loss: 156.27312329, Val R²: 0.583210
Epoch 120/200
Train Loss: 50.10682972, Train R²: 0.928383, Val Loss: 155.67285283, Val R²: 0.586406
Epoch 121/200
Train Loss: 49.16423966, Train R²: 0.931053, Val Loss: 155.04822209, Val R²: 0.589718
Epoch 122/200
Train Loss: 48.50725069, Train R²: 0.932883, Val Loss: 155.97177729, Val R²: 0.584816
Epoch 123/200
Train Loss: 47.45466895, Train R²: 0.935764, Val Loss: 158.43210666, Val R²: 0.571614
Epoch 00123: reducing learning rate of group 0 to 2.5000e-05.
Epoch 124/200
Train Loss: 46.86743298, Train R²: 0.937344, Val Loss: 158.80071778, Val R²: 0.569619
Epoch 125/200
Train Loss: 46.62265698, Train R²: 0.937997, Val Loss: 157.93008614, Val R²: 0.574325
Epoch 126/200
Train Loss: 46.24627096, Train R²: 0.938994, Val Loss: 156.88184124, Val R²: 0.579957
Epoch 127/200
Train Loss: 45.93899604, Train R²: 0.939802, Val Loss: 156.43725030, Val R²: 0.582334
Epoch 128/200
Train Loss: 45.57215569, Train R²: 0.940759, Val Loss: 156.72978837, Val R²: 0.580771
Epoch 129/200
Train Loss: 45.09326165, Train R²: 0.941998, Val Loss: 157.31277935, Val R²: 0.577646
Epoch 130/200
Train Loss: 44.72999400, Train R²: 0.942929, Val Loss: 157.50381320, Val R²: 0.576620
Epoch 131/200
Train Loss: 44.45504747, Train R²: 0.943628, Val Loss: 157.12300491, Val R²: 0.578665
Epoch 132/200
Train Loss: 44.07562692, Train R²: 0.944586, Val Loss: 156.51352153, Val R²: 0.581927
Epoch 133/200
Train Loss: 43.77728948, Train R²: 0.945334, Val Loss: 156.20508730, Val R²: 0.583573
Epoch 134/200
Train Loss: 43.52434382, Train R²: 0.945964, Val Loss: 156.43726903, Val R²: 0.582334
Epoch 00134: reducing learning rate of group 0 to 1.2500e-05.
Epoch 135/200
Train Loss: 43.22048751, Train R²: 0.946716, Val Loss: 156.67901104, Val R²: 0.581042
Epoch 136/200
Train Loss: 43.06933965, Train R²: 0.947088, Val Loss: 156.88147397, Val R²: 0.579959
Epoch 137/200
Train Loss: 42.94298404, Train R²: 0.947398, Val Loss: 156.97190110, Val R²: 0.579474
Epoch 138/200
Train Loss: 42.81387273, Train R²: 0.947714, Val Loss: 156.93160084, Val R²: 0.579690
Epoch 139/200
Train Loss: 42.67648101, Train R²: 0.948049, Val Loss: 156.78966803, Val R²: 0.580450
Epoch 140/200
Train Loss: 42.56406259, Train R²: 0.948322, Val Loss: 156.61231119, Val R²: 0.581399
Epoch 141/200
Train Loss: 42.42027305, Train R²: 0.948671, Val Loss: 156.47621125, Val R²: 0.582126
Epoch 142/200
Train Loss: 42.29319442, Train R²: 0.948978, Val Loss: 156.43959747, Val R²: 0.582322
Epoch 143/200
Train Loss: 42.17252830, Train R²: 0.949268, Val Loss: 156.49743533, Val R²: 0.582013
Epoch 144/200
Train Loss: 42.04758134, Train R²: 0.949569, Val Loss: 156.57208678, Val R²: 0.581614
Epoch 145/200
Train Loss: 41.93494188, Train R²: 0.949838, Val Loss: 156.63260032, Val R²: 0.581290
Epoch 00145: reducing learning rate of group 0 to 6.2500e-06.
Epoch 146/200
Train Loss: 41.81691984, Train R²: 0.950120, Val Loss: 156.65877778, Val R²: 0.581150
Epoch 147/200
Train Loss: 41.76471149, Train R²: 0.950245, Val Loss: 156.66998556, Val R²: 0.581091
Epoch 148/200
Train Loss: 41.70895909, Train R²: 0.950378, Val Loss: 156.65428946, Val R²: 0.581174
Epoch 149/200
Train Loss: 41.64730540, Train R²: 0.950524, Val Loss: 156.62805514, Val R²: 0.581315
Epoch 150/200
Train Loss: 41.59955431, Train R²: 0.950637, Val Loss: 156.61030957, Val R²: 0.581410
Epoch 151/200
Train Loss: 41.53158507, Train R²: 0.950799, Val Loss: 156.59910376, Val R²: 0.581470
Epoch 152/200
Train Loss: 41.47295806, Train R²: 0.950937, Val Loss: 156.60463505, Val R²: 0.581440
Epoch 153/200
Train Loss: 41.41238516, Train R²: 0.951081, Val Loss: 156.62970738, Val R²: 0.581306
Epoch 154/200
Train Loss: 41.34618934, Train R²: 0.951237, Val Loss: 156.66092216, Val R²: 0.581139
Epoch 155/200
Train Loss: 41.28720209, Train R²: 0.951376, Val Loss: 156.68509422, Val R²: 0.581010
Epoch 156/200
Train Loss: 41.22807712, Train R²: 0.951515, Val Loss: 156.70405278, Val R²: 0.580908
Epoch 00156: reducing learning rate of group 0 to 3.1250e-06.
Epoch 157/200
Train Loss: 41.15932209, Train R²: 0.951677, Val Loss: 156.71287065, Val R²: 0.580861
Epoch 158/200
Train Loss: 41.12903488, Train R²: 0.951748, Val Loss: 156.72036702, Val R²: 0.580821
Epoch 159/200
Train Loss: 41.09620893, Train R²: 0.951825, Val Loss: 156.72743308, Val R²: 0.580783
Epoch 160/200
Train Loss: 41.06586774, Train R²: 0.951896, Val Loss: 156.73125885, Val R²: 0.580763
Epoch 161/200
Train Loss: 41.03331538, Train R²: 0.951972, Val Loss: 156.72968245, Val R²: 0.580771
Epoch 162/200
Train Loss: 41.00216074, Train R²: 0.952045, Val Loss: 156.71806767, Val R²: 0.580833
Epoch 163/200
Train Loss: 40.96898122, Train R²: 0.952123, Val Loss: 156.70127956, Val R²: 0.580923
Epoch 164/200
Train Loss: 40.94138132, Train R²: 0.952187, Val Loss: 156.68091829, Val R²: 0.581032
Epoch 165/200
Train Loss: 40.91070270, Train R²: 0.952259, Val Loss: 156.65531181, Val R²: 0.581169
Epoch 166/200
Train Loss: 40.88074489, Train R²: 0.952329, Val Loss: 156.62731318, Val R²: 0.581319
Epoch 167/200
Train Loss: 40.85286725, Train R²: 0.952394, Val Loss: 156.60102445, Val R²: 0.581459
Epoch 00167: reducing learning rate of group 0 to 1.5625e-06.
Epoch 168/200
Train Loss: 40.82282782, Train R²: 0.952464, Val Loss: 156.59112139, Val R²: 0.581512
Epoch 169/200
Train Loss: 40.80891961, Train R²: 0.952496, Val Loss: 156.58475391, Val R²: 0.581546
Epoch 170/200
Train Loss: 40.79199631, Train R²: 0.952535, Val Loss: 156.58212826, Val R²: 0.581560
Epoch 171/200
Train Loss: 40.77605690, Train R²: 0.952572, Val Loss: 156.58123017, Val R²: 0.581565
Epoch 172/200
Train Loss: 40.75861620, Train R²: 0.952613, Val Loss: 156.58091833, Val R²: 0.581567
Epoch 173/200
Train Loss: 40.74343347, Train R²: 0.952648, Val Loss: 156.58201600, Val R²: 0.581561
Epoch 174/200
Train Loss: 40.72497734, Train R²: 0.952691, Val Loss: 156.58410530, Val R²: 0.581550
Epoch 175/200
Train Loss: 40.70780803, Train R²: 0.952731, Val Loss: 156.58673715, Val R²: 0.581536
Epoch 176/200
Train Loss: 40.69352709, Train R²: 0.952764, Val Loss: 156.58917562, Val R²: 0.581523
Epoch 177/200
Train Loss: 40.67698627, Train R²: 0.952803, Val Loss: 156.59163277, Val R²: 0.581509
Epoch 178/200
Train Loss: 40.66054785, Train R²: 0.952841, Val Loss: 156.59491306, Val R²: 0.581492
Epoch 00178: reducing learning rate of group 0 to 7.8125e-07.
Epoch 179/200
Train Loss: 40.64670440, Train R²: 0.952873, Val Loss: 156.59634739, Val R²: 0.581484
Epoch 180/200
Train Loss: 40.63922517, Train R²: 0.952890, Val Loss: 156.59665920, Val R²: 0.581483
Epoch 181/200
Train Loss: 40.63173427, Train R²: 0.952908, Val Loss: 156.59611665, Val R²: 0.581486
Epoch 182/200
Train Loss: 40.62524155, Train R²: 0.952923, Val Loss: 156.59601687, Val R²: 0.581486
Epoch 183/200
Train Loss: 40.61753417, Train R²: 0.952941, Val Loss: 156.59558034, Val R²: 0.581488
Epoch 184/200
Train Loss: 40.61161769, Train R²: 0.952954, Val Loss: 156.59391526, Val R²: 0.581497
Epoch 185/200
Train Loss: 40.60351114, Train R²: 0.952973, Val Loss: 156.59233748, Val R²: 0.581506
Epoch 186/200
Train Loss: 40.59644850, Train R²: 0.952989, Val Loss: 156.59125235, Val R²: 0.581512
Epoch 187/200
Train Loss: 40.58917110, Train R²: 0.953006, Val Loss: 156.59014227, Val R²: 0.581517
Epoch 188/200
Train Loss: 40.58123619, Train R²: 0.953025, Val Loss: 156.58810295, Val R²: 0.581528
Epoch 189/200
Train Loss: 40.57416501, Train R²: 0.953041, Val Loss: 156.58605736, Val R²: 0.581539
Epoch 00189: reducing learning rate of group 0 to 3.9063e-07.
Epoch 190/200
Train Loss: 40.56678015, Train R²: 0.953058, Val Loss: 156.58505950, Val R²: 0.581545
Epoch 191/200
Train Loss: 40.56303944, Train R²: 0.953067, Val Loss: 156.58399304, Val R²: 0.581550
Epoch 192/200
Train Loss: 40.55971584, Train R²: 0.953074, Val Loss: 156.58284548, Val R²: 0.581556
Epoch 193/200
Train Loss: 40.55640413, Train R²: 0.953082, Val Loss: 156.58194739, Val R²: 0.581561
Epoch 194/200
Train Loss: 40.55309223, Train R²: 0.953090, Val Loss: 156.58123017, Val R²: 0.581565
Epoch 195/200
Train Loss: 40.54941165, Train R²: 0.953098, Val Loss: 156.58040067, Val R²: 0.581570
Epoch 196/200
Train Loss: 40.54635145, Train R²: 0.953105, Val Loss: 156.57887265, Val R²: 0.581578
Epoch 197/200
Train Loss: 40.54294568, Train R²: 0.953113, Val Loss: 156.57723858, Val R²: 0.581586
Epoch 198/200
Train Loss: 40.53892624, Train R²: 0.953122, Val Loss: 156.57616582, Val R²: 0.581592
Epoch 199/200
Train Loss: 40.53564633, Train R²: 0.953130, Val Loss: 156.57585397, Val R²: 0.581594
Epoch 200/200
Train Loss: 40.53150419, Train R²: 0.953140, Val Loss: 156.57575418, Val R²: 0.581594
Epoch 00200: reducing learning rate of group 0 to 1.9531e-07.
Training Complete. Best Val Loss: 153.37265050156432
训练时间: 49.92 秒
