Using device: cuda
Total samples: 800, Training: 640, Validation: 160
Total trainable parameters: 4577537
Epoch 1/200
Train Loss: 1562.97927050, Train R²: -46.866455, Val Loss: 1566.39289452, Val R²: -38.377510
Saved best model with validation R2 -38.377510 to best_finetuned_model.pth
Epoch 2/200
Train Loss: 1560.96925819, Train R²: -46.743416, Val Loss: 1563.49956828, Val R²: -38.232174
Saved best model with validation R2 -38.232174 to best_finetuned_model.pth
Epoch 3/200
Train Loss: 1557.29966609, Train R²: -46.519215, Val Loss: 1558.86434625, Val R²: -37.999897
Saved best model with validation R2 -37.999897 to best_finetuned_model.pth
Epoch 4/200
Train Loss: 1551.52291314, Train R²: -46.167320, Val Loss: 1551.50391556, Val R²: -37.632481
Saved best model with validation R2 -37.632481 to best_finetuned_model.pth
Epoch 5/200
Train Loss: 1542.43561454, Train R²: -45.616417, Val Loss: 1540.22446416, Val R²: -37.072803
Saved best model with validation R2 -37.072803 to best_finetuned_model.pth
Epoch 6/200
Train Loss: 1528.64522535, Train R²: -44.786587, Val Loss: 1523.42446482, Val R²: -36.246773
Saved best model with validation R2 -36.246773 to best_finetuned_model.pth
Epoch 7/200
Train Loss: 1508.53846487, Train R²: -43.590015, Val Loss: 1498.99124414, Val R²: -35.061600
Saved best model with validation R2 -35.061600 to best_finetuned_model.pth
Epoch 8/200
Train Loss: 1479.55417272, Train R²: -41.893017, Val Loss: 1464.30341801, Val R²: -33.411922
Saved best model with validation R2 -33.411922 to best_finetuned_model.pth
Epoch 9/200
Train Loss: 1438.75162033, Train R²: -39.559860, Val Loss: 1415.93561824, Val R²: -31.176132
Saved best model with validation R2 -31.176132 to best_finetuned_model.pth
Epoch 10/200
Train Loss: 1382.59147075, Train R²: -36.455242, Val Loss: 1350.24473708, Val R²: -28.259830
Saved best model with validation R2 -28.259830 to best_finetuned_model.pth
Epoch 11/200
Train Loss: 1306.94438482, Train R²: -32.468716, Val Loss: 1263.31397523, Val R²: -24.613529
Saved best model with validation R2 -24.613529 to best_finetuned_model.pth
Epoch 12/200
Train Loss: 1208.68197844, Train R²: -27.625227, Val Loss: 1151.14623311, Val R²: -20.267080
Saved best model with validation R2 -20.267080 to best_finetuned_model.pth
Epoch 13/200
Train Loss: 1083.46883896, Train R²: -22.001581, Val Loss: 1013.13299966, Val R²: -15.473269
Saved best model with validation R2 -15.473269 to best_finetuned_model.pth
Epoch 14/200
Train Loss: 931.59154810, Train R²: -16.004971, Val Loss: 847.02468234, Val R²: -10.514338
Saved best model with validation R2 -10.514338 to best_finetuned_model.pth
Epoch 15/200
Train Loss: 755.81314862, Train R²: -10.193188, Val Loss: 662.79327471, Val R²: -6.050226
Saved best model with validation R2 -6.050226 to best_finetuned_model.pth
Epoch 16/200
Train Loss: 565.71945896, Train R²: -5.270859, Val Loss: 477.93182699, Val R²: -2.665884
Saved best model with validation R2 -2.665884 to best_finetuned_model.pth
Epoch 17/200
Train Loss: 389.46339386, Train R²: -1.972064, Val Loss: 325.74098222, Val R²: -0.702910
Saved best model with validation R2 -0.702910 to best_finetuned_model.pth
Epoch 18/200
Train Loss: 265.92709366, Train R²: -0.385639, Val Loss: 246.73603367, Val R²: 0.022961
Saved best model with validation R2 0.022961 to best_finetuned_model.pth
Epoch 19/200
Train Loss: 205.10555114, Train R²: 0.175711, Val Loss: 208.50703300, Val R²: 0.302269
Saved best model with validation R2 0.302269 to best_finetuned_model.pth
Epoch 20/200
Train Loss: 170.42104683, Train R²: 0.430923, Val Loss: 178.79432580, Val R²: 0.486956
Saved best model with validation R2 0.486956 to best_finetuned_model.pth
Epoch 21/200
Train Loss: 140.79802966, Train R²: 0.611565, Val Loss: 162.00408584, Val R²: 0.578790
Saved best model with validation R2 0.578790 to best_finetuned_model.pth
Epoch 22/200
Train Loss: 119.21895028, Train R²: 0.721506, Val Loss: 143.73361320, Val R²: 0.668439
Saved best model with validation R2 0.668439 to best_finetuned_model.pth
Epoch 23/200
Train Loss: 101.31931992, Train R²: 0.798855, Val Loss: 141.02406433, Val R²: 0.680822
Saved best model with validation R2 0.680822 to best_finetuned_model.pth
Epoch 24/200
Train Loss: 83.49002619, Train R²: 0.863418, Val Loss: 129.80750321, Val R²: 0.729575
Saved best model with validation R2 0.729575 to best_finetuned_model.pth
Epoch 25/200
Train Loss: 70.22321901, Train R²: 0.903376, Val Loss: 127.31794217, Val R²: 0.739849
Saved best model with validation R2 0.739849 to best_finetuned_model.pth
Epoch 26/200
Train Loss: 58.38285494, Train R²: 0.933212, Val Loss: 121.20931502, Val R²: 0.764214
Saved best model with validation R2 0.764214 to best_finetuned_model.pth
Epoch 27/200
Train Loss: 50.01194602, Train R²: 0.950991, Val Loss: 119.90219810, Val R²: 0.769272
Saved best model with validation R2 0.769272 to best_finetuned_model.pth
Epoch 28/200
Train Loss: 44.25887245, Train R²: 0.961618, Val Loss: 117.75479688, Val R²: 0.777462
Saved best model with validation R2 0.777462 to best_finetuned_model.pth
Epoch 29/200
Train Loss: 40.42667486, Train R²: 0.967977, Val Loss: 113.29781517, Val R²: 0.793989
Saved best model with validation R2 0.793989 to best_finetuned_model.pth
Epoch 30/200
Train Loss: 36.17356106, Train R²: 0.974361, Val Loss: 115.88254594, Val R²: 0.784482
Epoch 31/200
Train Loss: 33.97778903, Train R²: 0.977379, Val Loss: 112.44889812, Val R²: 0.797065
Saved best model with validation R2 0.797065 to best_finetuned_model.pth
Epoch 32/200
Train Loss: 32.68232105, Train R²: 0.979071, Val Loss: 114.09119245, Val R²: 0.791094
Epoch 33/200
Train Loss: 31.86129981, Train R²: 0.980109, Val Loss: 117.51959860, Val R²: 0.778350
Epoch 34/200
Train Loss: 32.46096796, Train R²: 0.979353, Val Loss: 112.79585404, Val R²: 0.795811
Epoch 35/200
Train Loss: 29.06078151, Train R²: 0.983452, Val Loss: 112.82064202, Val R²: 0.795721
Epoch 36/200
Train Loss: 26.89224876, Train R²: 0.985830, Val Loss: 113.05499042, Val R²: 0.794871
Epoch 37/200
Train Loss: 25.26295415, Train R²: 0.987495, Val Loss: 112.66472056, Val R²: 0.796285
Epoch 38/200
Train Loss: 24.11433279, Train R²: 0.988606, Val Loss: 112.71016047, Val R²: 0.796121
Epoch 39/200
Train Loss: 22.88191836, Train R²: 0.989741, Val Loss: 110.76450684, Val R²: 0.803099
Saved best model with validation R2 0.803099 to best_finetuned_model.pth
Epoch 40/200
Train Loss: 22.38090354, Train R²: 0.990185, Val Loss: 110.58396661, Val R²: 0.803740
Saved best model with validation R2 0.803740 to best_finetuned_model.pth
Epoch 41/200
Train Loss: 23.54810860, Train R²: 0.989135, Val Loss: 111.22365295, Val R²: 0.801463
Epoch 42/200
Train Loss: 26.99232495, Train R²: 0.985724, Val Loss: 115.29102273, Val R²: 0.786677
Epoch 43/200
Train Loss: 27.92990581, Train R²: 0.984715, Val Loss: 111.82775539, Val R²: 0.799301
Epoch 44/200
Train Loss: 24.80372984, Train R²: 0.987945, Val Loss: 112.04644859, Val R²: 0.798515
Epoch 45/200
Train Loss: 22.99958731, Train R²: 0.989635, Val Loss: 112.23230825, Val R²: 0.797846
Epoch 46/200
Train Loss: 24.02400082, Train R²: 0.988691, Val Loss: 113.62541168, Val R²: 0.792796
Epoch 47/200
Train Loss: 22.50726005, Train R²: 0.990074, Val Loss: 111.22385051, Val R²: 0.801462
Epoch 48/200
Train Loss: 22.29507821, Train R²: 0.990260, Val Loss: 113.99519932, Val R²: 0.791445
Epoch 49/200
Train Loss: 22.06657038, Train R²: 0.990459, Val Loss: 110.40485776, Val R²: 0.804376
Saved best model with validation R2 0.804376 to best_finetuned_model.pth
Epoch 50/200
Train Loss: 20.92603105, Train R²: 0.991420, Val Loss: 113.38977302, Val R²: 0.793655
Epoch 51/200
Train Loss: 19.51926712, Train R²: 0.992535, Val Loss: 111.03544972, Val R²: 0.802134
Epoch 52/200
Train Loss: 17.29999787, Train R²: 0.994136, Val Loss: 111.63288046, Val R²: 0.799999
Epoch 53/200
Train Loss: 16.24629533, Train R²: 0.994828, Val Loss: 111.37005373, Val R²: 0.800940
Epoch 54/200
Train Loss: 15.36367410, Train R²: 0.995375, Val Loss: 110.85813182, Val R²: 0.802766
Epoch 55/200
Train Loss: 15.87476600, Train R²: 0.995062, Val Loss: 110.06993515, Val R²: 0.805561
Saved best model with validation R2 0.805561 to best_finetuned_model.pth
Epoch 56/200
Train Loss: 15.75550064, Train R²: 0.995136, Val Loss: 113.74515014, Val R²: 0.792359
Epoch 57/200
Train Loss: 14.90605163, Train R²: 0.995646, Val Loss: 110.10174804, Val R²: 0.805448
Epoch 58/200
Train Loss: 13.65027181, Train R²: 0.996349, Val Loss: 110.70720198, Val R²: 0.803303
Epoch 59/200
Train Loss: 12.59421627, Train R²: 0.996892, Val Loss: 110.10274765, Val R²: 0.805445
Epoch 60/200
Train Loss: 12.15905960, Train R²: 0.997103, Val Loss: 112.37629049, Val R²: 0.797327
Epoch 61/200
Train Loss: 12.95532231, Train R²: 0.996711, Val Loss: 111.83465404, Val R²: 0.799276
Epoch 62/200
Train Loss: 12.59625130, Train R²: 0.996891, Val Loss: 112.10127780, Val R²: 0.798318
Epoch 63/200
Train Loss: 12.54447949, Train R²: 0.996917, Val Loss: 110.54374488, Val R²: 0.803883
Epoch 64/200
Train Loss: 13.60869321, Train R²: 0.996371, Val Loss: 111.88111890, Val R²: 0.799109
Epoch 65/200
Train Loss: 14.85955262, Train R²: 0.995673, Val Loss: 111.51958208, Val R²: 0.800405
Epoch 66/200
Train Loss: 17.01642493, Train R²: 0.994326, Val Loss: 111.29101245, Val R²: 0.801223
Epoch 00066: reducing learning rate of group 0 to 5.0000e-05.
Epoch 67/200
Train Loss: 14.05006414, Train R²: 0.996132, Val Loss: 111.51800846, Val R²: 0.800411
Epoch 68/200
Train Loss: 12.95912070, Train R²: 0.996709, Val Loss: 110.77394105, Val R²: 0.803065
Epoch 69/200
Train Loss: 11.41633494, Train R²: 0.997446, Val Loss: 109.83084650, Val R²: 0.806404
Saved best model with validation R2 0.806404 to best_finetuned_model.pth
Epoch 70/200
Train Loss: 9.99659433, Train R²: 0.998042, Val Loss: 110.54167501, Val R²: 0.803890
Epoch 71/200
Train Loss: 9.11687260, Train R²: 0.998371, Val Loss: 110.50969627, Val R²: 0.804004
Epoch 72/200
Train Loss: 8.34095931, Train R²: 0.998637, Val Loss: 110.51836315, Val R²: 0.803973
Epoch 73/200
Train Loss: 7.94396772, Train R²: 0.998764, Val Loss: 110.73716330, Val R²: 0.803196
Epoch 74/200
Train Loss: 7.61097672, Train R²: 0.998865, Val Loss: 110.76017959, Val R²: 0.803114
Epoch 75/200
Train Loss: 7.46833553, Train R²: 0.998907, Val Loss: 110.37186434, Val R²: 0.804492
Epoch 76/200
Train Loss: 7.30820220, Train R²: 0.998953, Val Loss: 110.50108968, Val R²: 0.804034
Epoch 77/200
Train Loss: 7.14253393, Train R²: 0.999000, Val Loss: 110.57419208, Val R²: 0.803775
Epoch 78/200
Train Loss: 7.03622722, Train R²: 0.999030, Val Loss: 110.51707394, Val R²: 0.803978
Epoch 79/200
Train Loss: 6.92778589, Train R²: 0.999060, Val Loss: 110.55618887, Val R²: 0.803839
Epoch 80/200
Train Loss: 6.83747677, Train R²: 0.999084, Val Loss: 110.52739515, Val R²: 0.803941
Epoch 00080: reducing learning rate of group 0 to 2.5000e-05.
Epoch 81/200
Train Loss: 6.75290732, Train R²: 0.999106, Val Loss: 110.46015967, Val R²: 0.804179
Epoch 82/200
Train Loss: 6.71165482, Train R²: 0.999117, Val Loss: 110.49281121, Val R²: 0.804064
Epoch 83/200
Train Loss: 6.66132046, Train R²: 0.999131, Val Loss: 110.48859705, Val R²: 0.804079
Epoch 84/200
Train Loss: 6.60631526, Train R²: 0.999145, Val Loss: 110.44016869, Val R²: 0.804250
Epoch 85/200
Train Loss: 6.56507973, Train R²: 0.999155, Val Loss: 110.47697636, Val R²: 0.804120
Epoch 86/200
Train Loss: 6.51641614, Train R²: 0.999168, Val Loss: 110.42743481, Val R²: 0.804296
Epoch 87/200
Train Loss: 6.46406287, Train R²: 0.999181, Val Loss: 110.47494767, Val R²: 0.804127
Epoch 88/200
Train Loss: 6.42533665, Train R²: 0.999191, Val Loss: 110.44173998, Val R²: 0.804245
Epoch 89/200
Train Loss: 6.37812310, Train R²: 0.999203, Val Loss: 110.42612951, Val R²: 0.804300
Epoch 90/200
Train Loss: 6.33020856, Train R²: 0.999215, Val Loss: 110.41687343, Val R²: 0.804333
Epoch 91/200
Train Loss: 6.28287510, Train R²: 0.999227, Val Loss: 110.40182113, Val R²: 0.804386
Epoch 00091: reducing learning rate of group 0 to 1.2500e-05.
Epoch 92/200
Train Loss: 6.24313344, Train R²: 0.999236, Val Loss: 110.39187389, Val R²: 0.804422
Epoch 93/200
Train Loss: 6.22040474, Train R²: 0.999242, Val Loss: 110.40568214, Val R²: 0.804373
Epoch 94/200
Train Loss: 6.19809000, Train R²: 0.999247, Val Loss: 110.39195793, Val R²: 0.804421
Epoch 95/200
Train Loss: 6.17646613, Train R²: 0.999252, Val Loss: 110.38390483, Val R²: 0.804450
Epoch 96/200
Train Loss: 6.15313379, Train R²: 0.999258, Val Loss: 110.38995776, Val R²: 0.804428
Epoch 97/200
Train Loss: 6.12929469, Train R²: 0.999264, Val Loss: 110.38393137, Val R²: 0.804450
Epoch 98/200
Train Loss: 6.10360620, Train R²: 0.999270, Val Loss: 110.38894218, Val R²: 0.804432
Epoch 99/200
Train Loss: 6.08422263, Train R²: 0.999275, Val Loss: 110.36835785, Val R²: 0.804505
Epoch 100/200
Train Loss: 6.06086107, Train R²: 0.999280, Val Loss: 110.34338891, Val R²: 0.804593
Epoch 101/200
Train Loss: 6.03524293, Train R²: 0.999286, Val Loss: 110.36096492, Val R²: 0.804531
Epoch 102/200
Train Loss: 6.01324845, Train R²: 0.999291, Val Loss: 110.35397414, Val R²: 0.804556
Epoch 00102: reducing learning rate of group 0 to 6.2500e-06.
Epoch 103/200
Train Loss: 5.99448369, Train R²: 0.999296, Val Loss: 110.34504477, Val R²: 0.804587
Epoch 104/200
Train Loss: 5.98306321, Train R²: 0.999299, Val Loss: 110.34892193, Val R²: 0.804574
Epoch 105/200
Train Loss: 5.97034081, Train R²: 0.999302, Val Loss: 110.35792797, Val R²: 0.804542
Epoch 106/200
Train Loss: 5.95737001, Train R²: 0.999305, Val Loss: 110.33851322, Val R²: 0.804611
Epoch 107/200
Train Loss: 5.94566049, Train R²: 0.999307, Val Loss: 110.34989451, Val R²: 0.804570
Epoch 108/200
Train Loss: 5.93540655, Train R²: 0.999310, Val Loss: 110.33432415, Val R²: 0.804625
Epoch 109/200
Train Loss: 5.92289149, Train R²: 0.999313, Val Loss: 110.35417856, Val R²: 0.804555
Epoch 110/200
Train Loss: 5.91164468, Train R²: 0.999315, Val Loss: 110.33758213, Val R²: 0.804614
Epoch 111/200
Train Loss: 5.89858314, Train R²: 0.999318, Val Loss: 110.33125018, Val R²: 0.804636
Epoch 112/200
Train Loss: 5.88875335, Train R²: 0.999321, Val Loss: 110.31203966, Val R²: 0.804704
Epoch 113/200
Train Loss: 5.87434048, Train R²: 0.999324, Val Loss: 110.33222911, Val R²: 0.804633
Epoch 00113: reducing learning rate of group 0 to 3.1250e-06.
Epoch 114/200
Train Loss: 5.86425365, Train R²: 0.999326, Val Loss: 110.33354084, Val R²: 0.804628
Epoch 115/200
Train Loss: 5.85772888, Train R²: 0.999328, Val Loss: 110.32832482, Val R²: 0.804647
Epoch 116/200
Train Loss: 5.85263189, Train R²: 0.999329, Val Loss: 110.32153557, Val R²: 0.804671
Epoch 117/200
Train Loss: 5.84498021, Train R²: 0.999331, Val Loss: 110.32948435, Val R²: 0.804643
Epoch 118/200
Train Loss: 5.83941030, Train R²: 0.999332, Val Loss: 110.32315635, Val R²: 0.804665
Epoch 119/200
Train Loss: 5.83337043, Train R²: 0.999333, Val Loss: 110.33073504, Val R²: 0.804638
Epoch 120/200
Train Loss: 5.82685339, Train R²: 0.999335, Val Loss: 110.31679259, Val R²: 0.804688
Epoch 121/200
Train Loss: 5.82038161, Train R²: 0.999336, Val Loss: 110.32441154, Val R²: 0.804660
Epoch 122/200
Train Loss: 5.81483681, Train R²: 0.999337, Val Loss: 110.31934026, Val R²: 0.804678
Epoch 123/200
Train Loss: 5.80834605, Train R²: 0.999339, Val Loss: 110.31468217, Val R²: 0.804695
Epoch 124/200
Train Loss: 5.80092988, Train R²: 0.999341, Val Loss: 110.32106022, Val R²: 0.804672
Epoch 00124: reducing learning rate of group 0 to 1.5625e-06.
Epoch 125/200
Train Loss: 5.79616421, Train R²: 0.999342, Val Loss: 110.31546650, Val R²: 0.804692
Epoch 126/200
Train Loss: 5.79240450, Train R²: 0.999343, Val Loss: 110.31437498, Val R²: 0.804696
Epoch 127/200
Train Loss: 5.78960749, Train R²: 0.999343, Val Loss: 110.31633315, Val R²: 0.804689
Epoch 128/200
Train Loss: 5.78628116, Train R²: 0.999344, Val Loss: 110.31218573, Val R²: 0.804704
Epoch 129/200
Train Loss: 5.78265652, Train R²: 0.999345, Val Loss: 110.31091447, Val R²: 0.804708
Epoch 130/200
Train Loss: 5.77919976, Train R²: 0.999346, Val Loss: 110.31507964, Val R²: 0.804694
Epoch 131/200
Train Loss: 5.77620522, Train R²: 0.999346, Val Loss: 110.30921383, Val R²: 0.804714
Epoch 132/200
Train Loss: 5.77246929, Train R²: 0.999347, Val Loss: 110.31111543, Val R²: 0.804708
Epoch 133/200
Train Loss: 5.76932182, Train R²: 0.999348, Val Loss: 110.30928289, Val R²: 0.804714
Epoch 134/200
Train Loss: 5.76565357, Train R²: 0.999349, Val Loss: 110.30916957, Val R²: 0.804715
Epoch 135/200
Train Loss: 5.76254263, Train R²: 0.999349, Val Loss: 110.30640210, Val R²: 0.804724
Epoch 00135: reducing learning rate of group 0 to 7.8125e-07.
Epoch 136/200
Train Loss: 5.75957905, Train R²: 0.999350, Val Loss: 110.30741932, Val R²: 0.804721
Epoch 137/200
Train Loss: 5.75767969, Train R²: 0.999350, Val Loss: 110.30725466, Val R²: 0.804721
Epoch 138/200
Train Loss: 5.75613496, Train R²: 0.999351, Val Loss: 110.30834624, Val R²: 0.804717
Epoch 139/200
Train Loss: 5.75412433, Train R²: 0.999351, Val Loss: 110.30554953, Val R²: 0.804727
Epoch 140/200
Train Loss: 5.75286079, Train R²: 0.999352, Val Loss: 110.30828250, Val R²: 0.804718
Epoch 141/200
Train Loss: 5.75073007, Train R²: 0.999352, Val Loss: 110.30516265, Val R²: 0.804729
Epoch 142/200
Train Loss: 5.74893107, Train R²: 0.999352, Val Loss: 110.30468368, Val R²: 0.804730
Epoch 143/200
Train Loss: 5.74725371, Train R²: 0.999353, Val Loss: 110.30590366, Val R²: 0.804726
Epoch 144/200
Train Loss: 5.74532244, Train R²: 0.999353, Val Loss: 110.30358498, Val R²: 0.804734
Epoch 145/200
Train Loss: 5.74370295, Train R²: 0.999354, Val Loss: 110.30497673, Val R²: 0.804729
Epoch 146/200
Train Loss: 5.74187309, Train R²: 0.999354, Val Loss: 110.30522285, Val R²: 0.804728
Epoch 00146: reducing learning rate of group 0 to 3.9063e-07.
Epoch 147/200
Train Loss: 5.74043330, Train R²: 0.999354, Val Loss: 110.30484039, Val R²: 0.804730
Epoch 148/200
Train Loss: 5.73971997, Train R²: 0.999354, Val Loss: 110.30343359, Val R²: 0.804735
Epoch 149/200
Train Loss: 5.73873303, Train R²: 0.999355, Val Loss: 110.30261995, Val R²: 0.804738
Epoch 150/200
Train Loss: 5.73803904, Train R²: 0.999355, Val Loss: 110.30292363, Val R²: 0.804737
Epoch 151/200
Train Loss: 5.73730095, Train R²: 0.999355, Val Loss: 110.30322819, Val R²: 0.804736
Epoch 152/200
Train Loss: 5.73650070, Train R²: 0.999355, Val Loss: 110.30229237, Val R²: 0.804739
Epoch 153/200
Train Loss: 5.73567401, Train R²: 0.999355, Val Loss: 110.30214098, Val R²: 0.804739
Epoch 154/200
Train Loss: 5.73489735, Train R²: 0.999356, Val Loss: 110.30262704, Val R²: 0.804738
Epoch 155/200
Train Loss: 5.73403281, Train R²: 0.999356, Val Loss: 110.30258808, Val R²: 0.804738
Epoch 156/200
Train Loss: 5.73321160, Train R²: 0.999356, Val Loss: 110.30211885, Val R²: 0.804739
Epoch 157/200
Train Loss: 5.73228702, Train R²: 0.999356, Val Loss: 110.30092538, Val R²: 0.804744
Epoch 00157: reducing learning rate of group 0 to 1.9531e-07.
Epoch 158/200
Train Loss: 5.73177172, Train R²: 0.999356, Val Loss: 110.30103428, Val R²: 0.804743
Epoch 159/200
Train Loss: 5.73147774, Train R²: 0.999356, Val Loss: 110.30112193, Val R²: 0.804743
Epoch 160/200
Train Loss: 5.73120698, Train R²: 0.999356, Val Loss: 110.30121755, Val R²: 0.804743
Epoch 161/200
Train Loss: 5.73090297, Train R²: 0.999356, Val Loss: 110.30146368, Val R²: 0.804742
Epoch 162/200
Train Loss: 5.73057023, Train R²: 0.999357, Val Loss: 110.30133619, Val R²: 0.804742
Epoch 163/200
Train Loss: 5.73025177, Train R²: 0.999357, Val Loss: 110.30132468, Val R²: 0.804742
Epoch 164/200
Train Loss: 5.72994873, Train R²: 0.999357, Val Loss: 110.30096080, Val R²: 0.804744
Epoch 165/200
Train Loss: 5.72969669, Train R²: 0.999357, Val Loss: 110.30108475, Val R²: 0.804743
Epoch 166/200
Train Loss: 5.72928590, Train R²: 0.999357, Val Loss: 110.30074743, Val R²: 0.804744
Epoch 167/200
Train Loss: 5.72905307, Train R²: 0.999357, Val Loss: 110.30083862, Val R²: 0.804744
Epoch 168/200
Train Loss: 5.72870023, Train R²: 0.999357, Val Loss: 110.30073946, Val R²: 0.804744
Epoch 00168: reducing learning rate of group 0 to 9.7656e-08.
Epoch 169/200
Train Loss: 5.72842443, Train R²: 0.999357, Val Loss: 110.30074566, Val R²: 0.804744
Epoch 170/200
Train Loss: 5.72833625, Train R²: 0.999357, Val Loss: 110.30066332, Val R²: 0.804745
Epoch 171/200
Train Loss: 5.72823777, Train R²: 0.999357, Val Loss: 110.30055707, Val R²: 0.804745
Epoch 172/200
Train Loss: 5.72813194, Train R²: 0.999357, Val Loss: 110.30054379, Val R²: 0.804745
Epoch 173/200
Train Loss: 5.72801834, Train R²: 0.999357, Val Loss: 110.30048536, Val R²: 0.804745
Epoch 174/200
Train Loss: 5.72793016, Train R²: 0.999357, Val Loss: 110.30040833, Val R²: 0.804745
Epoch 175/200
Train Loss: 5.72779244, Train R²: 0.999357, Val Loss: 110.30040036, Val R²: 0.804746
Epoch 176/200
Train Loss: 5.72772082, Train R²: 0.999357, Val Loss: 110.30031625, Val R²: 0.804746
Epoch 177/200
Train Loss: 5.72760143, Train R²: 0.999357, Val Loss: 110.30033042, Val R²: 0.804746
Epoch 178/200
Train Loss: 5.72749202, Train R²: 0.999357, Val Loss: 110.30021709, Val R²: 0.804746
Epoch 179/200
Train Loss: 5.72737671, Train R²: 0.999357, Val Loss: 110.30022506, Val R²: 0.804746
Epoch 00179: reducing learning rate of group 0 to 4.8828e-08.
Epoch 180/200
Train Loss: 5.72732142, Train R²: 0.999357, Val Loss: 110.30021621, Val R²: 0.804746
Epoch 181/200
Train Loss: 5.72727157, Train R²: 0.999357, Val Loss: 110.30016574, Val R²: 0.804746
Epoch 182/200
Train Loss: 5.72723019, Train R²: 0.999357, Val Loss: 110.30014803, Val R²: 0.804746
Epoch 183/200
Train Loss: 5.72719077, Train R²: 0.999357, Val Loss: 110.30012501, Val R²: 0.804747
Epoch 184/200
Train Loss: 5.72716335, Train R²: 0.999357, Val Loss: 110.30010819, Val R²: 0.804747
Epoch 185/200
Train Loss: 5.72711465, Train R²: 0.999357, Val Loss: 110.30011793, Val R²: 0.804747
Epoch 186/200
Train Loss: 5.72707959, Train R²: 0.999357, Val Loss: 110.30009491, Val R²: 0.804747
Epoch 187/200
Train Loss: 5.72704218, Train R²: 0.999357, Val Loss: 110.30005507, Val R²: 0.804747
Epoch 188/200
Train Loss: 5.72700457, Train R²: 0.999357, Val Loss: 110.30005418, Val R²: 0.804747
Epoch 189/200
Train Loss: 5.72696649, Train R²: 0.999357, Val Loss: 110.30002054, Val R²: 0.804747
Epoch 190/200
Train Loss: 5.72693294, Train R²: 0.999357, Val Loss: 110.29998158, Val R²: 0.804747
Epoch 00190: reducing learning rate of group 0 to 2.4414e-08.
Epoch 191/200
Train Loss: 5.72689335, Train R²: 0.999357, Val Loss: 110.29998513, Val R²: 0.804747
Epoch 192/200
Train Loss: 5.72688424, Train R²: 0.999357, Val Loss: 110.29998690, Val R²: 0.804747
Epoch 193/200
Train Loss: 5.72686956, Train R²: 0.999357, Val Loss: 110.29997096, Val R²: 0.804747
Epoch 194/200
Train Loss: 5.72685261, Train R²: 0.999357, Val Loss: 110.29995325, Val R²: 0.804747
Epoch 195/200
Train Loss: 5.72683488, Train R²: 0.999357, Val Loss: 110.29997096, Val R²: 0.804747
Epoch 196/200
Train Loss: 5.72682041, Train R²: 0.999357, Val Loss: 110.29995502, Val R²: 0.804747
Epoch 197/200
Train Loss: 5.72680870, Train R²: 0.999357, Val Loss: 110.29995679, Val R²: 0.804747
Epoch 198/200
Train Loss: 5.72679491, Train R²: 0.999357, Val Loss: 110.29994174, Val R²: 0.804747
Epoch 199/200
Train Loss: 5.72677496, Train R²: 0.999357, Val Loss: 110.29995325, Val R²: 0.804747
Epoch 200/200
Train Loss: 5.72676250, Train R²: 0.999357, Val Loss: 110.29992404, Val R²: 0.804747
Training Complete. Best Val Loss: 109.83084650383971
训练时间: 153.19 秒
