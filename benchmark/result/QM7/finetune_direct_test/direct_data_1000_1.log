Using device: cuda
Total samples: 1000, Training: 800, Validation: 200
Total trainable parameters: 4577537
Epoch 1/200
Train Loss: 1553.42924525, Train R²: -42.627304, Val Loss: 1572.50072814, Val R²: -51.785568
Saved best model with validation R2 -51.785568 to best_finetuned_model.pth
Epoch 2/200
Train Loss: 1549.56924660, Train R²: -42.410763, Val Loss: 1566.81112455, Val R²: -51.404285
Saved best model with validation R2 -51.404285 to best_finetuned_model.pth
Epoch 3/200
Train Loss: 1542.02289218, Train R²: -41.988979, Val Loss: 1556.65188787, Val R²: -50.726910
Saved best model with validation R2 -50.726910 to best_finetuned_model.pth
Epoch 4/200
Train Loss: 1529.02735097, Train R²: -41.267437, Val Loss: 1539.47005492, Val R²: -49.591316
Saved best model with validation R2 -49.591316 to best_finetuned_model.pth
Epoch 5/200
Train Loss: 1507.55588619, Train R²: -40.088692, Val Loss: 1511.74919712, Val R²: -47.785755
Saved best model with validation R2 -47.785755 to best_finetuned_model.pth
Epoch 6/200
Train Loss: 1473.39610085, Train R²: -38.247723, Val Loss: 1468.34413201, Val R²: -45.024517
Saved best model with validation R2 -45.024517 to best_finetuned_model.pth
Epoch 7/200
Train Loss: 1420.80988524, Train R²: -35.496174, Val Loss: 1402.55472621, Val R²: -40.992634
Saved best model with validation R2 -40.992634 to best_finetuned_model.pth
Epoch 8/200
Train Loss: 1342.72088313, Train R²: -31.594692, Val Loss: 1306.82455976, Val R²: -35.455921
Saved best model with validation R2 -35.455921 to best_finetuned_model.pth
Epoch 9/200
Train Loss: 1231.11319545, Train R²: -26.401316, Val Loss: 1173.81424851, Val R²: -28.412521
Saved best model with validation R2 -28.412521 to best_finetuned_model.pth
Epoch 10/200
Train Loss: 1080.60784978, Train R²: -20.111139, Val Loss: 996.90275980, Val R²: -20.214808
Saved best model with validation R2 -20.214808 to best_finetuned_model.pth
Epoch 11/200
Train Loss: 888.17345857, Train R²: -13.261693, Val Loss: 778.74268456, Val R²: -11.945579
Saved best model with validation R2 -11.945579 to best_finetuned_model.pth
Epoch 12/200
Train Loss: 660.99237420, Train R²: -6.898930, Val Loss: 544.51885241, Val R²: -5.329354
Saved best model with validation R2 -5.329354 to best_finetuned_model.pth
Epoch 13/200
Train Loss: 439.35840367, Train R²: -2.489903, Val Loss: 339.17367509, Val R²: -1.455714
Saved best model with validation R2 -1.455714 to best_finetuned_model.pth
Epoch 14/200
Train Loss: 280.17430624, Train R²: -0.419162, Val Loss: 219.76555974, Val R²: -0.030986
Saved best model with validation R2 -0.030986 to best_finetuned_model.pth
Epoch 15/200
Train Loss: 198.02894122, Train R²: 0.291022, Val Loss: 178.03916561, Val R²: 0.323349
Saved best model with validation R2 0.323349 to best_finetuned_model.pth
Epoch 16/200
Train Loss: 177.83093314, Train R²: 0.428271, Val Loss: 157.99721764, Val R²: 0.467116
Saved best model with validation R2 0.467116 to best_finetuned_model.pth
Epoch 17/200
Train Loss: 160.53366396, Train R²: 0.534084, Val Loss: 140.58392345, Val R²: 0.578105
Saved best model with validation R2 0.578105 to best_finetuned_model.pth
Epoch 18/200
Train Loss: 138.90664581, Train R²: 0.651164, Val Loss: 129.04652050, Val R²: 0.644511
Saved best model with validation R2 0.644511 to best_finetuned_model.pth
Epoch 19/200
Train Loss: 126.13789330, Train R²: 0.712349, Val Loss: 116.44576703, Val R²: 0.710545
Saved best model with validation R2 0.710545 to best_finetuned_model.pth
Epoch 20/200
Train Loss: 113.14056816, Train R²: 0.768574, Val Loss: 109.24564985, Val R²: 0.745234
Saved best model with validation R2 0.745234 to best_finetuned_model.pth
Epoch 21/200
Train Loss: 104.00290392, Train R²: 0.804446, Val Loss: 116.84564655, Val R²: 0.708554
Epoch 22/200
Train Loss: 104.23783362, Train R²: 0.803562, Val Loss: 108.95462762, Val R²: 0.746589
Saved best model with validation R2 0.746589 to best_finetuned_model.pth
Epoch 23/200
Train Loss: 90.92198558, Train R²: 0.850544, Val Loss: 94.52901588, Val R²: 0.809250
Saved best model with validation R2 0.809250 to best_finetuned_model.pth
Epoch 24/200
Train Loss: 79.79659928, Train R²: 0.884882, Val Loss: 91.79386617, Val R²: 0.820129
Saved best model with validation R2 0.820129 to best_finetuned_model.pth
Epoch 25/200
Train Loss: 71.53138570, Train R²: 0.907494, Val Loss: 85.90162945, Val R²: 0.842480
Saved best model with validation R2 0.842480 to best_finetuned_model.pth
Epoch 26/200
Train Loss: 64.23036923, Train R²: 0.925414, Val Loss: 80.40534438, Val R²: 0.861992
Saved best model with validation R2 0.861992 to best_finetuned_model.pth
Epoch 27/200
Train Loss: 60.99429597, Train R²: 0.932740, Val Loss: 78.09876935, Val R²: 0.869797
Saved best model with validation R2 0.869797 to best_finetuned_model.pth
Epoch 28/200
Train Loss: 56.47905343, Train R²: 0.942330, Val Loss: 77.74636610, Val R²: 0.870969
Saved best model with validation R2 0.870969 to best_finetuned_model.pth
Epoch 29/200
Train Loss: 50.92757840, Train R²: 0.953110, Val Loss: 72.98217854, Val R²: 0.886298
Saved best model with validation R2 0.886298 to best_finetuned_model.pth
Epoch 30/200
Train Loss: 46.75340374, Train R²: 0.960481, Val Loss: 71.25927256, Val R²: 0.891603
Saved best model with validation R2 0.891603 to best_finetuned_model.pth
Epoch 31/200
Train Loss: 43.14789972, Train R²: 0.966341, Val Loss: 69.23483892, Val R²: 0.897675
Saved best model with validation R2 0.897675 to best_finetuned_model.pth
Epoch 32/200
Train Loss: 42.32120675, Train R²: 0.967619, Val Loss: 74.19125135, Val R²: 0.882500
Epoch 33/200
Train Loss: 39.84538211, Train R²: 0.971297, Val Loss: 67.45132917, Val R²: 0.902879
Saved best model with validation R2 0.902879 to best_finetuned_model.pth
Epoch 34/200
Train Loss: 36.42186240, Train R²: 0.976017, Val Loss: 67.79101834, Val R²: 0.901898
Epoch 35/200
Train Loss: 37.48576618, Train R²: 0.974596, Val Loss: 66.99931403, Val R²: 0.904176
Saved best model with validation R2 0.904176 to best_finetuned_model.pth
Epoch 36/200
Train Loss: 43.49326243, Train R²: 0.965801, Val Loss: 72.70696550, Val R²: 0.887154
Epoch 37/200
Train Loss: 35.36744201, Train R²: 0.977386, Val Loss: 64.01274516, Val R²: 0.912529
Saved best model with validation R2 0.912529 to best_finetuned_model.pth
Epoch 38/200
Train Loss: 29.35833399, Train R²: 0.984417, Val Loss: 66.42274898, Val R²: 0.905818
Epoch 39/200
Train Loss: 27.17646932, Train R²: 0.986648, Val Loss: 61.56220642, Val R²: 0.919098
Saved best model with validation R2 0.919098 to best_finetuned_model.pth
Epoch 40/200
Train Loss: 26.15439258, Train R²: 0.987633, Val Loss: 61.82527113, Val R²: 0.918405
Epoch 41/200
Train Loss: 23.67039455, Train R²: 0.989871, Val Loss: 60.90489348, Val R²: 0.920816
Saved best model with validation R2 0.920816 to best_finetuned_model.pth
Epoch 42/200
Train Loss: 21.99135257, Train R²: 0.991257, Val Loss: 61.12993981, Val R²: 0.920230
Epoch 43/200
Train Loss: 21.72802900, Train R²: 0.991465, Val Loss: 59.92211196, Val R²: 0.923351
Saved best model with validation R2 0.923351 to best_finetuned_model.pth
Epoch 44/200
Train Loss: 20.67626087, Train R²: 0.992271, Val Loss: 60.78337107, Val R²: 0.921132
Epoch 45/200
Train Loss: 18.23277086, Train R²: 0.993990, Val Loss: 60.69562993, Val R²: 0.921359
Epoch 46/200
Train Loss: 18.23288201, Train R²: 0.993990, Val Loss: 61.63406532, Val R²: 0.918909
Epoch 47/200
Train Loss: 17.17284597, Train R²: 0.994668, Val Loss: 60.00054500, Val R²: 0.923150
Epoch 48/200
Train Loss: 15.57447934, Train R²: 0.995615, Val Loss: 59.20919238, Val R²: 0.925164
Saved best model with validation R2 0.925164 to best_finetuned_model.pth
Epoch 49/200
Train Loss: 14.66528813, Train R²: 0.996112, Val Loss: 59.95726856, Val R²: 0.923261
Epoch 50/200
Train Loss: 13.57206063, Train R²: 0.996670, Val Loss: 59.65787714, Val R²: 0.924025
Epoch 51/200
Train Loss: 13.98239168, Train R²: 0.996465, Val Loss: 59.40990799, Val R²: 0.924656
Epoch 52/200
Train Loss: 12.33098659, Train R²: 0.997251, Val Loss: 58.10416940, Val R²: 0.927931
Saved best model with validation R2 0.927931 to best_finetuned_model.pth
Epoch 53/200
Train Loss: 12.36200767, Train R²: 0.997237, Val Loss: 60.40192011, Val R²: 0.922118
Epoch 54/200
Train Loss: 11.27617835, Train R²: 0.997701, Val Loss: 59.54725400, Val R²: 0.924307
Epoch 55/200
Train Loss: 11.50100171, Train R²: 0.997609, Val Loss: 58.95339422, Val R²: 0.925809
Epoch 56/200
Train Loss: 10.89665864, Train R²: 0.997853, Val Loss: 59.62179957, Val R²: 0.924117
Epoch 57/200
Train Loss: 11.17939318, Train R²: 0.997741, Val Loss: 58.45571031, Val R²: 0.927056
Epoch 58/200
Train Loss: 10.36262004, Train R²: 0.998059, Val Loss: 59.44965960, Val R²: 0.924555
Epoch 59/200
Train Loss: 10.00146555, Train R²: 0.998192, Val Loss: 60.24874067, Val R²: 0.922513
Epoch 60/200
Train Loss: 9.62857012, Train R²: 0.998324, Val Loss: 58.82160356, Val R²: 0.926140
Epoch 61/200
Train Loss: 8.63698958, Train R²: 0.998651, Val Loss: 58.73370731, Val R²: 0.926361
Epoch 62/200
Train Loss: 8.97702819, Train R²: 0.998543, Val Loss: 59.80186328, Val R²: 0.923658
Epoch 63/200
Train Loss: 11.22857369, Train R²: 0.997721, Val Loss: 59.34756661, Val R²: 0.924814
Epoch 00063: reducing learning rate of group 0 to 5.0000e-05.
Epoch 64/200
Train Loss: 10.27637865, Train R²: 0.998091, Val Loss: 58.70004637, Val R²: 0.926445
Epoch 65/200
Train Loss: 9.72933592, Train R²: 0.998289, Val Loss: 59.11225480, Val R²: 0.925409
Epoch 66/200
Train Loss: 9.90243228, Train R²: 0.998227, Val Loss: 59.38095550, Val R²: 0.924729
Epoch 67/200
Train Loss: 8.13042462, Train R²: 0.998805, Val Loss: 59.10355675, Val R²: 0.925431
Epoch 68/200
Train Loss: 7.02095806, Train R²: 0.999109, Val Loss: 59.27182380, Val R²: 0.925005
Epoch 69/200
Train Loss: 6.65425074, Train R²: 0.999199, Val Loss: 59.29046713, Val R²: 0.924958
Epoch 70/200
Train Loss: 6.38034465, Train R²: 0.999264, Val Loss: 59.01272278, Val R²: 0.925660
Epoch 71/200
Train Loss: 6.19160338, Train R²: 0.999307, Val Loss: 59.16118954, Val R²: 0.925285
Epoch 72/200
Train Loss: 6.04366213, Train R²: 0.999340, Val Loss: 59.06289472, Val R²: 0.925533
Epoch 73/200
Train Loss: 5.96191748, Train R²: 0.999357, Val Loss: 59.09163749, Val R²: 0.925461
Epoch 74/200
Train Loss: 5.81954805, Train R²: 0.999388, Val Loss: 59.15396774, Val R²: 0.925303
Epoch 00074: reducing learning rate of group 0 to 2.5000e-05.
Epoch 75/200
Train Loss: 5.73759313, Train R²: 0.999405, Val Loss: 59.00939225, Val R²: 0.925668
Epoch 76/200
Train Loss: 5.66520193, Train R²: 0.999420, Val Loss: 59.10999595, Val R²: 0.925414
Epoch 77/200
Train Loss: 5.59295541, Train R²: 0.999434, Val Loss: 59.05344859, Val R²: 0.925557
Epoch 78/200
Train Loss: 5.53740353, Train R²: 0.999446, Val Loss: 59.02865929, Val R²: 0.925619
Epoch 79/200
Train Loss: 5.49231146, Train R²: 0.999455, Val Loss: 59.06415333, Val R²: 0.925530
Epoch 80/200
Train Loss: 5.45898152, Train R²: 0.999461, Val Loss: 59.07392986, Val R²: 0.925505
Epoch 81/200
Train Loss: 5.39726306, Train R²: 0.999473, Val Loss: 59.02831927, Val R²: 0.925620
Epoch 82/200
Train Loss: 5.35383732, Train R²: 0.999482, Val Loss: 59.03671676, Val R²: 0.925599
Epoch 83/200
Train Loss: 5.32884280, Train R²: 0.999487, Val Loss: 59.01605569, Val R²: 0.925651
Epoch 84/200
Train Loss: 5.28991856, Train R²: 0.999494, Val Loss: 59.07271448, Val R²: 0.925508
Epoch 85/200
Train Loss: 5.24545214, Train R²: 0.999503, Val Loss: 59.08754814, Val R²: 0.925471
Epoch 00085: reducing learning rate of group 0 to 1.2500e-05.
Epoch 86/200
Train Loss: 5.19088854, Train R²: 0.999513, Val Loss: 59.08583674, Val R²: 0.925475
Epoch 87/200
Train Loss: 5.17179319, Train R²: 0.999516, Val Loss: 59.04702906, Val R²: 0.925573
Epoch 88/200
Train Loss: 5.15559158, Train R²: 0.999519, Val Loss: 59.08483097, Val R²: 0.925478
Epoch 89/200
Train Loss: 5.13287366, Train R²: 0.999524, Val Loss: 59.03884339, Val R²: 0.925594
Epoch 90/200
Train Loss: 5.11750144, Train R²: 0.999527, Val Loss: 59.06677874, Val R²: 0.925523
Epoch 91/200
Train Loss: 5.09460902, Train R²: 0.999531, Val Loss: 59.06495477, Val R²: 0.925528
Epoch 92/200
Train Loss: 5.07266082, Train R²: 0.999535, Val Loss: 59.08585344, Val R²: 0.925475
Epoch 93/200
Train Loss: 5.06387236, Train R²: 0.999536, Val Loss: 59.06942019, Val R²: 0.925517
Epoch 94/200
Train Loss: 5.03854508, Train R²: 0.999541, Val Loss: 59.04993658, Val R²: 0.925566
Epoch 95/200
Train Loss: 5.02347291, Train R²: 0.999544, Val Loss: 59.09114926, Val R²: 0.925462
Epoch 96/200
Train Loss: 5.00882705, Train R²: 0.999546, Val Loss: 59.05167688, Val R²: 0.925561
Epoch 00096: reducing learning rate of group 0 to 6.2500e-06.
Epoch 97/200
Train Loss: 4.98867106, Train R²: 0.999550, Val Loss: 59.06513259, Val R²: 0.925528
Epoch 98/200
Train Loss: 4.97568220, Train R²: 0.999552, Val Loss: 59.09158250, Val R²: 0.925461
Epoch 99/200
Train Loss: 4.96390753, Train R²: 0.999555, Val Loss: 59.07949875, Val R²: 0.925491
Epoch 100/200
Train Loss: 4.95810980, Train R²: 0.999556, Val Loss: 59.08546887, Val R²: 0.925476
Epoch 101/200
Train Loss: 4.94775203, Train R²: 0.999557, Val Loss: 59.08212699, Val R²: 0.925485
Epoch 102/200
Train Loss: 4.93682647, Train R²: 0.999559, Val Loss: 59.06468552, Val R²: 0.925529
Epoch 103/200
Train Loss: 4.92690381, Train R²: 0.999561, Val Loss: 59.07134177, Val R²: 0.925512
Epoch 104/200
Train Loss: 4.91603940, Train R²: 0.999563, Val Loss: 59.08078388, Val R²: 0.925488
Epoch 105/200
Train Loss: 4.90505684, Train R²: 0.999565, Val Loss: 59.07759984, Val R²: 0.925496
Epoch 106/200
Train Loss: 4.89762401, Train R²: 0.999566, Val Loss: 59.08096037, Val R²: 0.925488
Epoch 107/200
Train Loss: 4.88784051, Train R²: 0.999568, Val Loss: 59.08932476, Val R²: 0.925466
Epoch 00107: reducing learning rate of group 0 to 3.1250e-06.
Epoch 108/200
Train Loss: 4.87760374, Train R²: 0.999570, Val Loss: 59.07855875, Val R²: 0.925494
Epoch 109/200
Train Loss: 4.87235575, Train R²: 0.999571, Val Loss: 59.08752892, Val R²: 0.925471
Epoch 110/200
Train Loss: 4.86654235, Train R²: 0.999572, Val Loss: 59.08109673, Val R²: 0.925487
Epoch 111/200
Train Loss: 4.86348805, Train R²: 0.999572, Val Loss: 59.08255686, Val R²: 0.925484
Epoch 112/200
Train Loss: 4.85887731, Train R²: 0.999573, Val Loss: 59.08899786, Val R²: 0.925467
Epoch 113/200
Train Loss: 4.85276023, Train R²: 0.999574, Val Loss: 59.09324258, Val R²: 0.925457
Epoch 114/200
Train Loss: 4.84758272, Train R²: 0.999575, Val Loss: 59.09588032, Val R²: 0.925450
Epoch 115/200
Train Loss: 4.84465196, Train R²: 0.999576, Val Loss: 59.09011966, Val R²: 0.925465
Epoch 116/200
Train Loss: 4.84033950, Train R²: 0.999576, Val Loss: 59.08593971, Val R²: 0.925475
Epoch 117/200
Train Loss: 4.83462320, Train R²: 0.999577, Val Loss: 59.08485977, Val R²: 0.925478
Epoch 118/200
Train Loss: 4.82889507, Train R²: 0.999578, Val Loss: 59.08261996, Val R²: 0.925483
Epoch 00118: reducing learning rate of group 0 to 1.5625e-06.
Epoch 119/200
Train Loss: 4.82527779, Train R²: 0.999579, Val Loss: 59.09584818, Val R²: 0.925450
Epoch 120/200
Train Loss: 4.82263231, Train R²: 0.999580, Val Loss: 59.08949230, Val R²: 0.925466
Epoch 121/200
Train Loss: 4.82011610, Train R²: 0.999580, Val Loss: 59.09192323, Val R²: 0.925460
Epoch 122/200
Train Loss: 4.81714574, Train R²: 0.999581, Val Loss: 59.09329699, Val R²: 0.925456
Epoch 123/200
Train Loss: 4.81522854, Train R²: 0.999581, Val Loss: 59.09402585, Val R²: 0.925455
Epoch 124/200
Train Loss: 4.81284905, Train R²: 0.999581, Val Loss: 59.09721425, Val R²: 0.925447
Epoch 125/200
Train Loss: 4.81062857, Train R²: 0.999582, Val Loss: 59.09249238, Val R²: 0.925458
Epoch 126/200
Train Loss: 4.80757379, Train R²: 0.999582, Val Loss: 59.09628126, Val R²: 0.925449
Epoch 127/200
Train Loss: 4.80500225, Train R²: 0.999583, Val Loss: 59.09424949, Val R²: 0.925454
Epoch 128/200
Train Loss: 4.80288489, Train R²: 0.999583, Val Loss: 59.09427002, Val R²: 0.925454
Epoch 129/200
Train Loss: 4.80023470, Train R²: 0.999583, Val Loss: 59.09280068, Val R²: 0.925458
Epoch 00129: reducing learning rate of group 0 to 7.8125e-07.
Epoch 130/200
Train Loss: 4.79793260, Train R²: 0.999584, Val Loss: 59.09184828, Val R²: 0.925460
Epoch 131/200
Train Loss: 4.79672144, Train R²: 0.999584, Val Loss: 59.09283584, Val R²: 0.925458
Epoch 132/200
Train Loss: 4.79574569, Train R²: 0.999584, Val Loss: 59.09382453, Val R²: 0.925455
Epoch 133/200
Train Loss: 4.79403372, Train R²: 0.999584, Val Loss: 59.09476000, Val R²: 0.925453
Epoch 134/200
Train Loss: 4.79250947, Train R²: 0.999585, Val Loss: 59.09127622, Val R²: 0.925462
Epoch 135/200
Train Loss: 4.79146483, Train R²: 0.999585, Val Loss: 59.09508162, Val R²: 0.925452
Epoch 136/200
Train Loss: 4.79020543, Train R²: 0.999585, Val Loss: 59.08990861, Val R²: 0.925465
Epoch 137/200
Train Loss: 4.78879436, Train R²: 0.999585, Val Loss: 59.09418272, Val R²: 0.925454
Epoch 138/200
Train Loss: 4.78720807, Train R²: 0.999586, Val Loss: 59.09339185, Val R²: 0.925456
Epoch 139/200
Train Loss: 4.78603851, Train R²: 0.999586, Val Loss: 59.09223309, Val R²: 0.925459
Epoch 140/200
Train Loss: 4.78428420, Train R²: 0.999586, Val Loss: 59.09597671, Val R²: 0.925450
Epoch 00140: reducing learning rate of group 0 to 3.9063e-07.
Epoch 141/200
Train Loss: 4.78339045, Train R²: 0.999586, Val Loss: 59.09572222, Val R²: 0.925450
Epoch 142/200
Train Loss: 4.78260402, Train R²: 0.999586, Val Loss: 59.09352517, Val R²: 0.925456
Epoch 143/200
Train Loss: 4.78221826, Train R²: 0.999587, Val Loss: 59.09275540, Val R²: 0.925458
Epoch 144/200
Train Loss: 4.78154782, Train R²: 0.999587, Val Loss: 59.09468605, Val R²: 0.925453
Epoch 145/200
Train Loss: 4.78104426, Train R²: 0.999587, Val Loss: 59.09540531, Val R²: 0.925451
Epoch 146/200
Train Loss: 4.78053530, Train R²: 0.999587, Val Loss: 59.09431071, Val R²: 0.925454
Epoch 147/200
Train Loss: 4.78008456, Train R²: 0.999587, Val Loss: 59.09505200, Val R²: 0.925452
Epoch 148/200
Train Loss: 4.77938178, Train R²: 0.999587, Val Loss: 59.09247098, Val R²: 0.925459
Epoch 149/200
Train Loss: 4.77885157, Train R²: 0.999587, Val Loss: 59.09621099, Val R²: 0.925449
Epoch 150/200
Train Loss: 4.77819595, Train R²: 0.999587, Val Loss: 59.09515053, Val R²: 0.925452
Epoch 151/200
Train Loss: 4.77751652, Train R²: 0.999587, Val Loss: 59.09425225, Val R²: 0.925454
Epoch 00151: reducing learning rate of group 0 to 1.9531e-07.
Epoch 152/200
Train Loss: 4.77719404, Train R²: 0.999587, Val Loss: 59.09482102, Val R²: 0.925453
Epoch 153/200
Train Loss: 4.77700822, Train R²: 0.999587, Val Loss: 59.09453368, Val R²: 0.925453
Epoch 154/200
Train Loss: 4.77681373, Train R²: 0.999587, Val Loss: 59.09522229, Val R²: 0.925452
Epoch 155/200
Train Loss: 4.77666079, Train R²: 0.999587, Val Loss: 59.09657631, Val R²: 0.925448
Epoch 156/200
Train Loss: 4.77632397, Train R²: 0.999588, Val Loss: 59.09530166, Val R²: 0.925451
Epoch 157/200
Train Loss: 4.77612490, Train R²: 0.999588, Val Loss: 59.09500428, Val R²: 0.925452
Epoch 158/200
Train Loss: 4.77590235, Train R²: 0.999588, Val Loss: 59.09462193, Val R²: 0.925453
Epoch 159/200
Train Loss: 4.77570487, Train R²: 0.999588, Val Loss: 59.09570652, Val R²: 0.925450
Epoch 160/200
Train Loss: 4.77546808, Train R²: 0.999588, Val Loss: 59.09568520, Val R²: 0.925450
Epoch 161/200
Train Loss: 4.77529579, Train R²: 0.999588, Val Loss: 59.09605371, Val R²: 0.925449
Epoch 162/200
Train Loss: 4.77525146, Train R²: 0.999588, Val Loss: 59.09499433, Val R²: 0.925452
Epoch 00162: reducing learning rate of group 0 to 9.7656e-08.
Epoch 163/200
Train Loss: 4.77489902, Train R²: 0.999588, Val Loss: 59.09527786, Val R²: 0.925451
Epoch 164/200
Train Loss: 4.77478915, Train R²: 0.999588, Val Loss: 59.09530818, Val R²: 0.925451
Epoch 165/200
Train Loss: 4.77473833, Train R²: 0.999588, Val Loss: 59.09544249, Val R²: 0.925451
Epoch 166/200
Train Loss: 4.77464603, Train R²: 0.999588, Val Loss: 59.09547876, Val R²: 0.925451
Epoch 167/200
Train Loss: 4.77457205, Train R²: 0.999588, Val Loss: 59.09561766, Val R²: 0.925451
Epoch 168/200
Train Loss: 4.77450039, Train R²: 0.999588, Val Loss: 59.09553425, Val R²: 0.925451
Epoch 169/200
Train Loss: 4.77442420, Train R²: 0.999588, Val Loss: 59.09566277, Val R²: 0.925451
Epoch 170/200
Train Loss: 4.77434718, Train R²: 0.999588, Val Loss: 59.09576027, Val R²: 0.925450
Epoch 171/200
Train Loss: 4.77425890, Train R²: 0.999588, Val Loss: 59.09579559, Val R²: 0.925450
Epoch 172/200
Train Loss: 4.77417092, Train R²: 0.999588, Val Loss: 59.09565430, Val R²: 0.925451
Epoch 173/200
Train Loss: 4.77407832, Train R²: 0.999588, Val Loss: 59.09567145, Val R²: 0.925451
Epoch 00173: reducing learning rate of group 0 to 4.8828e-08.
Epoch 174/200
Train Loss: 4.77400354, Train R²: 0.999588, Val Loss: 59.09565166, Val R²: 0.925451
Epoch 175/200
Train Loss: 4.77398448, Train R²: 0.999588, Val Loss: 59.09566884, Val R²: 0.925451
Epoch 176/200
Train Loss: 4.77396016, Train R²: 0.999588, Val Loss: 59.09560526, Val R²: 0.925451
Epoch 177/200
Train Loss: 4.77393232, Train R²: 0.999588, Val Loss: 59.09565335, Val R²: 0.925451
Epoch 178/200
Train Loss: 4.77391670, Train R²: 0.999588, Val Loss: 59.09567880, Val R²: 0.925450
Epoch 179/200
Train Loss: 4.77389880, Train R²: 0.999588, Val Loss: 59.09573226, Val R²: 0.925450
Epoch 180/200
Train Loss: 4.77386876, Train R²: 0.999588, Val Loss: 59.09568297, Val R²: 0.925450
Epoch 181/200
Train Loss: 4.77384873, Train R²: 0.999588, Val Loss: 59.09571697, Val R²: 0.925450
Epoch 182/200
Train Loss: 4.77382266, Train R²: 0.999588, Val Loss: 59.09583616, Val R²: 0.925450
Epoch 183/200
Train Loss: 4.77378726, Train R²: 0.999588, Val Loss: 59.09583397, Val R²: 0.925450
Epoch 184/200
Train Loss: 4.77378396, Train R²: 0.999588, Val Loss: 59.09584335, Val R²: 0.925450
Epoch 00184: reducing learning rate of group 0 to 2.4414e-08.
Epoch 185/200
Train Loss: 4.77376173, Train R²: 0.999588, Val Loss: 59.09586566, Val R²: 0.925450
Epoch 186/200
Train Loss: 4.77374024, Train R²: 0.999588, Val Loss: 59.09585773, Val R²: 0.925450
Epoch 187/200
Train Loss: 4.77373081, Train R²: 0.999588, Val Loss: 59.09587673, Val R²: 0.925450
Epoch 188/200
Train Loss: 4.77372152, Train R²: 0.999588, Val Loss: 59.09588557, Val R²: 0.925450
Epoch 189/200
Train Loss: 4.77372542, Train R²: 0.999588, Val Loss: 59.09586368, Val R²: 0.925450
Epoch 190/200
Train Loss: 4.77371323, Train R²: 0.999588, Val Loss: 59.09587578, Val R²: 0.925450
Epoch 191/200
Train Loss: 4.77370850, Train R²: 0.999588, Val Loss: 59.09586752, Val R²: 0.925450
Epoch 192/200
Train Loss: 4.77370266, Train R²: 0.999588, Val Loss: 59.09586033, Val R²: 0.925450
Epoch 193/200
Train Loss: 4.77369593, Train R²: 0.999588, Val Loss: 59.09588925, Val R²: 0.925450
Epoch 194/200
Train Loss: 4.77369577, Train R²: 0.999588, Val Loss: 59.09591532, Val R²: 0.925450
Epoch 195/200
Train Loss: 4.77367748, Train R²: 0.999588, Val Loss: 59.09592907, Val R²: 0.925450
Epoch 00195: reducing learning rate of group 0 to 1.2207e-08.
Epoch 196/200
Train Loss: 4.77366642, Train R²: 0.999588, Val Loss: 59.09592535, Val R²: 0.925450
Epoch 197/200
Train Loss: 4.77367317, Train R²: 0.999588, Val Loss: 59.09592440, Val R²: 0.925450
Epoch 198/200
Train Loss: 4.77366387, Train R²: 0.999588, Val Loss: 59.09592399, Val R²: 0.925450
Epoch 199/200
Train Loss: 4.77367289, Train R²: 0.999588, Val Loss: 59.09591490, Val R²: 0.925450
Epoch 200/200
Train Loss: 4.77366403, Train R²: 0.999588, Val Loss: 59.09590639, Val R²: 0.925450
Training Complete. Best Val Loss: 58.10416940248888
训练时间: 173.53 秒
