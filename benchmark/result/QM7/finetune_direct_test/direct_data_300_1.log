Using device: cuda
Total samples: 300, Training: 240, Validation: 60
Total trainable parameters: 4577537
Epoch 1/200
Train Loss: 1545.25134417, Train R²: -46.765911, Val Loss: 1572.40524357, Val R²: -42.083744
Saved best model with validation R2 -42.083744 to best_finetuned_model.pth
Epoch 2/200
Train Loss: 1544.62117794, Train R²: -46.726959, Val Loss: 1571.66376811, Val R²: -42.043129
Saved best model with validation R2 -42.043129 to best_finetuned_model.pth
Epoch 3/200
Train Loss: 1543.77326163, Train R²: -46.674572, Val Loss: 1570.62304516, Val R²: -41.986145
Saved best model with validation R2 -41.986145 to best_finetuned_model.pth
Epoch 4/200
Train Loss: 1542.64976042, Train R²: -46.605213, Val Loss: 1569.35129910, Val R²: -41.916550
Saved best model with validation R2 -41.916550 to best_finetuned_model.pth
Epoch 5/200
Train Loss: 1541.31235208, Train R²: -46.522705, Val Loss: 1567.90090567, Val R²: -41.837265
Saved best model with validation R2 -41.837265 to best_finetuned_model.pth
Epoch 6/200
Train Loss: 1539.79066218, Train R²: -46.428921, Val Loss: 1566.24702713, Val R²: -41.746941
Saved best model with validation R2 -41.746941 to best_finetuned_model.pth
Epoch 7/200
Train Loss: 1538.02609969, Train R²: -46.320274, Val Loss: 1564.30535702, Val R²: -41.641022
Saved best model with validation R2 -41.641022 to best_finetuned_model.pth
Epoch 8/200
Train Loss: 1535.96880394, Train R²: -46.193768, Val Loss: 1562.02096657, Val R²: -41.516567
Saved best model with validation R2 -41.516567 to best_finetuned_model.pth
Epoch 9/200
Train Loss: 1533.54509770, Train R²: -46.044937, Val Loss: 1559.33110660, Val R²: -41.370270
Saved best model with validation R2 -41.370270 to best_finetuned_model.pth
Epoch 10/200
Train Loss: 1530.66529109, Train R²: -45.868420, Val Loss: 1556.14122110, Val R²: -41.197102
Saved best model with validation R2 -41.197102 to best_finetuned_model.pth
Epoch 11/200
Train Loss: 1527.24728406, Train R²: -45.659332, Val Loss: 1552.39677596, Val R²: -40.994263
Saved best model with validation R2 -40.994263 to best_finetuned_model.pth
Epoch 12/200
Train Loss: 1523.22115378, Train R²: -45.413651, Val Loss: 1547.95655624, Val R²: -40.754391
Saved best model with validation R2 -40.754391 to best_finetuned_model.pth
Epoch 13/200
Train Loss: 1518.46933895, Train R²: -45.124519, Val Loss: 1542.79308399, Val R²: -40.476292
Saved best model with validation R2 -40.476292 to best_finetuned_model.pth
Epoch 14/200
Train Loss: 1512.94332346, Train R²: -44.789417, Val Loss: 1536.69743281, Val R²: -40.149185
Saved best model with validation R2 -40.149185 to best_finetuned_model.pth
Epoch 15/200
Train Loss: 1506.44340529, Train R²: -44.396820, Val Loss: 1529.64538374, Val R²: -39.772381
Saved best model with validation R2 -39.772381 to best_finetuned_model.pth
Epoch 16/200
Train Loss: 1498.90431649, Train R²: -43.943577, Val Loss: 1521.38193758, Val R²: -39.333046
Saved best model with validation R2 -39.333046 to best_finetuned_model.pth
Epoch 17/200
Train Loss: 1490.10468872, Train R²: -43.417427, Val Loss: 1511.85506911, Val R²: -38.829498
Saved best model with validation R2 -38.829498 to best_finetuned_model.pth
Epoch 18/200
Train Loss: 1479.98879500, Train R²: -42.816399, Val Loss: 1500.80070296, Val R²: -38.249184
Saved best model with validation R2 -38.249184 to best_finetuned_model.pth
Epoch 19/200
Train Loss: 1468.29058205, Train R²: -42.126465, Val Loss: 1488.10575901, Val R²: -37.587982
Saved best model with validation R2 -37.587982 to best_finetuned_model.pth
Epoch 20/200
Train Loss: 1454.84843999, Train R²: -41.340431, Val Loss: 1473.47276867, Val R²: -36.832825
Saved best model with validation R2 -36.832825 to best_finetuned_model.pth
Epoch 21/200
Train Loss: 1439.38957548, Train R²: -40.445419, Val Loss: 1456.72037468, Val R²: -35.977444
Saved best model with validation R2 -35.977444 to best_finetuned_model.pth
Epoch 22/200
Train Loss: 1421.64113498, Train R²: -39.429630, Val Loss: 1437.75163015, Val R²: -35.020706
Saved best model with validation R2 -35.020706 to best_finetuned_model.pth
Epoch 23/200
Train Loss: 1401.66585414, Train R²: -38.301472, Val Loss: 1416.15310789, Val R²: -33.946598
Saved best model with validation R2 -33.946598 to best_finetuned_model.pth
Epoch 24/200
Train Loss: 1378.83176699, Train R²: -37.031406, Val Loss: 1391.82941663, Val R²: -32.756435
Saved best model with validation R2 -32.756435 to best_finetuned_model.pth
Epoch 25/200
Train Loss: 1353.25716021, Train R²: -35.633675, Val Loss: 1364.37036761, Val R²: -31.437630
Saved best model with validation R2 -31.437630 to best_finetuned_model.pth
Epoch 26/200
Train Loss: 1324.56518966, Train R²: -34.096714, Val Loss: 1333.65194110, Val R²: -29.993425
Saved best model with validation R2 -29.993425 to best_finetuned_model.pth
Epoch 27/200
Train Loss: 1292.36920034, Train R²: -32.411274, Val Loss: 1299.47369154, Val R²: -28.425203
Saved best model with validation R2 -28.425203 to best_finetuned_model.pth
Epoch 28/200
Train Loss: 1256.78590394, Train R²: -30.596748, Val Loss: 1261.46531859, Val R²: -26.729061
Saved best model with validation R2 -26.729061 to best_finetuned_model.pth
Epoch 29/200
Train Loss: 1217.50516768, Train R²: -28.652506, Val Loss: 1219.89620460, Val R²: -24.931656
Saved best model with validation R2 -24.931656 to best_finetuned_model.pth
Epoch 30/200
Train Loss: 1173.91482158, Train R²: -26.567219, Val Loss: 1174.34252882, Val R²: -23.031124
Saved best model with validation R2 -23.031124 to best_finetuned_model.pth
Epoch 31/200
Train Loss: 1126.42756832, Train R²: -24.382027, Val Loss: 1124.18314789, Val R²: -21.022095
Saved best model with validation R2 -21.022095 to best_finetuned_model.pth
Epoch 32/200
Train Loss: 1074.74051132, Train R²: -22.106121, Val Loss: 1070.79538195, Val R²: -18.980093
Saved best model with validation R2 -18.980093 to best_finetuned_model.pth
Epoch 33/200
Train Loss: 1018.84571943, Train R²: -19.765226, Val Loss: 1012.74725376, Val R²: -16.872555
Saved best model with validation R2 -16.872555 to best_finetuned_model.pth
Epoch 34/200
Train Loss: 959.29829607, Train R²: -17.408871, Val Loss: 951.10853087, Val R²: -14.763211
Saved best model with validation R2 -14.763211 to best_finetuned_model.pth
Epoch 35/200
Train Loss: 895.73495894, Train R²: -15.050140, Val Loss: 885.03117883, Val R²: -12.649025
Saved best model with validation R2 -12.649025 to best_finetuned_model.pth
Epoch 36/200
Train Loss: 827.31976134, Train R²: -12.691991, Val Loss: 816.85995587, Val R²: -10.627321
Saved best model with validation R2 -10.627321 to best_finetuned_model.pth
Epoch 37/200
Train Loss: 757.28495572, Train R²: -10.471984, Val Loss: 745.62079840, Val R²: -8.687697
Saved best model with validation R2 -8.687697 to best_finetuned_model.pth
Epoch 38/200
Train Loss: 683.96169026, Train R²: -8.358007, Val Loss: 673.27209396, Val R²: -6.898885
Saved best model with validation R2 -6.898885 to best_finetuned_model.pth
Epoch 39/200
Train Loss: 611.53868268, Train R²: -6.481137, Val Loss: 600.09879395, Val R²: -5.275235
Saved best model with validation R2 -5.275235 to best_finetuned_model.pth
Epoch 40/200
Train Loss: 538.52614259, Train R²: -4.801407, Val Loss: 528.10329837, Val R²: -3.859843
Saved best model with validation R2 -3.859843 to best_finetuned_model.pth
Epoch 41/200
Train Loss: 467.69589701, Train R²: -3.375694, Val Loss: 460.80596852, Val R²: -2.700162
Saved best model with validation R2 -2.700162 to best_finetuned_model.pth
Epoch 42/200
Train Loss: 402.32508363, Train R²: -2.237979, Val Loss: 399.46354652, Val R²: -1.780602
Saved best model with validation R2 -1.780602 to best_finetuned_model.pth
Epoch 43/200
Train Loss: 344.38273434, Train R²: -1.372480, Val Loss: 346.17162922, Val R²: -1.088178
Saved best model with validation R2 -1.088178 to best_finetuned_model.pth
Epoch 44/200
Train Loss: 294.96451323, Train R²: -0.740441, Val Loss: 307.25364878, Val R²: -0.645048
Saved best model with validation R2 -0.645048 to best_finetuned_model.pth
Epoch 45/200
Train Loss: 253.95119333, Train R²: -0.290091, Val Loss: 276.05308310, Val R²: -0.327914
Saved best model with validation R2 -0.327914 to best_finetuned_model.pth
Epoch 46/200
Train Loss: 222.96985035, Train R²: 0.005483, Val Loss: 258.37428943, Val R²: -0.163277
Saved best model with validation R2 -0.163277 to best_finetuned_model.pth
Epoch 47/200
Train Loss: 200.65134303, Train R²: 0.194614, Val Loss: 236.61789312, Val R²: 0.024382
Saved best model with validation R2 0.024382 to best_finetuned_model.pth
Epoch 48/200
Train Loss: 181.63798990, Train R²: 0.340016, Val Loss: 225.38935062, Val R²: 0.114780
Saved best model with validation R2 0.114780 to best_finetuned_model.pth
Epoch 49/200
Train Loss: 164.71355344, Train R²: 0.457277, Val Loss: 214.91021926, Val R²: 0.195180
Saved best model with validation R2 0.195180 to best_finetuned_model.pth
Epoch 50/200
Train Loss: 152.41632096, Train R²: 0.535289, Val Loss: 210.58310043, Val R²: 0.227263
Saved best model with validation R2 0.227263 to best_finetuned_model.pth
Epoch 51/200
Train Loss: 134.18371826, Train R²: 0.639820, Val Loss: 197.97830729, Val R²: 0.317002
Saved best model with validation R2 0.317002 to best_finetuned_model.pth
Epoch 52/200
Train Loss: 120.97198851, Train R²: 0.707255, Val Loss: 190.14198026, Val R²: 0.370000
Saved best model with validation R2 0.370000 to best_finetuned_model.pth
Epoch 53/200
Train Loss: 111.33403106, Train R²: 0.752043, Val Loss: 185.23011449, Val R²: 0.402129
Saved best model with validation R2 0.402129 to best_finetuned_model.pth
Epoch 54/200
Train Loss: 104.27424247, Train R²: 0.782493, Val Loss: 177.32826750, Val R²: 0.452051
Saved best model with validation R2 0.452051 to best_finetuned_model.pth
Epoch 55/200
Train Loss: 96.43491808, Train R²: 0.813968, Val Loss: 170.95030231, Val R²: 0.490758
Saved best model with validation R2 0.490758 to best_finetuned_model.pth
Epoch 56/200
Train Loss: 92.47275218, Train R²: 0.828940, Val Loss: 172.28068599, Val R²: 0.482801
Epoch 57/200
Train Loss: 89.65771573, Train R²: 0.839197, Val Loss: 169.04415913, Val R²: 0.502051
Saved best model with validation R2 0.502051 to best_finetuned_model.pth
Epoch 58/200
Train Loss: 87.25418186, Train R²: 0.847703, Val Loss: 171.99106309, Val R²: 0.484538
Epoch 59/200
Train Loss: 83.31743976, Train R²: 0.861135, Val Loss: 167.49348751, Val R²: 0.511145
Saved best model with validation R2 0.511145 to best_finetuned_model.pth
Epoch 60/200
Train Loss: 80.32799705, Train R²: 0.870922, Val Loss: 166.45317120, Val R²: 0.517198
Saved best model with validation R2 0.517198 to best_finetuned_model.pth
Epoch 61/200
Train Loss: 80.80958239, Train R²: 0.869369, Val Loss: 169.30909696, Val R²: 0.500489
Epoch 62/200
Train Loss: 76.36793944, Train R²: 0.883335, Val Loss: 166.61098028, Val R²: 0.516283
Epoch 63/200
Train Loss: 74.29683327, Train R²: 0.889577, Val Loss: 167.45281518, Val R²: 0.511382
Epoch 64/200
Train Loss: 71.97463158, Train R²: 0.896372, Val Loss: 169.62132688, Val R²: 0.498645
Epoch 65/200
Train Loss: 67.87699962, Train R²: 0.907835, Val Loss: 163.98809027, Val R²: 0.531393
Saved best model with validation R2 0.531393 to best_finetuned_model.pth
Epoch 66/200
Train Loss: 63.25267668, Train R²: 0.919965, Val Loss: 162.45576922, Val R²: 0.540109
Saved best model with validation R2 0.540109 to best_finetuned_model.pth
Epoch 67/200
Train Loss: 60.29136571, Train R²: 0.927284, Val Loss: 163.67020360, Val R²: 0.533208
Epoch 68/200
Train Loss: 58.25911244, Train R²: 0.932104, Val Loss: 163.60001480, Val R²: 0.533608
Epoch 69/200
Train Loss: 55.83369099, Train R²: 0.937639, Val Loss: 162.27734145, Val R²: 0.541119
Saved best model with validation R2 0.541119 to best_finetuned_model.pth
Epoch 70/200
Train Loss: 54.27085213, Train R²: 0.941081, Val Loss: 161.74323788, Val R²: 0.544134
Saved best model with validation R2 0.544134 to best_finetuned_model.pth
Epoch 71/200
Train Loss: 52.56967335, Train R²: 0.944717, Val Loss: 162.03629151, Val R²: 0.542481
Epoch 72/200
Train Loss: 51.34328575, Train R²: 0.947266, Val Loss: 162.43341785, Val R²: 0.540236
Epoch 73/200
Train Loss: 49.83853748, Train R²: 0.950312, Val Loss: 162.28707204, Val R²: 0.541064
Epoch 74/200
Train Loss: 48.64419918, Train R²: 0.952665, Val Loss: 161.33026709, Val R²: 0.546459
Saved best model with validation R2 0.546459 to best_finetuned_model.pth
Epoch 75/200
Train Loss: 47.31293208, Train R²: 0.955220, Val Loss: 161.10427039, Val R²: 0.547729
Saved best model with validation R2 0.547729 to best_finetuned_model.pth
Epoch 76/200
Train Loss: 46.24695514, Train R²: 0.957216, Val Loss: 160.01483696, Val R²: 0.553825
Saved best model with validation R2 0.553825 to best_finetuned_model.pth
Epoch 77/200
Train Loss: 45.28607886, Train R²: 0.958975, Val Loss: 160.84681304, Val R²: 0.549173
Epoch 78/200
Train Loss: 43.95931050, Train R²: 0.961344, Val Loss: 162.25172139, Val R²: 0.541264
Epoch 79/200
Train Loss: 42.55887084, Train R²: 0.963767, Val Loss: 161.80120784, Val R²: 0.543808
Epoch 80/200
Train Loss: 41.58375971, Train R²: 0.965409, Val Loss: 161.08532653, Val R²: 0.547835
Epoch 81/200
Train Loss: 40.46388406, Train R²: 0.967247, Val Loss: 161.39529549, Val R²: 0.546094
Epoch 82/200
Train Loss: 39.21836741, Train R²: 0.969232, Val Loss: 161.88871202, Val R²: 0.543314
Epoch 83/200
Train Loss: 38.35078570, Train R²: 0.970578, Val Loss: 161.98724390, Val R²: 0.542758
Epoch 84/200
Train Loss: 37.20997395, Train R²: 0.972303, Val Loss: 161.78442809, Val R²: 0.543902
Epoch 85/200
Train Loss: 36.34107106, Train R²: 0.973581, Val Loss: 160.97768916, Val R²: 0.548440
Epoch 86/200
Train Loss: 35.59699581, Train R²: 0.974652, Val Loss: 160.81420641, Val R²: 0.549356
Epoch 87/200
Train Loss: 34.66718244, Train R²: 0.975959, Val Loss: 161.48312847, Val R²: 0.545599
Epoch 00087: reducing learning rate of group 0 to 5.0000e-05.
Epoch 88/200
Train Loss: 34.01790408, Train R²: 0.976851, Val Loss: 161.48841386, Val R²: 0.545570
Epoch 89/200
Train Loss: 33.65926555, Train R²: 0.977336, Val Loss: 161.21046027, Val R²: 0.547133
Epoch 90/200
Train Loss: 33.28535901, Train R²: 0.977837, Val Loss: 161.37003163, Val R²: 0.546236
Epoch 91/200
Train Loss: 32.99148571, Train R²: 0.978227, Val Loss: 161.68789863, Val R²: 0.544446
Epoch 92/200
Train Loss: 32.56236948, Train R²: 0.978789, Val Loss: 161.77623676, Val R²: 0.543948
Epoch 93/200
Train Loss: 32.30966717, Train R²: 0.979117, Val Loss: 161.77502946, Val R²: 0.543955
Epoch 94/200
Train Loss: 31.98578848, Train R²: 0.979534, Val Loss: 161.92456408, Val R²: 0.543112
Epoch 95/200
Train Loss: 31.63260968, Train R²: 0.979983, Val Loss: 161.99760077, Val R²: 0.542700
Epoch 96/200
Train Loss: 31.37542266, Train R²: 0.980308, Val Loss: 162.00980150, Val R²: 0.542631
Epoch 97/200
Train Loss: 31.09581373, Train R²: 0.980657, Val Loss: 162.00573871, Val R²: 0.542654
Epoch 98/200
Train Loss: 30.79541771, Train R²: 0.981029, Val Loss: 162.08332396, Val R²: 0.542215
Epoch 00098: reducing learning rate of group 0 to 2.5000e-05.
Epoch 99/200
Train Loss: 30.53595899, Train R²: 0.981347, Val Loss: 162.11272963, Val R²: 0.542049
Epoch 100/200
Train Loss: 30.41020824, Train R²: 0.981501, Val Loss: 162.10863328, Val R²: 0.542072
Epoch 101/200
Train Loss: 30.29701025, Train R²: 0.981638, Val Loss: 162.10872967, Val R²: 0.542072
Epoch 102/200
Train Loss: 30.13501658, Train R²: 0.981834, Val Loss: 162.13334838, Val R²: 0.541933
Epoch 103/200
Train Loss: 30.02280715, Train R²: 0.981969, Val Loss: 162.21164319, Val R²: 0.541490
Epoch 104/200
Train Loss: 29.88240608, Train R²: 0.982137, Val Loss: 162.27323120, Val R²: 0.541142
Epoch 105/200
Train Loss: 29.76489381, Train R²: 0.982277, Val Loss: 162.30318008, Val R²: 0.540973
Epoch 106/200
Train Loss: 29.65352649, Train R²: 0.982410, Val Loss: 162.32054394, Val R²: 0.540874
Epoch 107/200
Train Loss: 29.51207925, Train R²: 0.982577, Val Loss: 162.34266422, Val R²: 0.540749
Epoch 108/200
Train Loss: 29.38978395, Train R²: 0.982721, Val Loss: 162.35064049, Val R²: 0.540704
Epoch 109/200
Train Loss: 29.27186861, Train R²: 0.982860, Val Loss: 162.37032687, Val R²: 0.540593
Epoch 00109: reducing learning rate of group 0 to 1.2500e-05.
Epoch 110/200
Train Loss: 29.17493275, Train R²: 0.982973, Val Loss: 162.39130980, Val R²: 0.540474
Epoch 111/200
Train Loss: 29.10961751, Train R²: 0.983049, Val Loss: 162.41470117, Val R²: 0.540342
Epoch 112/200
Train Loss: 29.06109841, Train R²: 0.983106, Val Loss: 162.42691265, Val R²: 0.540272
Epoch 113/200
Train Loss: 28.99421830, Train R²: 0.983183, Val Loss: 162.43737976, Val R²: 0.540213
Epoch 114/200
Train Loss: 28.93492126, Train R²: 0.983252, Val Loss: 162.44547763, Val R²: 0.540167
Epoch 115/200
Train Loss: 28.88496780, Train R²: 0.983310, Val Loss: 162.45537849, Val R²: 0.540111
Epoch 116/200
Train Loss: 28.82596359, Train R²: 0.983378, Val Loss: 162.46095084, Val R²: 0.540080
Epoch 117/200
Train Loss: 28.76342449, Train R²: 0.983450, Val Loss: 162.47329708, Val R²: 0.540010
Epoch 118/200
Train Loss: 28.72045128, Train R²: 0.983499, Val Loss: 162.49230751, Val R²: 0.539902
Epoch 119/200
Train Loss: 28.65059265, Train R²: 0.983579, Val Loss: 162.50827503, Val R²: 0.539812
Epoch 120/200
Train Loss: 28.61260507, Train R²: 0.983623, Val Loss: 162.52400063, Val R²: 0.539723
Epoch 00120: reducing learning rate of group 0 to 6.2500e-06.
Epoch 121/200
Train Loss: 28.55882528, Train R²: 0.983684, Val Loss: 162.52630196, Val R²: 0.539710
Epoch 122/200
Train Loss: 28.52478660, Train R²: 0.983723, Val Loss: 162.53034572, Val R²: 0.539687
Epoch 123/200
Train Loss: 28.50093907, Train R²: 0.983751, Val Loss: 162.53730341, Val R²: 0.539647
Epoch 124/200
Train Loss: 28.47699286, Train R²: 0.983778, Val Loss: 162.54361194, Val R²: 0.539612
Epoch 125/200
Train Loss: 28.44799568, Train R²: 0.983811, Val Loss: 162.55371108, Val R²: 0.539554
Epoch 126/200
Train Loss: 28.41098126, Train R²: 0.983853, Val Loss: 162.56187524, Val R²: 0.539508
Epoch 127/200
Train Loss: 28.39478627, Train R²: 0.983871, Val Loss: 162.57109623, Val R²: 0.539456
Epoch 128/200
Train Loss: 28.35725959, Train R²: 0.983914, Val Loss: 162.57542721, Val R²: 0.539431
Epoch 129/200
Train Loss: 28.33118852, Train R²: 0.983944, Val Loss: 162.58035874, Val R²: 0.539403
Epoch 130/200
Train Loss: 28.30258650, Train R²: 0.983976, Val Loss: 162.58962072, Val R²: 0.539351
Epoch 131/200
Train Loss: 28.27659746, Train R²: 0.984005, Val Loss: 162.59941070, Val R²: 0.539295
Epoch 00131: reducing learning rate of group 0 to 3.1250e-06.
Epoch 132/200
Train Loss: 28.25007587, Train R²: 0.984035, Val Loss: 162.60374693, Val R²: 0.539271
Epoch 133/200
Train Loss: 28.23763286, Train R²: 0.984049, Val Loss: 162.60655161, Val R²: 0.539255
Epoch 134/200
Train Loss: 28.22311677, Train R²: 0.984066, Val Loss: 162.61010693, Val R²: 0.539235
Epoch 135/200
Train Loss: 28.20844602, Train R²: 0.984082, Val Loss: 162.61471912, Val R²: 0.539209
Epoch 136/200
Train Loss: 28.19441864, Train R²: 0.984098, Val Loss: 162.61853850, Val R²: 0.539187
Epoch 137/200
Train Loss: 28.17860015, Train R²: 0.984116, Val Loss: 162.62144500, Val R²: 0.539171
Epoch 138/200
Train Loss: 28.16524236, Train R²: 0.984131, Val Loss: 162.62449558, Val R²: 0.539153
Epoch 139/200
Train Loss: 28.14876104, Train R²: 0.984150, Val Loss: 162.62867502, Val R²: 0.539130
Epoch 140/200
Train Loss: 28.13368102, Train R²: 0.984167, Val Loss: 162.63198366, Val R²: 0.539111
Epoch 141/200
Train Loss: 28.12249492, Train R²: 0.984179, Val Loss: 162.63583265, Val R²: 0.539089
Epoch 142/200
Train Loss: 28.10737514, Train R²: 0.984196, Val Loss: 162.63933930, Val R²: 0.539069
Epoch 00142: reducing learning rate of group 0 to 1.5625e-06.
Epoch 143/200
Train Loss: 28.09523324, Train R²: 0.984210, Val Loss: 162.64173506, Val R²: 0.539056
Epoch 144/200
Train Loss: 28.08676423, Train R²: 0.984219, Val Loss: 162.64317611, Val R²: 0.539047
Epoch 145/200
Train Loss: 28.08192053, Train R²: 0.984225, Val Loss: 162.64559583, Val R²: 0.539034
Epoch 146/200
Train Loss: 28.07316888, Train R²: 0.984235, Val Loss: 162.64730703, Val R²: 0.539024
Epoch 147/200
Train Loss: 28.06656085, Train R²: 0.984242, Val Loss: 162.64960661, Val R²: 0.539011
Epoch 148/200
Train Loss: 28.05692088, Train R²: 0.984253, Val Loss: 162.65227241, Val R²: 0.538996
Epoch 149/200
Train Loss: 28.05074994, Train R²: 0.984260, Val Loss: 162.65508225, Val R²: 0.538980
Epoch 150/200
Train Loss: 28.04291831, Train R²: 0.984269, Val Loss: 162.65741774, Val R²: 0.538967
Epoch 151/200
Train Loss: 28.03550640, Train R²: 0.984277, Val Loss: 162.65963313, Val R²: 0.538954
Epoch 152/200
Train Loss: 28.02900772, Train R²: 0.984284, Val Loss: 162.66286910, Val R²: 0.538936
Epoch 153/200
Train Loss: 28.02154152, Train R²: 0.984293, Val Loss: 162.66541461, Val R²: 0.538921
Epoch 00153: reducing learning rate of group 0 to 7.8125e-07.
Epoch 154/200
Train Loss: 28.01483656, Train R²: 0.984300, Val Loss: 162.66628511, Val R²: 0.538916
Epoch 155/200
Train Loss: 28.01022198, Train R²: 0.984305, Val Loss: 162.66749781, Val R²: 0.538910
Epoch 156/200
Train Loss: 28.00641662, Train R²: 0.984310, Val Loss: 162.66844035, Val R²: 0.538904
Epoch 157/200
Train Loss: 28.00361867, Train R²: 0.984313, Val Loss: 162.66919077, Val R²: 0.538900
Epoch 158/200
Train Loss: 27.99899087, Train R²: 0.984318, Val Loss: 162.67024135, Val R²: 0.538894
Epoch 159/200
Train Loss: 27.99608882, Train R²: 0.984321, Val Loss: 162.67137598, Val R²: 0.538888
Epoch 160/200
Train Loss: 27.99180897, Train R²: 0.984326, Val Loss: 162.67254661, Val R²: 0.538881
Epoch 161/200
Train Loss: 27.98830884, Train R²: 0.984330, Val Loss: 162.67378328, Val R²: 0.538874
Epoch 162/200
Train Loss: 27.98464021, Train R²: 0.984334, Val Loss: 162.67449766, Val R²: 0.538870
Epoch 163/200
Train Loss: 27.98076711, Train R²: 0.984338, Val Loss: 162.67543415, Val R²: 0.538865
Epoch 164/200
Train Loss: 27.97675651, Train R²: 0.984343, Val Loss: 162.67623857, Val R²: 0.538860
Epoch 00164: reducing learning rate of group 0 to 3.9063e-07.
Epoch 165/200
Train Loss: 27.97337116, Train R²: 0.984347, Val Loss: 162.67664077, Val R²: 0.538858
Epoch 166/200
Train Loss: 27.97181664, Train R²: 0.984348, Val Loss: 162.67721707, Val R²: 0.538854
Epoch 167/200
Train Loss: 27.96947296, Train R²: 0.984351, Val Loss: 162.67777535, Val R²: 0.538851
Epoch 168/200
Train Loss: 27.96827179, Train R²: 0.984352, Val Loss: 162.67848971, Val R²: 0.538847
Epoch 169/200
Train Loss: 27.96610144, Train R²: 0.984355, Val Loss: 162.67897596, Val R²: 0.538844
Epoch 170/200
Train Loss: 27.96356350, Train R²: 0.984358, Val Loss: 162.67945620, Val R²: 0.538842
Epoch 171/200
Train Loss: 27.96195527, Train R²: 0.984359, Val Loss: 162.67989441, Val R²: 0.538839
Epoch 172/200
Train Loss: 27.95975720, Train R²: 0.984362, Val Loss: 162.68042868, Val R²: 0.538836
Epoch 173/200
Train Loss: 27.95809366, Train R²: 0.984364, Val Loss: 162.68078285, Val R²: 0.538834
Epoch 174/200
Train Loss: 27.95596397, Train R²: 0.984366, Val Loss: 162.68148519, Val R²: 0.538830
Epoch 175/200
Train Loss: 27.95408986, Train R²: 0.984368, Val Loss: 162.68195942, Val R²: 0.538828
Epoch 00175: reducing learning rate of group 0 to 1.9531e-07.
Epoch 176/200
Train Loss: 27.95256618, Train R²: 0.984370, Val Loss: 162.68227758, Val R²: 0.538826
Epoch 177/200
Train Loss: 27.95139286, Train R²: 0.984371, Val Loss: 162.68260773, Val R²: 0.538824
Epoch 178/200
Train Loss: 27.95049673, Train R²: 0.984372, Val Loss: 162.68292589, Val R²: 0.538822
Epoch 179/200
Train Loss: 27.94953414, Train R²: 0.984373, Val Loss: 162.68323803, Val R²: 0.538820
Epoch 180/200
Train Loss: 27.94837403, Train R²: 0.984375, Val Loss: 162.68352617, Val R²: 0.538819
Epoch 181/200
Train Loss: 27.94749174, Train R²: 0.984376, Val Loss: 162.68376628, Val R²: 0.538817
Epoch 182/200
Train Loss: 27.94647147, Train R²: 0.984377, Val Loss: 162.68399439, Val R²: 0.538816
Epoch 183/200
Train Loss: 27.94570732, Train R²: 0.984378, Val Loss: 162.68431254, Val R²: 0.538814
Epoch 184/200
Train Loss: 27.94458439, Train R²: 0.984379, Val Loss: 162.68470873, Val R²: 0.538812
Epoch 185/200
Train Loss: 27.94356399, Train R²: 0.984380, Val Loss: 162.68494283, Val R²: 0.538811
Epoch 186/200
Train Loss: 27.94256934, Train R²: 0.984381, Val Loss: 162.68541105, Val R²: 0.538808
Epoch 00186: reducing learning rate of group 0 to 9.7656e-08.
Epoch 187/200
Train Loss: 27.94180392, Train R²: 0.984382, Val Loss: 162.68553711, Val R²: 0.538807
Epoch 188/200
Train Loss: 27.94127542, Train R²: 0.984382, Val Loss: 162.68568118, Val R²: 0.538806
Epoch 189/200
Train Loss: 27.94090482, Train R²: 0.984383, Val Loss: 162.68592129, Val R²: 0.538805
Epoch 190/200
Train Loss: 27.94030153, Train R²: 0.984384, Val Loss: 162.68602333, Val R²: 0.538805
Epoch 191/200
Train Loss: 27.93983648, Train R²: 0.984384, Val Loss: 162.68617940, Val R²: 0.538804
Epoch 192/200
Train Loss: 27.93942482, Train R²: 0.984385, Val Loss: 162.68628145, Val R²: 0.538803
Epoch 193/200
Train Loss: 27.93895704, Train R²: 0.984385, Val Loss: 162.68647354, Val R²: 0.538802
Epoch 194/200
Train Loss: 27.93838537, Train R²: 0.984386, Val Loss: 162.68658159, Val R²: 0.538801
Epoch 195/200
Train Loss: 27.93795465, Train R²: 0.984386, Val Loss: 162.68675566, Val R²: 0.538800
Epoch 196/200
Train Loss: 27.93744664, Train R²: 0.984387, Val Loss: 162.68688172, Val R²: 0.538800
Epoch 197/200
Train Loss: 27.93699303, Train R²: 0.984387, Val Loss: 162.68706781, Val R²: 0.538799
Epoch 00197: reducing learning rate of group 0 to 4.8828e-08.
Epoch 198/200
Train Loss: 27.93655188, Train R²: 0.984388, Val Loss: 162.68706180, Val R²: 0.538799
Epoch 199/200
Train Loss: 27.93644740, Train R²: 0.984388, Val Loss: 162.68714584, Val R²: 0.538798
Epoch 200/200
Train Loss: 27.93625429, Train R²: 0.984388, Val Loss: 162.68712183, Val R²: 0.538798
Training Complete. Best Val Loss: 160.01483695856143
训练时间: 74.93 秒
