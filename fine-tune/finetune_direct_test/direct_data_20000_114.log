Using device: cuda
Total samples: 20000, Training: 16000, Validation: 4000
Total trainable parameters: 4577537
Epoch 1/200
Train Loss: 2.58658873, Train R²: -0.237213, Val Loss: 1.54406410, Val R²: 0.315137
Saved best model with validation R2 0.315137 to best_finetuned_model.pth
Epoch 2/200
Train Loss: 0.96137920, Train R²: 0.540155, Val Loss: 0.72890421, Val R²: 0.676698
Saved best model with validation R2 0.676698 to best_finetuned_model.pth
Epoch 3/200
Train Loss: 0.44201861, Train R²: 0.788574, Val Loss: 0.47358541, Val R²: 0.789943
Saved best model with validation R2 0.789943 to best_finetuned_model.pth
Epoch 4/200
Train Loss: 0.23724495, Train R²: 0.886521, Val Loss: 0.41386531, Val R²: 0.816432
Saved best model with validation R2 0.816432 to best_finetuned_model.pth
Epoch 5/200
Train Loss: 0.15541459, Train R²: 0.925662, Val Loss: 0.42851780, Val R²: 0.809933
Epoch 6/200
Train Loss: 0.09723077, Train R²: 0.953493, Val Loss: 0.34108243, Val R²: 0.848714
Saved best model with validation R2 0.848714 to best_finetuned_model.pth
Epoch 7/200
Train Loss: 0.06642459, Train R²: 0.968228, Val Loss: 0.33274450, Val R²: 0.852413
Saved best model with validation R2 0.852413 to best_finetuned_model.pth
Epoch 8/200
Train Loss: 0.03953292, Train R²: 0.981091, Val Loss: 0.32409573, Val R²: 0.856249
Saved best model with validation R2 0.856249 to best_finetuned_model.pth
Epoch 9/200
Train Loss: 0.02571281, Train R²: 0.987701, Val Loss: 0.32203221, Val R²: 0.857164
Saved best model with validation R2 0.857164 to best_finetuned_model.pth
Epoch 10/200
Train Loss: 0.02185690, Train R²: 0.989545, Val Loss: 0.31640951, Val R²: 0.859658
Saved best model with validation R2 0.859658 to best_finetuned_model.pth
Epoch 11/200
Train Loss: 0.01554865, Train R²: 0.992563, Val Loss: 0.32499160, Val R²: 0.855851
Epoch 12/200
Train Loss: 0.01334519, Train R²: 0.993617, Val Loss: 0.32094292, Val R²: 0.857647
Epoch 13/200
Train Loss: 0.01395498, Train R²: 0.993325, Val Loss: 0.31230455, Val R²: 0.861479
Saved best model with validation R2 0.861479 to best_finetuned_model.pth
Epoch 14/200
Train Loss: 0.01330113, Train R²: 0.993638, Val Loss: 0.31085209, Val R²: 0.862123
Saved best model with validation R2 0.862123 to best_finetuned_model.pth
Epoch 15/200
Train Loss: 0.01185046, Train R²: 0.994332, Val Loss: 0.30941214, Val R²: 0.862762
Saved best model with validation R2 0.862762 to best_finetuned_model.pth
Epoch 16/200
Train Loss: 0.00920626, Train R²: 0.995596, Val Loss: 0.31238689, Val R²: 0.861442
Epoch 17/200
Train Loss: 0.00883812, Train R²: 0.995773, Val Loss: 0.31035204, Val R²: 0.862345
Epoch 18/200
Train Loss: 0.01036515, Train R²: 0.995042, Val Loss: 0.31174973, Val R²: 0.861725
Epoch 19/200
Train Loss: 0.01031142, Train R²: 0.995068, Val Loss: 0.32352591, Val R²: 0.856502
Epoch 20/200
Train Loss: 0.01557012, Train R²: 0.992553, Val Loss: 0.30696969, Val R²: 0.863845
Saved best model with validation R2 0.863845 to best_finetuned_model.pth
Epoch 21/200
Train Loss: 0.01208271, Train R²: 0.994221, Val Loss: 0.31518257, Val R²: 0.860202
Epoch 22/200
Train Loss: 0.01083759, Train R²: 0.994816, Val Loss: 0.30212241, Val R²: 0.865995
Saved best model with validation R2 0.865995 to best_finetuned_model.pth
Epoch 23/200
Train Loss: 0.00772204, Train R²: 0.996306, Val Loss: 0.30430468, Val R²: 0.865027
Epoch 24/200
Train Loss: 0.00590539, Train R²: 0.997175, Val Loss: 0.30140044, Val R²: 0.866315
Saved best model with validation R2 0.866315 to best_finetuned_model.pth
Epoch 25/200
Train Loss: 0.00605359, Train R²: 0.997104, Val Loss: 0.29954654, Val R²: 0.867137
Saved best model with validation R2 0.867137 to best_finetuned_model.pth
Epoch 26/200
Train Loss: 0.00541339, Train R²: 0.997411, Val Loss: 0.30149896, Val R²: 0.866271
Epoch 27/200
Train Loss: 0.00433693, Train R²: 0.997926, Val Loss: 0.29792147, Val R²: 0.867858
Saved best model with validation R2 0.867858 to best_finetuned_model.pth
Epoch 28/200
Train Loss: 0.00434753, Train R²: 0.997921, Val Loss: 0.30047960, Val R²: 0.866724
Epoch 29/200
Train Loss: 0.00415356, Train R²: 0.998013, Val Loss: 0.29459781, Val R²: 0.869332
Saved best model with validation R2 0.869332 to best_finetuned_model.pth
Epoch 30/200
Train Loss: 0.00399786, Train R²: 0.998088, Val Loss: 0.30297272, Val R²: 0.865618
Epoch 31/200
Train Loss: 0.00910745, Train R²: 0.995644, Val Loss: 0.30014661, Val R²: 0.866871
Epoch 32/200
Train Loss: 0.01285437, Train R²: 0.993852, Val Loss: 0.29069073, Val R²: 0.871065
Saved best model with validation R2 0.871065 to best_finetuned_model.pth
Epoch 33/200
Train Loss: 0.01133854, Train R²: 0.994577, Val Loss: 0.29769513, Val R²: 0.867959
Epoch 34/200
Train Loss: 0.01079791, Train R²: 0.994835, Val Loss: 0.29065049, Val R²: 0.871083
Saved best model with validation R2 0.871083 to best_finetuned_model.pth
Epoch 35/200
Train Loss: 0.01143188, Train R²: 0.994532, Val Loss: 0.29313014, Val R²: 0.869983
Epoch 36/200
Train Loss: 0.00901391, Train R²: 0.995688, Val Loss: 0.28905965, Val R²: 0.871789
Saved best model with validation R2 0.871789 to best_finetuned_model.pth
Epoch 37/200
Train Loss: 0.00783048, Train R²: 0.996255, Val Loss: 0.29528021, Val R²: 0.869030
Epoch 38/200
Train Loss: 0.00621380, Train R²: 0.997028, Val Loss: 0.29112963, Val R²: 0.870871
Epoch 39/200
Train Loss: 0.00518491, Train R²: 0.997520, Val Loss: 0.29006137, Val R²: 0.871345
Epoch 40/200
Train Loss: 0.00513256, Train R²: 0.997545, Val Loss: 0.28717809, Val R²: 0.872623
Saved best model with validation R2 0.872623 to best_finetuned_model.pth
Epoch 41/200
Train Loss: 0.00525182, Train R²: 0.997488, Val Loss: 0.28814322, Val R²: 0.872195
Epoch 42/200
Train Loss: 0.00464638, Train R²: 0.997778, Val Loss: 0.28816442, Val R²: 0.872186
Epoch 43/200
Train Loss: 0.00471858, Train R²: 0.997743, Val Loss: 0.28557105, Val R²: 0.873336
Saved best model with validation R2 0.873336 to best_finetuned_model.pth
Epoch 44/200
Train Loss: 0.00458178, Train R²: 0.997808, Val Loss: 0.28252336, Val R²: 0.874688
Saved best model with validation R2 0.874688 to best_finetuned_model.pth
Epoch 45/200
Train Loss: 0.00469368, Train R²: 0.997755, Val Loss: 0.28722073, Val R²: 0.872604
Epoch 46/200
Train Loss: 0.00804781, Train R²: 0.996151, Val Loss: 0.29307897, Val R²: 0.870006
Epoch 47/200
Train Loss: 0.01102778, Train R²: 0.994725, Val Loss: 0.28673619, Val R²: 0.872819
Epoch 48/200
Train Loss: 0.00908223, Train R²: 0.995656, Val Loss: 0.28955551, Val R²: 0.871569
Epoch 49/200
Train Loss: 0.00853405, Train R²: 0.995918, Val Loss: 0.28861786, Val R²: 0.871985
Epoch 50/200
Train Loss: 0.01052206, Train R²: 0.994967, Val Loss: 0.27859955, Val R²: 0.876428
Saved best model with validation R2 0.876428 to best_finetuned_model.pth
Epoch 51/200
Train Loss: 0.00957234, Train R²: 0.995421, Val Loss: 0.28621193, Val R²: 0.873052
Epoch 52/200
Train Loss: 0.00952328, Train R²: 0.995445, Val Loss: 0.28886427, Val R²: 0.871876
Epoch 53/200
Train Loss: 0.00880707, Train R²: 0.995787, Val Loss: 0.28150000, Val R²: 0.875142
Epoch 54/200
Train Loss: 0.00648477, Train R²: 0.996898, Val Loss: 0.27542076, Val R²: 0.877838
Saved best model with validation R2 0.877838 to best_finetuned_model.pth
Epoch 55/200
Train Loss: 0.00496638, Train R²: 0.997625, Val Loss: 0.27437089, Val R²: 0.878304
Saved best model with validation R2 0.878304 to best_finetuned_model.pth
Epoch 56/200
Train Loss: 0.00413594, Train R²: 0.998022, Val Loss: 0.27934029, Val R²: 0.876100
Epoch 57/200
Train Loss: 0.00328454, Train R²: 0.998429, Val Loss: 0.27826942, Val R²: 0.876575
Epoch 58/200
Train Loss: 0.00320712, Train R²: 0.998466, Val Loss: 0.27466197, Val R²: 0.878175
Epoch 59/200
Train Loss: 0.00348389, Train R²: 0.998334, Val Loss: 0.27020843, Val R²: 0.880150
Saved best model with validation R2 0.880150 to best_finetuned_model.pth
Epoch 60/200
Train Loss: 0.00465067, Train R²: 0.997775, Val Loss: 0.27667417, Val R²: 0.877282
Epoch 61/200
Train Loss: 0.00451920, Train R²: 0.997838, Val Loss: 0.27407702, Val R²: 0.878434
Epoch 62/200
Train Loss: 0.00473122, Train R²: 0.997737, Val Loss: 0.27377194, Val R²: 0.878570
Epoch 63/200
Train Loss: 0.00882726, Train R²: 0.995778, Val Loss: 0.27497886, Val R²: 0.878034
Epoch 64/200
Train Loss: 0.01652206, Train R²: 0.992097, Val Loss: 0.27733942, Val R²: 0.876987
Epoch 65/200
Train Loss: 0.01251193, Train R²: 0.994015, Val Loss: 0.28730992, Val R²: 0.872565
Epoch 66/200
Train Loss: 0.00935747, Train R²: 0.995524, Val Loss: 0.28767150, Val R²: 0.872405
Epoch 67/200
Train Loss: 0.01147421, Train R²: 0.994512, Val Loss: 0.27299169, Val R²: 0.878916
Epoch 68/200
Train Loss: 0.00915827, Train R²: 0.995619, Val Loss: 0.27448291, Val R²: 0.878254
Epoch 69/200
Train Loss: 0.00613422, Train R²: 0.997066, Val Loss: 0.26615285, Val R²: 0.881949
Saved best model with validation R2 0.881949 to best_finetuned_model.pth
Epoch 70/200
Train Loss: 0.00475560, Train R²: 0.997725, Val Loss: 0.26213624, Val R²: 0.883731
Saved best model with validation R2 0.883731 to best_finetuned_model.pth
Epoch 71/200
Train Loss: 0.00369435, Train R²: 0.998233, Val Loss: 0.27051660, Val R²: 0.880014
Epoch 72/200
Train Loss: 0.00310691, Train R²: 0.998514, Val Loss: 0.27209700, Val R²: 0.879313
Epoch 73/200
Train Loss: 0.00244524, Train R²: 0.998830, Val Loss: 0.27385065, Val R²: 0.878535
Epoch 74/200
Train Loss: 0.00223978, Train R²: 0.998929, Val Loss: 0.26398158, Val R²: 0.882912
Epoch 75/200
Train Loss: 0.00222365, Train R²: 0.998936, Val Loss: 0.26881877, Val R²: 0.880767
Epoch 76/200
Train Loss: 0.00240733, Train R²: 0.998849, Val Loss: 0.26553382, Val R²: 0.882224
Epoch 77/200
Train Loss: 0.00251541, Train R²: 0.998797, Val Loss: 0.26968553, Val R²: 0.880382
Epoch 78/200
Train Loss: 0.00362318, Train R²: 0.998267, Val Loss: 0.27179256, Val R²: 0.879448
Epoch 79/200
Train Loss: 0.00644906, Train R²: 0.996915, Val Loss: 0.26794022, Val R²: 0.881156
Epoch 80/200
Train Loss: 0.00836438, Train R²: 0.995999, Val Loss: 0.26109878, Val R²: 0.884191
Saved best model with validation R2 0.884191 to best_finetuned_model.pth
Epoch 81/200
Train Loss: 0.00926509, Train R²: 0.995568, Val Loss: 0.26793361, Val R²: 0.881159
Epoch 82/200
Train Loss: 0.00918696, Train R²: 0.995606, Val Loss: 0.26545837, Val R²: 0.882257
Epoch 83/200
Train Loss: 0.00865593, Train R²: 0.995860, Val Loss: 0.26393218, Val R²: 0.882934
Epoch 84/200
Train Loss: 0.00860402, Train R²: 0.995885, Val Loss: 0.26818748, Val R²: 0.881047
Epoch 85/200
Train Loss: 0.00665424, Train R²: 0.996817, Val Loss: 0.26375306, Val R²: 0.883013
Epoch 86/200
Train Loss: 0.00525754, Train R²: 0.997485, Val Loss: 0.26392939, Val R²: 0.882935
Epoch 87/200
Train Loss: 0.00493382, Train R²: 0.997640, Val Loss: 0.26171290, Val R²: 0.883918
Epoch 88/200
Train Loss: 0.00444832, Train R²: 0.997872, Val Loss: 0.26664896, Val R²: 0.881729
Epoch 89/200
Train Loss: 0.00518280, Train R²: 0.997521, Val Loss: 0.25522573, Val R²: 0.886796
Saved best model with validation R2 0.886796 to best_finetuned_model.pth
Epoch 90/200
Train Loss: 0.00582070, Train R²: 0.997216, Val Loss: 0.26304082, Val R²: 0.883329
Epoch 91/200
Train Loss: 0.00530679, Train R²: 0.997462, Val Loss: 0.25311461, Val R²: 0.887732
Saved best model with validation R2 0.887732 to best_finetuned_model.pth
Epoch 92/200
Train Loss: 0.00598112, Train R²: 0.997139, Val Loss: 0.25649801, Val R²: 0.886231
Epoch 93/200
Train Loss: 0.00517967, Train R²: 0.997522, Val Loss: 0.25221970, Val R²: 0.888129
Saved best model with validation R2 0.888129 to best_finetuned_model.pth
Epoch 94/200
Train Loss: 0.00585138, Train R²: 0.997201, Val Loss: 0.26244849, Val R²: 0.883592
Epoch 95/200
Train Loss: 0.00850612, Train R²: 0.995931, Val Loss: 0.25464564, Val R²: 0.887053
Epoch 96/200
Train Loss: 0.00658555, Train R²: 0.996850, Val Loss: 0.25528056, Val R²: 0.886771
Epoch 97/200
Train Loss: 0.00542455, Train R²: 0.997405, Val Loss: 0.25998094, Val R²: 0.884687
Epoch 98/200
Train Loss: 0.00482854, Train R²: 0.997690, Val Loss: 0.25859822, Val R²: 0.885300
Epoch 99/200
Train Loss: 0.00417904, Train R²: 0.998001, Val Loss: 0.25570517, Val R²: 0.886583
Epoch 100/200
Train Loss: 0.00380727, Train R²: 0.998179, Val Loss: 0.25244143, Val R²: 0.888031
Epoch 101/200
Train Loss: 0.00349175, Train R²: 0.998330, Val Loss: 0.25115806, Val R²: 0.888600
Saved best model with validation R2 0.888600 to best_finetuned_model.pth
Epoch 102/200
Train Loss: 0.00334161, Train R²: 0.998402, Val Loss: 0.24826525, Val R²: 0.889883
Saved best model with validation R2 0.889883 to best_finetuned_model.pth
Epoch 103/200
Train Loss: 0.00338894, Train R²: 0.998379, Val Loss: 0.26217907, Val R²: 0.883712
Epoch 104/200
Train Loss: 0.00338672, Train R²: 0.998380, Val Loss: 0.25050549, Val R²: 0.888889
Epoch 105/200
Train Loss: 0.00319427, Train R²: 0.998472, Val Loss: 0.25748281, Val R²: 0.885795
Epoch 106/200
Train Loss: 0.00334671, Train R²: 0.998399, Val Loss: 0.25737889, Val R²: 0.885841
Epoch 107/200
Train Loss: 0.00401189, Train R²: 0.998081, Val Loss: 0.25213818, Val R²: 0.888165
Epoch 108/200
Train Loss: 0.00617402, Train R²: 0.997047, Val Loss: 0.25834631, Val R²: 0.885412
Epoch 109/200
Train Loss: 0.00611818, Train R²: 0.997074, Val Loss: 0.25919338, Val R²: 0.885036
Epoch 110/200
Train Loss: 0.00558694, Train R²: 0.997328, Val Loss: 0.24961815, Val R²: 0.889283
Epoch 111/200
Train Loss: 0.00505144, Train R²: 0.997584, Val Loss: 0.25077684, Val R²: 0.888769
Epoch 112/200
Train Loss: 0.00556044, Train R²: 0.997340, Val Loss: 0.25047566, Val R²: 0.888903
Epoch 113/200
Train Loss: 0.00533714, Train R²: 0.997447, Val Loss: 0.25018455, Val R²: 0.889032
Epoch 00113: reducing learning rate of group 0 to 5.0000e-05.
Epoch 114/200
Train Loss: 0.00312662, Train R²: 0.998504, Val Loss: 0.24612642, Val R²: 0.890832
Saved best model with validation R2 0.890832 to best_finetuned_model.pth
Epoch 115/200
Train Loss: 0.00093249, Train R²: 0.999554, Val Loss: 0.24485625, Val R²: 0.891395
Saved best model with validation R2 0.891395 to best_finetuned_model.pth
Epoch 116/200
Train Loss: 0.00038628, Train R²: 0.999815, Val Loss: 0.24691766, Val R²: 0.890481
Epoch 117/200
Train Loss: 0.00019828, Train R²: 0.999905, Val Loss: 0.24691566, Val R²: 0.890482
Epoch 118/200
Train Loss: 0.00011611, Train R²: 0.999944, Val Loss: 0.24629229, Val R²: 0.890758
Epoch 119/200
Train Loss: 0.00007373, Train R²: 0.999965, Val Loss: 0.24599156, Val R²: 0.890892
Epoch 120/200
Train Loss: 0.00005080, Train R²: 0.999976, Val Loss: 0.24622543, Val R²: 0.890788
Epoch 121/200
Train Loss: 0.00003696, Train R²: 0.999982, Val Loss: 0.24632418, Val R²: 0.890744
Epoch 122/200
Train Loss: 0.00004245, Train R²: 0.999980, Val Loss: 0.24631926, Val R²: 0.890746
Epoch 123/200
Train Loss: 0.00004186, Train R²: 0.999980, Val Loss: 0.24666919, Val R²: 0.890591
Epoch 124/200
Train Loss: 0.00002709, Train R²: 0.999987, Val Loss: 0.24620676, Val R²: 0.890796
Epoch 125/200
Train Loss: 0.00002374, Train R²: 0.999989, Val Loss: 0.24655039, Val R²: 0.890644
Epoch 126/200
Train Loss: 0.00002121, Train R²: 0.999990, Val Loss: 0.24618920, Val R²: 0.890804
Epoch 00126: reducing learning rate of group 0 to 2.5000e-05.
Epoch 127/200
Train Loss: 0.00001291, Train R²: 0.999994, Val Loss: 0.24643204, Val R²: 0.890696
Epoch 128/200
Train Loss: 0.00000975, Train R²: 0.999995, Val Loss: 0.24639909, Val R²: 0.890711
Epoch 129/200
Train Loss: 0.00000876, Train R²: 0.999996, Val Loss: 0.24648057, Val R²: 0.890675
Epoch 130/200
Train Loss: 0.00000852, Train R²: 0.999996, Val Loss: 0.24643959, Val R²: 0.890693
Epoch 131/200
Train Loss: 0.00000650, Train R²: 0.999997, Val Loss: 0.24640374, Val R²: 0.890709
Epoch 132/200
Train Loss: 0.00000820, Train R²: 0.999996, Val Loss: 0.24660876, Val R²: 0.890618
Epoch 133/200
Train Loss: 0.00000608, Train R²: 0.999997, Val Loss: 0.24656818, Val R²: 0.890636
Epoch 134/200
Train Loss: 0.00000800, Train R²: 0.999996, Val Loss: 0.24646523, Val R²: 0.890681
Epoch 135/200
Train Loss: 0.00000609, Train R²: 0.999997, Val Loss: 0.24663825, Val R²: 0.890605
Epoch 136/200
Train Loss: 0.00000525, Train R²: 0.999997, Val Loss: 0.24640923, Val R²: 0.890706
Epoch 137/200
Train Loss: 0.00000610, Train R²: 0.999997, Val Loss: 0.24648192, Val R²: 0.890674
Epoch 00137: reducing learning rate of group 0 to 1.2500e-05.
Epoch 138/200
Train Loss: 0.00000524, Train R²: 0.999997, Val Loss: 0.24652385, Val R²: 0.890655
Epoch 139/200
Train Loss: 0.00000529, Train R²: 0.999997, Val Loss: 0.24655656, Val R²: 0.890641
Epoch 140/200
Train Loss: 0.00000455, Train R²: 0.999998, Val Loss: 0.24668160, Val R²: 0.890585
Epoch 141/200
Train Loss: 0.00000467, Train R²: 0.999998, Val Loss: 0.24651708, Val R²: 0.890658
Epoch 142/200
Train Loss: 0.00000461, Train R²: 0.999998, Val Loss: 0.24651077, Val R²: 0.890661
Epoch 143/200
Train Loss: 0.00000458, Train R²: 0.999998, Val Loss: 0.24649732, Val R²: 0.890667
Epoch 144/200
Train Loss: 0.00000428, Train R²: 0.999998, Val Loss: 0.24654331, Val R²: 0.890647
Epoch 145/200
Train Loss: 0.00000628, Train R²: 0.999997, Val Loss: 0.24650269, Val R²: 0.890665
Epoch 146/200
Train Loss: 0.00000696, Train R²: 0.999997, Val Loss: 0.24645574, Val R²: 0.890686
Epoch 147/200
Train Loss: 0.00000712, Train R²: 0.999997, Val Loss: 0.24661204, Val R²: 0.890616
Epoch 148/200
Train Loss: 0.00000920, Train R²: 0.999996, Val Loss: 0.24663721, Val R²: 0.890605
Epoch 00148: reducing learning rate of group 0 to 6.2500e-06.
Epoch 149/200
Train Loss: 0.00000704, Train R²: 0.999997, Val Loss: 0.24653326, Val R²: 0.890651
Epoch 150/200
Train Loss: 0.00000454, Train R²: 0.999998, Val Loss: 0.24643952, Val R²: 0.890693
Epoch 151/200
Train Loss: 0.00000395, Train R²: 0.999998, Val Loss: 0.24660575, Val R²: 0.890619
Epoch 152/200
Train Loss: 0.00000418, Train R²: 0.999998, Val Loss: 0.24639200, Val R²: 0.890714
Epoch 153/200
Train Loss: 0.00000475, Train R²: 0.999998, Val Loss: 0.24647903, Val R²: 0.890675
Epoch 154/200
Train Loss: 0.00000524, Train R²: 0.999997, Val Loss: 0.24647335, Val R²: 0.890678
Epoch 155/200
Train Loss: 0.00000345, Train R²: 0.999998, Val Loss: 0.24647884, Val R²: 0.890675
Epoch 156/200
Train Loss: 0.00000407, Train R²: 0.999998, Val Loss: 0.24652331, Val R²: 0.890656
Epoch 157/200
Train Loss: 0.00000392, Train R²: 0.999998, Val Loss: 0.24644721, Val R²: 0.890689
Epoch 158/200
Train Loss: 0.00000337, Train R²: 0.999998, Val Loss: 0.24645435, Val R²: 0.890686
Epoch 159/200
Train Loss: 0.00000399, Train R²: 0.999998, Val Loss: 0.24656249, Val R²: 0.890638
Epoch 00159: reducing learning rate of group 0 to 3.1250e-06.
Epoch 160/200
Train Loss: 0.00000335, Train R²: 0.999998, Val Loss: 0.24647355, Val R²: 0.890678
Epoch 161/200
Train Loss: 0.00000496, Train R²: 0.999998, Val Loss: 0.24652550, Val R²: 0.890655
Epoch 162/200
Train Loss: 0.00000306, Train R²: 0.999999, Val Loss: 0.24653097, Val R²: 0.890652
Epoch 163/200
Train Loss: 0.00000299, Train R²: 0.999999, Val Loss: 0.24649424, Val R²: 0.890669
Epoch 164/200
Train Loss: 0.00000438, Train R²: 0.999998, Val Loss: 0.24655332, Val R²: 0.890642
Epoch 165/200
Train Loss: 0.00000291, Train R²: 0.999999, Val Loss: 0.24646371, Val R²: 0.890682
Epoch 166/200
Train Loss: 0.00000337, Train R²: 0.999998, Val Loss: 0.24653346, Val R²: 0.890651
Epoch 167/200
Train Loss: 0.00000389, Train R²: 0.999998, Val Loss: 0.24648537, Val R²: 0.890673
Epoch 168/200
Train Loss: 0.00000313, Train R²: 0.999999, Val Loss: 0.24653951, Val R²: 0.890648
Epoch 169/200
Train Loss: 0.00000336, Train R²: 0.999998, Val Loss: 0.24657169, Val R²: 0.890634
Epoch 170/200
Train Loss: 0.00000457, Train R²: 0.999998, Val Loss: 0.24654842, Val R²: 0.890645
Epoch 00170: reducing learning rate of group 0 to 1.5625e-06.
Epoch 171/200
Train Loss: 0.00000307, Train R²: 0.999999, Val Loss: 0.24651901, Val R²: 0.890658
Epoch 172/200
Train Loss: 0.00000401, Train R²: 0.999998, Val Loss: 0.24652913, Val R²: 0.890653
Epoch 173/200
Train Loss: 0.00000314, Train R²: 0.999999, Val Loss: 0.24651820, Val R²: 0.890658
Epoch 174/200
Train Loss: 0.00000348, Train R²: 0.999998, Val Loss: 0.24651801, Val R²: 0.890658
Epoch 175/200
Train Loss: 0.00000372, Train R²: 0.999998, Val Loss: 0.24650846, Val R²: 0.890662
Epoch 176/200
Train Loss: 0.00000331, Train R²: 0.999998, Val Loss: 0.24650765, Val R²: 0.890663
Epoch 177/200
Train Loss: 0.00000318, Train R²: 0.999998, Val Loss: 0.24650873, Val R²: 0.890662
Epoch 178/200
Train Loss: 0.00000397, Train R²: 0.999998, Val Loss: 0.24648640, Val R²: 0.890672
Epoch 179/200
Train Loss: 0.00000402, Train R²: 0.999998, Val Loss: 0.24651472, Val R²: 0.890659
Epoch 180/200
Train Loss: 0.00000325, Train R²: 0.999998, Val Loss: 0.24653438, Val R²: 0.890651
Epoch 181/200
Train Loss: 0.00000346, Train R²: 0.999998, Val Loss: 0.24655582, Val R²: 0.890641
Epoch 00181: reducing learning rate of group 0 to 7.8125e-07.
Epoch 182/200
Train Loss: 0.00000348, Train R²: 0.999998, Val Loss: 0.24650307, Val R²: 0.890665
Epoch 183/200
Train Loss: 0.00000341, Train R²: 0.999998, Val Loss: 0.24649633, Val R²: 0.890668
Epoch 184/200
Train Loss: 0.00000351, Train R²: 0.999998, Val Loss: 0.24648212, Val R²: 0.890674
Epoch 185/200
Train Loss: 0.00000258, Train R²: 0.999999, Val Loss: 0.24654871, Val R²: 0.890644
Epoch 186/200
Train Loss: 0.00000322, Train R²: 0.999998, Val Loss: 0.24653359, Val R²: 0.890651
Epoch 187/200
Train Loss: 0.00000467, Train R²: 0.999998, Val Loss: 0.24647824, Val R²: 0.890676
Epoch 188/200
Train Loss: 0.00000298, Train R²: 0.999999, Val Loss: 0.24654407, Val R²: 0.890646
Epoch 189/200
Train Loss: 0.00000270, Train R²: 0.999999, Val Loss: 0.24654897, Val R²: 0.890644
Epoch 190/200
Train Loss: 0.00000295, Train R²: 0.999999, Val Loss: 0.24655556, Val R²: 0.890641
Epoch 191/200
Train Loss: 0.00000332, Train R²: 0.999998, Val Loss: 0.24653248, Val R²: 0.890652
Epoch 192/200
Train Loss: 0.00000292, Train R²: 0.999999, Val Loss: 0.24650292, Val R²: 0.890665
Epoch 00192: reducing learning rate of group 0 to 3.9063e-07.
Epoch 193/200
Train Loss: 0.00000259, Train R²: 0.999999, Val Loss: 0.24647800, Val R²: 0.890676
Epoch 194/200
Train Loss: 0.00000312, Train R²: 0.999999, Val Loss: 0.24657118, Val R²: 0.890634
Epoch 195/200
Train Loss: 0.00000338, Train R²: 0.999998, Val Loss: 0.24653248, Val R²: 0.890652
Epoch 196/200
Train Loss: 0.00000337, Train R²: 0.999998, Val Loss: 0.24658848, Val R²: 0.890627
Epoch 197/200
Train Loss: 0.00000303, Train R²: 0.999999, Val Loss: 0.24645946, Val R²: 0.890684
Epoch 198/200
Train Loss: 0.00000309, Train R²: 0.999999, Val Loss: 0.24647189, Val R²: 0.890678
Epoch 199/200
Train Loss: 0.00000256, Train R²: 0.999999, Val Loss: 0.24649760, Val R²: 0.890667
Epoch 200/200
Train Loss: 0.00000318, Train R²: 0.999999, Val Loss: 0.24655336, Val R²: 0.890642
Training Complete. Best Val Loss: 0.2448562514781952
训练时间: 3277.99 秒
