Using device: cuda
Total samples: 1000, Training: 800, Validation: 200
Total trainable parameters: 4577537
Epoch 1/300
Train Loss: 4.73857480, Train R²: -1.261918, Val Loss: 2.09227204, Val R²: 0.023812
Saved Best Model with Val Loss: 2.092272
Epoch 2/300
Train Loss: 2.02875889, Train R²: 0.031589, Val Loss: 2.00813770, Val R²: 0.063066
Saved Best Model with Val Loss: 2.008138
Epoch 3/300
Train Loss: 1.79326818, Train R²: 0.143999, Val Loss: 2.01897812, Val R²: 0.058008
Epoch 4/300
Train Loss: 1.63281126, Train R²: 0.220592, Val Loss: 1.71547389, Val R²: 0.199614
Saved Best Model with Val Loss: 1.715474
Epoch 5/300
Train Loss: 1.24053195, Train R²: 0.407843, Val Loss: 2.29262257, Val R²: -0.069665
Epoch 6/300
Train Loss: 1.42657588, Train R²: 0.319036, Val Loss: 2.18634892, Val R²: -0.020081
Epoch 7/300
Train Loss: 1.33343694, Train R²: 0.363495, Val Loss: 1.65104735, Val R²: 0.229673
Saved Best Model with Val Loss: 1.651047
Epoch 8/300
Train Loss: 1.02901298, Train R²: 0.508810, Val Loss: 1.59691846, Val R²: 0.254928
Saved Best Model with Val Loss: 1.596918
Epoch 9/300
Train Loss: 0.79960694, Train R²: 0.618315, Val Loss: 1.56223977, Val R²: 0.271108
Saved Best Model with Val Loss: 1.562240
Epoch 10/300
Train Loss: 0.70371838, Train R²: 0.664086, Val Loss: 1.59905207, Val R²: 0.253933
Epoch 11/300
Train Loss: 0.58074671, Train R²: 0.722786, Val Loss: 1.58939314, Val R²: 0.258439
Epoch 12/300
Train Loss: 0.44509667, Train R²: 0.787537, Val Loss: 1.68836427, Val R²: 0.212262
Epoch 13/300
Train Loss: 0.40888082, Train R²: 0.804824, Val Loss: 1.51881135, Val R²: 0.291371
Saved Best Model with Val Loss: 1.518811
Epoch 14/300
Train Loss: 0.38160129, Train R²: 0.817846, Val Loss: 1.53050208, Val R²: 0.285916
Epoch 15/300
Train Loss: 0.35168863, Train R²: 0.832124, Val Loss: 1.47761309, Val R²: 0.310592
Saved Best Model with Val Loss: 1.477613
Epoch 16/300
Train Loss: 0.27996911, Train R²: 0.866359, Val Loss: 1.52809906, Val R²: 0.287037
Epoch 17/300
Train Loss: 0.23616523, Train R²: 0.887269, Val Loss: 1.39983177, Val R²: 0.346883
Saved Best Model with Val Loss: 1.399832
Epoch 18/300
Train Loss: 0.22266705, Train R²: 0.893712, Val Loss: 1.50811255, Val R²: 0.296362
Epoch 19/300
Train Loss: 0.19644400, Train R²: 0.906229, Val Loss: 1.62442040, Val R²: 0.242097
Epoch 20/300
Train Loss: 0.44077068, Train R²: 0.789602, Val Loss: 2.41735387, Val R²: -0.127861
Epoch 21/300
Train Loss: 1.21258178, Train R²: 0.421185, Val Loss: 1.74078774, Val R²: 0.187803
Epoch 22/300
Train Loss: 0.44281819, Train R²: 0.788625, Val Loss: 1.50996792, Val R²: 0.295497
Epoch 23/300
Train Loss: 0.43854371, Train R²: 0.790665, Val Loss: 1.82338929, Val R²: 0.149264
Epoch 24/300
Train Loss: 0.39901218, Train R²: 0.809535, Val Loss: 1.38275170, Val R²: 0.354852
Saved Best Model with Val Loss: 1.382752
Epoch 25/300
Train Loss: 0.26872922, Train R²: 0.871724, Val Loss: 1.49979687, Val R²: 0.300242
Epoch 26/300
Train Loss: 0.23671385, Train R²: 0.887007, Val Loss: 1.34098899, Val R²: 0.374337
Saved Best Model with Val Loss: 1.340989
Epoch 27/300
Train Loss: 0.13679713, Train R²: 0.934701, Val Loss: 1.45027244, Val R²: 0.323349
Epoch 28/300
Train Loss: 0.12529313, Train R²: 0.940192, Val Loss: 1.33993924, Val R²: 0.374827
Saved Best Model with Val Loss: 1.339939
Epoch 29/300
Train Loss: 0.10496418, Train R²: 0.949896, Val Loss: 1.16767538, Val R²: 0.455199
Saved Best Model with Val Loss: 1.167675
Epoch 30/300
Train Loss: 0.10352643, Train R²: 0.950583, Val Loss: 1.18913770, Val R²: 0.445186
Epoch 31/300
Train Loss: 0.08191048, Train R²: 0.960901, Val Loss: 1.27569544, Val R²: 0.404801
Epoch 32/300
Train Loss: 0.07210842, Train R²: 0.965580, Val Loss: 1.46785378, Val R²: 0.315146
Epoch 33/300
Train Loss: 0.05873302, Train R²: 0.971964, Val Loss: 1.28116167, Val R²: 0.402250
Epoch 34/300
Train Loss: 0.05181720, Train R²: 0.975266, Val Loss: 1.23032022, Val R²: 0.425971
Epoch 35/300
Train Loss: 0.04155324, Train R²: 0.980165, Val Loss: 1.27139759, Val R²: 0.406806
Epoch 36/300
Train Loss: 0.03011957, Train R²: 0.985623, Val Loss: 1.29558682, Val R²: 0.395520
Epoch 37/300
Train Loss: 0.02683268, Train R²: 0.987192, Val Loss: 1.26128817, Val R²: 0.411523
Epoch 38/300
Train Loss: 0.02258375, Train R²: 0.989220, Val Loss: 1.22871661, Val R²: 0.426720
Epoch 39/300
Train Loss: 0.01909701, Train R²: 0.990884, Val Loss: 1.24807703, Val R²: 0.417687
Epoch 40/300
Train Loss: 0.01510779, Train R²: 0.992788, Val Loss: 1.26558578, Val R²: 0.409518
Epoch 00040: reducing learning rate of group 0 to 5.0000e-04.
Epoch 41/300
Train Loss: 0.01097838, Train R²: 0.994760, Val Loss: 1.25827801, Val R²: 0.412927
Epoch 42/300
Train Loss: 0.00923904, Train R²: 0.995590, Val Loss: 1.25425577, Val R²: 0.414804
Epoch 43/300
Train Loss: 0.00643241, Train R²: 0.996930, Val Loss: 1.25341249, Val R²: 0.415197
Epoch 44/300
Train Loss: 0.00643656, Train R²: 0.996928, Val Loss: 1.25377369, Val R²: 0.415029
Epoch 45/300
Train Loss: 0.00578950, Train R²: 0.997236, Val Loss: 1.27071261, Val R²: 0.407126
Epoch 46/300
Train Loss: 0.00494458, Train R²: 0.997640, Val Loss: 1.26838577, Val R²: 0.408211
Epoch 47/300
Train Loss: 0.00365319, Train R²: 0.998256, Val Loss: 1.26902544, Val R²: 0.407913
Epoch 48/300
Train Loss: 0.00271961, Train R²: 0.998702, Val Loss: 1.26473391, Val R²: 0.409915
Epoch 49/300
Train Loss: 0.00227959, Train R²: 0.998912, Val Loss: 1.25799572, Val R²: 0.413059
Epoch 50/300
Train Loss: 0.00159485, Train R²: 0.999239, Val Loss: 1.25160444, Val R²: 0.416041
Epoch 51/300
Train Loss: 0.00157837, Train R²: 0.999247, Val Loss: 1.25060356, Val R²: 0.416508
Epoch 00051: reducing learning rate of group 0 to 2.5000e-04.
Epoch 52/300
Train Loss: 0.00119840, Train R²: 0.999428, Val Loss: 1.25211656, Val R²: 0.415802
Epoch 53/300
Train Loss: 0.00113481, Train R²: 0.999458, Val Loss: 1.25348377, Val R²: 0.415164
Epoch 54/300
Train Loss: 0.00112121, Train R²: 0.999465, Val Loss: 1.25495934, Val R²: 0.414476
Epoch 55/300
Train Loss: 0.00091711, Train R²: 0.999562, Val Loss: 1.25714493, Val R²: 0.413456
Epoch 56/300
Train Loss: 0.00076222, Train R²: 0.999636, Val Loss: 1.26047957, Val R²: 0.411900
Epoch 57/300
Train Loss: 0.00065904, Train R²: 0.999685, Val Loss: 1.26243281, Val R²: 0.410989
Epoch 58/300
Train Loss: 0.00072623, Train R²: 0.999653, Val Loss: 1.26353085, Val R²: 0.410476
Epoch 59/300
Train Loss: 0.00061445, Train R²: 0.999707, Val Loss: 1.26296628, Val R²: 0.410740
Epoch 60/300
Train Loss: 0.00059979, Train R²: 0.999714, Val Loss: 1.26307166, Val R²: 0.410691
Epoch 61/300
Train Loss: 0.00045865, Train R²: 0.999781, Val Loss: 1.26379526, Val R²: 0.410353
Epoch 62/300
Train Loss: 0.00060544, Train R²: 0.999711, Val Loss: 1.26395988, Val R²: 0.410276
Epoch 00062: reducing learning rate of group 0 to 1.2500e-04.
Epoch 63/300
Train Loss: 0.00048613, Train R²: 0.999768, Val Loss: 1.26398909, Val R²: 0.410263
Epoch 64/300
Train Loss: 0.00039025, Train R²: 0.999814, Val Loss: 1.26368940, Val R²: 0.410402
Epoch 65/300
Train Loss: 0.00067424, Train R²: 0.999678, Val Loss: 1.26349986, Val R²: 0.410491
Epoch 66/300
Train Loss: 0.00061948, Train R²: 0.999704, Val Loss: 1.26279533, Val R²: 0.410819
Epoch 67/300
Train Loss: 0.00036552, Train R²: 0.999826, Val Loss: 1.26288176, Val R²: 0.410779
Epoch 68/300
Train Loss: 0.00045573, Train R²: 0.999782, Val Loss: 1.26278532, Val R²: 0.410824
Epoch 69/300
Train Loss: 0.00035540, Train R²: 0.999830, Val Loss: 1.26276362, Val R²: 0.410834
Epoch 70/300
Train Loss: 0.00039057, Train R²: 0.999814, Val Loss: 1.26331985, Val R²: 0.410575
Epoch 71/300
Train Loss: 0.00031901, Train R²: 0.999848, Val Loss: 1.26317012, Val R²: 0.410645
Epoch 72/300
Train Loss: 0.00037443, Train R²: 0.999821, Val Loss: 1.26253450, Val R²: 0.410941
Epoch 73/300
Train Loss: 0.00035812, Train R²: 0.999829, Val Loss: 1.26240003, Val R²: 0.411004
Epoch 00073: reducing learning rate of group 0 to 6.2500e-05.
Epoch 74/300
Train Loss: 0.00027633, Train R²: 0.999868, Val Loss: 1.26274514, Val R²: 0.410843
Epoch 75/300
Train Loss: 0.00049681, Train R²: 0.999763, Val Loss: 1.26280892, Val R²: 0.410813
Epoch 76/300
Train Loss: 0.00036539, Train R²: 0.999826, Val Loss: 1.26313591, Val R²: 0.410661
Epoch 77/300
Train Loss: 0.00035325, Train R²: 0.999831, Val Loss: 1.26316559, Val R²: 0.410647
Epoch 78/300
Train Loss: 0.00055641, Train R²: 0.999734, Val Loss: 1.26282680, Val R²: 0.410805
Epoch 79/300
Train Loss: 0.00038845, Train R²: 0.999815, Val Loss: 1.26344347, Val R²: 0.410517
Epoch 80/300
Train Loss: 0.00090252, Train R²: 0.999569, Val Loss: 1.26340306, Val R²: 0.410536
Epoch 81/300
Train Loss: 0.00026110, Train R²: 0.999875, Val Loss: 1.26371229, Val R²: 0.410392
Epoch 82/300
Train Loss: 0.00027591, Train R²: 0.999868, Val Loss: 1.26382530, Val R²: 0.410339
Epoch 83/300
Train Loss: 0.00026736, Train R²: 0.999872, Val Loss: 1.26387691, Val R²: 0.410315
Epoch 84/300
Train Loss: 0.00029959, Train R²: 0.999857, Val Loss: 1.26381159, Val R²: 0.410345
Epoch 00084: reducing learning rate of group 0 to 3.1250e-05.
Epoch 85/300
Train Loss: 0.00080943, Train R²: 0.999614, Val Loss: 1.26399195, Val R²: 0.410261
Epoch 86/300
Train Loss: 0.00035598, Train R²: 0.999830, Val Loss: 1.26439524, Val R²: 0.410073
Epoch 87/300
Train Loss: 0.00021508, Train R²: 0.999897, Val Loss: 1.26420474, Val R²: 0.410162
Epoch 88/300
Train Loss: 0.00038450, Train R²: 0.999816, Val Loss: 1.26380038, Val R²: 0.410351
Epoch 89/300
Train Loss: 0.00061818, Train R²: 0.999705, Val Loss: 1.26327980, Val R²: 0.410594
Epoch 90/300
Train Loss: 0.00028239, Train R²: 0.999865, Val Loss: 1.26350367, Val R²: 0.410489
Epoch 91/300
Train Loss: 0.00021944, Train R²: 0.999895, Val Loss: 1.26399612, Val R²: 0.410259
Epoch 92/300
Train Loss: 0.00059115, Train R²: 0.999718, Val Loss: 1.26419389, Val R²: 0.410167
Epoch 93/300
Train Loss: 0.00022293, Train R²: 0.999894, Val Loss: 1.26400316, Val R²: 0.410256
Epoch 94/300
Train Loss: 0.00031950, Train R²: 0.999847, Val Loss: 1.26368463, Val R²: 0.410405
Epoch 95/300
Train Loss: 0.00042717, Train R²: 0.999796, Val Loss: 1.26381838, Val R²: 0.410342
Epoch 00095: reducing learning rate of group 0 to 1.5625e-05.
Epoch 96/300
Train Loss: 0.00029067, Train R²: 0.999861, Val Loss: 1.26375604, Val R²: 0.410371
Epoch 97/300
Train Loss: 0.00045514, Train R²: 0.999783, Val Loss: 1.26397133, Val R²: 0.410271
Epoch 98/300
Train Loss: 0.00040060, Train R²: 0.999809, Val Loss: 1.26371491, Val R²: 0.410390
Epoch 99/300
Train Loss: 0.00024377, Train R²: 0.999884, Val Loss: 1.26374245, Val R²: 0.410378
Epoch 100/300
Train Loss: 0.00022340, Train R²: 0.999893, Val Loss: 1.26392460, Val R²: 0.410293
Epoch 101/300
Train Loss: 0.00034964, Train R²: 0.999833, Val Loss: 1.26387513, Val R²: 0.410316
Epoch 102/300
Train Loss: 0.00041940, Train R²: 0.999800, Val Loss: 1.26390123, Val R²: 0.410304
Epoch 103/300
Train Loss: 0.00025685, Train R²: 0.999877, Val Loss: 1.26404130, Val R²: 0.410238
Epoch 104/300
Train Loss: 0.00028383, Train R²: 0.999865, Val Loss: 1.26406670, Val R²: 0.410226
Epoch 105/300
Train Loss: 0.00023571, Train R²: 0.999887, Val Loss: 1.26397371, Val R²: 0.410270
Epoch 106/300
Train Loss: 0.00025235, Train R²: 0.999880, Val Loss: 1.26403594, Val R²: 0.410241
Epoch 00106: reducing learning rate of group 0 to 7.8125e-06.
Epoch 107/300
Train Loss: 0.00020402, Train R²: 0.999903, Val Loss: 1.26406324, Val R²: 0.410228
Epoch 108/300
Train Loss: 0.00026681, Train R²: 0.999873, Val Loss: 1.26432633, Val R²: 0.410105
Epoch 109/300
Train Loss: 0.00020678, Train R²: 0.999901, Val Loss: 1.26407123, Val R²: 0.410224
Epoch 110/300
Train Loss: 0.00024622, Train R²: 0.999882, Val Loss: 1.26421273, Val R²: 0.410158
Epoch 111/300
Train Loss: 0.00021598, Train R²: 0.999897, Val Loss: 1.26405954, Val R²: 0.410230
Epoch 112/300
Train Loss: 0.00068793, Train R²: 0.999672, Val Loss: 1.26416790, Val R²: 0.410179
Epoch 113/300
Train Loss: 0.00034357, Train R²: 0.999836, Val Loss: 1.26416993, Val R²: 0.410178
Epoch 114/300
Train Loss: 0.00055465, Train R²: 0.999735, Val Loss: 1.26413703, Val R²: 0.410194
Epoch 115/300
Train Loss: 0.00027448, Train R²: 0.999869, Val Loss: 1.26415908, Val R²: 0.410183
Epoch 116/300
Train Loss: 0.00044191, Train R²: 0.999789, Val Loss: 1.26415920, Val R²: 0.410183
Epoch 117/300
Train Loss: 0.00025454, Train R²: 0.999878, Val Loss: 1.26419580, Val R²: 0.410166
Epoch 00117: reducing learning rate of group 0 to 3.9063e-06.
Epoch 118/300
Train Loss: 0.00040180, Train R²: 0.999808, Val Loss: 1.26409829, Val R²: 0.410212
Epoch 119/300
Train Loss: 0.00041838, Train R²: 0.999800, Val Loss: 1.26412797, Val R²: 0.410198
Epoch 120/300
Train Loss: 0.00030355, Train R²: 0.999855, Val Loss: 1.26415122, Val R²: 0.410187
Epoch 121/300
Train Loss: 0.00025365, Train R²: 0.999879, Val Loss: 1.26429951, Val R²: 0.410118
Epoch 122/300
Train Loss: 0.00039368, Train R²: 0.999812, Val Loss: 1.26417851, Val R²: 0.410174
Epoch 123/300
Train Loss: 0.00041478, Train R²: 0.999802, Val Loss: 1.26419199, Val R²: 0.410168
Epoch 124/300
Train Loss: 0.00032232, Train R²: 0.999846, Val Loss: 1.26432061, Val R²: 0.410108
Epoch 125/300
Train Loss: 0.00037449, Train R²: 0.999821, Val Loss: 1.26429009, Val R²: 0.410122
Epoch 126/300
Train Loss: 0.00036607, Train R²: 0.999825, Val Loss: 1.26415849, Val R²: 0.410183
Epoch 127/300
Train Loss: 0.00029234, Train R²: 0.999860, Val Loss: 1.26453507, Val R²: 0.410008
Epoch 128/300
Train Loss: 0.00027713, Train R²: 0.999868, Val Loss: 1.26435244, Val R²: 0.410093
Epoch 00128: reducing learning rate of group 0 to 1.9531e-06.
Epoch 129/300
Train Loss: 0.00031687, Train R²: 0.999849, Val Loss: 1.26423669, Val R²: 0.410147
Epoch 130/300
Train Loss: 0.00026986, Train R²: 0.999871, Val Loss: 1.26425397, Val R²: 0.410139
Epoch 131/300
Train Loss: 0.00036970, Train R²: 0.999824, Val Loss: 1.26427269, Val R²: 0.410130
Epoch 132/300
Train Loss: 0.00023035, Train R²: 0.999890, Val Loss: 1.26442719, Val R²: 0.410058
Epoch 133/300
Train Loss: 0.00044835, Train R²: 0.999786, Val Loss: 1.26444566, Val R²: 0.410050
Epoch 134/300
Train Loss: 0.00022978, Train R²: 0.999890, Val Loss: 1.26432335, Val R²: 0.410107
Epoch 135/300
Train Loss: 0.00039533, Train R²: 0.999811, Val Loss: 1.26446128, Val R²: 0.410042
Epoch 136/300
Train Loss: 0.00023681, Train R²: 0.999887, Val Loss: 1.26434720, Val R²: 0.410095
Epoch 137/300
Train Loss: 0.00032913, Train R²: 0.999843, Val Loss: 1.26435792, Val R²: 0.410090
Epoch 138/300
Train Loss: 0.00033833, Train R²: 0.999839, Val Loss: 1.26448023, Val R²: 0.410033
Epoch 139/300
Train Loss: 0.00019966, Train R²: 0.999905, Val Loss: 1.26434994, Val R²: 0.410094
Epoch 00139: reducing learning rate of group 0 to 9.7656e-07.
Epoch 140/300
Train Loss: 0.00025539, Train R²: 0.999878, Val Loss: 1.26460588, Val R²: 0.409975
Epoch 141/300
Train Loss: 0.00020577, Train R²: 0.999902, Val Loss: 1.26448405, Val R²: 0.410032
Epoch 142/300
Train Loss: 0.00023243, Train R²: 0.999889, Val Loss: 1.26460505, Val R²: 0.409975
Epoch 143/300
Train Loss: 0.00023333, Train R²: 0.999889, Val Loss: 1.26434886, Val R²: 0.410095
Epoch 144/300
Train Loss: 0.00020957, Train R²: 0.999900, Val Loss: 1.26435435, Val R²: 0.410092
Epoch 145/300
Train Loss: 0.00023145, Train R²: 0.999890, Val Loss: 1.26435769, Val R²: 0.410091
Epoch 146/300
Train Loss: 0.00028163, Train R²: 0.999866, Val Loss: 1.26449907, Val R²: 0.410025
Epoch 147/300
Train Loss: 0.00025798, Train R²: 0.999877, Val Loss: 1.26462388, Val R²: 0.409966
Epoch 148/300
Train Loss: 0.00024296, Train R²: 0.999884, Val Loss: 1.26436460, Val R²: 0.410087
Epoch 149/300
Train Loss: 0.00039514, Train R²: 0.999811, Val Loss: 1.26450276, Val R²: 0.410023
Epoch 150/300
Train Loss: 0.00024011, Train R²: 0.999885, Val Loss: 1.26436901, Val R²: 0.410085
Epoch 00150: reducing learning rate of group 0 to 4.8828e-07.
Epoch 151/300
Train Loss: 0.00035430, Train R²: 0.999831, Val Loss: 1.26498079, Val R²: 0.409800
Epoch 152/300
Train Loss: 0.00032949, Train R²: 0.999843, Val Loss: 1.26449466, Val R²: 0.410027
Epoch 153/300
Train Loss: 0.00024023, Train R²: 0.999885, Val Loss: 1.26451218, Val R²: 0.410018
Epoch 154/300
Train Loss: 0.00022628, Train R²: 0.999892, Val Loss: 1.26484501, Val R²: 0.409863
Epoch 155/300
Train Loss: 0.00025706, Train R²: 0.999877, Val Loss: 1.26472449, Val R²: 0.409919
Epoch 156/300
Train Loss: 0.00025376, Train R²: 0.999879, Val Loss: 1.26463532, Val R²: 0.409961
Epoch 157/300
Train Loss: 0.00021057, Train R²: 0.999899, Val Loss: 1.26451552, Val R²: 0.410017
Epoch 158/300
Train Loss: 0.00021082, Train R²: 0.999899, Val Loss: 1.26437545, Val R²: 0.410082
Epoch 159/300
Train Loss: 0.00020828, Train R²: 0.999901, Val Loss: 1.26437175, Val R²: 0.410084
Epoch 160/300
Train Loss: 0.00043479, Train R²: 0.999792, Val Loss: 1.26497924, Val R²: 0.409801
Epoch 161/300
Train Loss: 0.00041510, Train R²: 0.999802, Val Loss: 1.26448977, Val R²: 0.410029
Epoch 00161: reducing learning rate of group 0 to 2.4414e-07.
Epoch 162/300
Train Loss: 0.00043634, Train R²: 0.999792, Val Loss: 1.26450646, Val R²: 0.410021
Epoch 163/300
Train Loss: 0.00024250, Train R²: 0.999884, Val Loss: 1.26436710, Val R²: 0.410086
Epoch 164/300
Train Loss: 0.00023288, Train R²: 0.999889, Val Loss: 1.26436663, Val R²: 0.410086
Epoch 165/300
Train Loss: 0.00025967, Train R²: 0.999876, Val Loss: 1.26448762, Val R²: 0.410030
Epoch 166/300
Train Loss: 0.00029660, Train R²: 0.999858, Val Loss: 1.26436508, Val R²: 0.410087
Epoch 167/300
Train Loss: 0.00024406, Train R²: 0.999884, Val Loss: 1.26450312, Val R²: 0.410023
Epoch 168/300
Train Loss: 0.00034143, Train R²: 0.999837, Val Loss: 1.26462448, Val R²: 0.409966
Epoch 169/300
Train Loss: 0.00023957, Train R²: 0.999886, Val Loss: 1.26485217, Val R²: 0.409860
Epoch 170/300
Train Loss: 0.00036658, Train R²: 0.999825, Val Loss: 1.26436472, Val R²: 0.410087
Epoch 171/300
Train Loss: 0.00042683, Train R²: 0.999796, Val Loss: 1.26450348, Val R²: 0.410023
Epoch 172/300
Train Loss: 0.00020466, Train R²: 0.999902, Val Loss: 1.26450324, Val R²: 0.410023
Epoch 00172: reducing learning rate of group 0 to 1.2207e-07.
Epoch 173/300
Train Loss: 0.00031591, Train R²: 0.999849, Val Loss: 1.26462460, Val R²: 0.409966
Epoch 174/300
Train Loss: 0.00025286, Train R²: 0.999879, Val Loss: 1.26471341, Val R²: 0.409925
Epoch 175/300
Train Loss: 0.00033767, Train R²: 0.999839, Val Loss: 1.26471364, Val R²: 0.409925
Epoch 176/300
Train Loss: 0.00042354, Train R²: 0.999798, Val Loss: 1.26436460, Val R²: 0.410087
Epoch 177/300
Train Loss: 0.00033824, Train R²: 0.999839, Val Loss: 1.26436400, Val R²: 0.410088
Epoch 178/300
Train Loss: 0.00034089, Train R²: 0.999837, Val Loss: 1.26448476, Val R²: 0.410031
Epoch 179/300
Train Loss: 0.00024926, Train R²: 0.999881, Val Loss: 1.26462257, Val R²: 0.409967
Epoch 180/300
Train Loss: 0.00023309, Train R²: 0.999889, Val Loss: 1.26448357, Val R²: 0.410032
Epoch 181/300
Train Loss: 0.00043946, Train R²: 0.999790, Val Loss: 1.26471043, Val R²: 0.409926
Epoch 182/300
Train Loss: 0.00028614, Train R²: 0.999863, Val Loss: 1.26462126, Val R²: 0.409968
Epoch 183/300
Train Loss: 0.00023544, Train R²: 0.999888, Val Loss: 1.26471019, Val R²: 0.409926
Epoch 00183: reducing learning rate of group 0 to 6.1035e-08.
Epoch 184/300
Train Loss: 0.00068569, Train R²: 0.999673, Val Loss: 1.26436102, Val R²: 0.410089
Epoch 185/300
Train Loss: 0.00025176, Train R²: 0.999880, Val Loss: 1.26436114, Val R²: 0.410089
Epoch 186/300
Train Loss: 0.00022031, Train R²: 0.999895, Val Loss: 1.26471007, Val R²: 0.409926
Epoch 187/300
Train Loss: 0.00027638, Train R²: 0.999868, Val Loss: 1.26436114, Val R²: 0.410089
Epoch 188/300
Train Loss: 0.00036876, Train R²: 0.999824, Val Loss: 1.26449931, Val R²: 0.410024
Epoch 189/300
Train Loss: 0.00029462, Train R²: 0.999859, Val Loss: 1.26448274, Val R²: 0.410032
Epoch 190/300
Train Loss: 0.00028455, Train R²: 0.999864, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 191/300
Train Loss: 0.00026148, Train R²: 0.999875, Val Loss: 1.26436114, Val R²: 0.410089
Epoch 192/300
Train Loss: 0.00044973, Train R²: 0.999785, Val Loss: 1.26462078, Val R²: 0.409968
Epoch 193/300
Train Loss: 0.00023205, Train R²: 0.999889, Val Loss: 1.26470995, Val R²: 0.409926
Epoch 194/300
Train Loss: 0.00030921, Train R²: 0.999852, Val Loss: 1.26448238, Val R²: 0.410032
Epoch 00194: reducing learning rate of group 0 to 3.0518e-08.
Epoch 195/300
Train Loss: 0.00040667, Train R²: 0.999806, Val Loss: 1.26448214, Val R²: 0.410032
Epoch 196/300
Train Loss: 0.00028516, Train R²: 0.999864, Val Loss: 1.26448226, Val R²: 0.410032
Epoch 197/300
Train Loss: 0.00028548, Train R²: 0.999864, Val Loss: 1.26448250, Val R²: 0.410032
Epoch 198/300
Train Loss: 0.00029504, Train R²: 0.999859, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 199/300
Train Loss: 0.00022048, Train R²: 0.999895, Val Loss: 1.26448250, Val R²: 0.410032
Epoch 200/300
Train Loss: 0.00044373, Train R²: 0.999788, Val Loss: 1.26449895, Val R²: 0.410025
Epoch 201/300
Train Loss: 0.00027727, Train R²: 0.999868, Val Loss: 1.26448226, Val R²: 0.410032
Epoch 202/300
Train Loss: 0.00025195, Train R²: 0.999880, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 203/300
Train Loss: 0.00027260, Train R²: 0.999870, Val Loss: 1.26436043, Val R²: 0.410089
Epoch 204/300
Train Loss: 0.00022382, Train R²: 0.999893, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 205/300
Train Loss: 0.00020364, Train R²: 0.999903, Val Loss: 1.26470971, Val R²: 0.409926
Epoch 00205: reducing learning rate of group 0 to 1.5259e-08.
Epoch 206/300
Train Loss: 0.00021969, Train R²: 0.999895, Val Loss: 1.26449919, Val R²: 0.410025
Epoch 207/300
Train Loss: 0.00038925, Train R²: 0.999814, Val Loss: 1.26436067, Val R²: 0.410089
Epoch 208/300
Train Loss: 0.00026506, Train R²: 0.999873, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 209/300
Train Loss: 0.00022550, Train R²: 0.999892, Val Loss: 1.26436067, Val R²: 0.410089
Epoch 210/300
Train Loss: 0.00038730, Train R²: 0.999815, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 211/300
Train Loss: 0.00036079, Train R²: 0.999828, Val Loss: 1.26470971, Val R²: 0.409926
Epoch 212/300
Train Loss: 0.00023778, Train R²: 0.999887, Val Loss: 1.26436067, Val R²: 0.410089
Epoch 213/300
Train Loss: 0.00025536, Train R²: 0.999878, Val Loss: 1.26436126, Val R²: 0.410089
Epoch 214/300
Train Loss: 0.00023993, Train R²: 0.999885, Val Loss: 1.26497006, Val R²: 0.409805
Epoch 215/300
Train Loss: 0.00026131, Train R²: 0.999875, Val Loss: 1.26448250, Val R²: 0.410032
Epoch 216/300
Train Loss: 0.00021893, Train R²: 0.999895, Val Loss: 1.26449931, Val R²: 0.410025
Epoch 217/300
Train Loss: 0.00020800, Train R²: 0.999901, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 218/300
Train Loss: 0.00030081, Train R²: 0.999856, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 219/300
Train Loss: 0.00060047, Train R²: 0.999713, Val Loss: 1.26448250, Val R²: 0.410032
Epoch 220/300
Train Loss: 0.00031093, Train R²: 0.999852, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 221/300
Train Loss: 0.00045604, Train R²: 0.999782, Val Loss: 1.26449931, Val R²: 0.410024
Epoch 222/300
Train Loss: 0.00021035, Train R²: 0.999900, Val Loss: 1.26448250, Val R²: 0.410032
Epoch 223/300
Train Loss: 0.00030599, Train R²: 0.999854, Val Loss: 1.26449907, Val R²: 0.410025
Epoch 224/300
Train Loss: 0.00022463, Train R²: 0.999893, Val Loss: 1.26436114, Val R²: 0.410089
Epoch 225/300
Train Loss: 0.00025541, Train R²: 0.999878, Val Loss: 1.26448274, Val R²: 0.410032
Epoch 226/300
Train Loss: 0.00026405, Train R²: 0.999874, Val Loss: 1.26448226, Val R²: 0.410032
Epoch 227/300
Train Loss: 0.00035397, Train R²: 0.999831, Val Loss: 1.26483178, Val R²: 0.409869
Epoch 228/300
Train Loss: 0.00024838, Train R²: 0.999881, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 229/300
Train Loss: 0.00028512, Train R²: 0.999864, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 230/300
Train Loss: 0.00025415, Train R²: 0.999879, Val Loss: 1.26436114, Val R²: 0.410089
Epoch 231/300
Train Loss: 0.00018702, Train R²: 0.999911, Val Loss: 1.26449907, Val R²: 0.410025
Epoch 232/300
Train Loss: 0.00036680, Train R²: 0.999825, Val Loss: 1.26449907, Val R²: 0.410025
Epoch 233/300
Train Loss: 0.00019637, Train R²: 0.999906, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 234/300
Train Loss: 0.00041432, Train R²: 0.999802, Val Loss: 1.26448250, Val R²: 0.410032
Epoch 235/300
Train Loss: 0.00025176, Train R²: 0.999880, Val Loss: 1.26436067, Val R²: 0.410089
Epoch 236/300
Train Loss: 0.00024957, Train R²: 0.999881, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 237/300
Train Loss: 0.00033231, Train R²: 0.999841, Val Loss: 1.26436114, Val R²: 0.410089
Epoch 238/300
Train Loss: 0.00023048, Train R²: 0.999890, Val Loss: 1.26449931, Val R²: 0.410024
Epoch 239/300
Train Loss: 0.00022600, Train R²: 0.999892, Val Loss: 1.26448250, Val R²: 0.410032
Epoch 240/300
Train Loss: 0.00054895, Train R²: 0.999738, Val Loss: 1.26470995, Val R²: 0.409926
Epoch 241/300
Train Loss: 0.00027551, Train R²: 0.999868, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 242/300
Train Loss: 0.00021708, Train R²: 0.999896, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 243/300
Train Loss: 0.00031578, Train R²: 0.999849, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 244/300
Train Loss: 0.00024945, Train R²: 0.999881, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 245/300
Train Loss: 0.00042633, Train R²: 0.999796, Val Loss: 1.26436114, Val R²: 0.410089
Epoch 246/300
Train Loss: 0.00027918, Train R²: 0.999867, Val Loss: 1.26449919, Val R²: 0.410025
Epoch 247/300
Train Loss: 0.00038252, Train R²: 0.999817, Val Loss: 1.26470959, Val R²: 0.409926
Epoch 248/300
Train Loss: 0.00022049, Train R²: 0.999895, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 249/300
Train Loss: 0.00022945, Train R²: 0.999890, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 250/300
Train Loss: 0.00027203, Train R²: 0.999870, Val Loss: 1.26448238, Val R²: 0.410032
Epoch 251/300
Train Loss: 0.00023068, Train R²: 0.999890, Val Loss: 1.26448238, Val R²: 0.410032
Epoch 252/300
Train Loss: 0.00028623, Train R²: 0.999863, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 253/300
Train Loss: 0.00029543, Train R²: 0.999859, Val Loss: 1.26483130, Val R²: 0.409870
Epoch 254/300
Train Loss: 0.00027161, Train R²: 0.999870, Val Loss: 1.26448238, Val R²: 0.410032
Epoch 255/300
Train Loss: 0.00022357, Train R²: 0.999893, Val Loss: 1.26449931, Val R²: 0.410024
Epoch 256/300
Train Loss: 0.00065574, Train R²: 0.999687, Val Loss: 1.26449931, Val R²: 0.410024
Epoch 257/300
Train Loss: 0.00040461, Train R²: 0.999807, Val Loss: 1.26436043, Val R²: 0.410089
Epoch 258/300
Train Loss: 0.00026988, Train R²: 0.999871, Val Loss: 1.26448238, Val R²: 0.410032
Epoch 259/300
Train Loss: 0.00021289, Train R²: 0.999898, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 260/300
Train Loss: 0.00026248, Train R²: 0.999875, Val Loss: 1.26448250, Val R²: 0.410032
Epoch 261/300
Train Loss: 0.00020594, Train R²: 0.999902, Val Loss: 1.26471007, Val R²: 0.409926
Epoch 262/300
Train Loss: 0.00023383, Train R²: 0.999888, Val Loss: 1.26436114, Val R²: 0.410089
Epoch 263/300
Train Loss: 0.00023806, Train R²: 0.999886, Val Loss: 1.26436067, Val R²: 0.410089
Epoch 264/300
Train Loss: 0.00039578, Train R²: 0.999811, Val Loss: 1.26470995, Val R²: 0.409926
Epoch 265/300
Train Loss: 0.00057720, Train R²: 0.999724, Val Loss: 1.26449907, Val R²: 0.410025
Epoch 266/300
Train Loss: 0.00024156, Train R²: 0.999885, Val Loss: 1.26484811, Val R²: 0.409862
Epoch 267/300
Train Loss: 0.00029243, Train R²: 0.999860, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 268/300
Train Loss: 0.00023344, Train R²: 0.999889, Val Loss: 1.26436067, Val R²: 0.410089
Epoch 269/300
Train Loss: 0.00046370, Train R²: 0.999779, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 270/300
Train Loss: 0.00024170, Train R²: 0.999885, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 271/300
Train Loss: 0.00020059, Train R²: 0.999904, Val Loss: 1.26449919, Val R²: 0.410025
Epoch 272/300
Train Loss: 0.00051658, Train R²: 0.999753, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 273/300
Train Loss: 0.00035646, Train R²: 0.999830, Val Loss: 1.26470971, Val R²: 0.409926
Epoch 274/300
Train Loss: 0.00024580, Train R²: 0.999883, Val Loss: 1.26483130, Val R²: 0.409870
Epoch 275/300
Train Loss: 0.00023980, Train R²: 0.999886, Val Loss: 1.26436043, Val R²: 0.410089
Epoch 276/300
Train Loss: 0.00037907, Train R²: 0.999819, Val Loss: 1.26471007, Val R²: 0.409926
Epoch 277/300
Train Loss: 0.00032803, Train R²: 0.999843, Val Loss: 1.26448250, Val R²: 0.410032
Epoch 278/300
Train Loss: 0.00027833, Train R²: 0.999867, Val Loss: 1.26436114, Val R²: 0.410089
Epoch 279/300
Train Loss: 0.00025855, Train R²: 0.999877, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 280/300
Train Loss: 0.00022200, Train R²: 0.999894, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 281/300
Train Loss: 0.00023490, Train R²: 0.999888, Val Loss: 1.26448226, Val R²: 0.410032
Epoch 282/300
Train Loss: 0.00023487, Train R²: 0.999888, Val Loss: 1.26449931, Val R²: 0.410024
Epoch 283/300
Train Loss: 0.00040943, Train R²: 0.999805, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 284/300
Train Loss: 0.00026213, Train R²: 0.999875, Val Loss: 1.26449931, Val R²: 0.410025
Epoch 285/300
Train Loss: 0.00053089, Train R²: 0.999747, Val Loss: 1.26436067, Val R²: 0.410089
Epoch 286/300
Train Loss: 0.00027870, Train R²: 0.999867, Val Loss: 1.26470947, Val R²: 0.409926
Epoch 287/300
Train Loss: 0.00024270, Train R²: 0.999884, Val Loss: 1.26436067, Val R²: 0.410089
Epoch 288/300
Train Loss: 0.00029571, Train R²: 0.999859, Val Loss: 1.26436067, Val R²: 0.410089
Epoch 289/300
Train Loss: 0.00021287, Train R²: 0.999898, Val Loss: 1.26470971, Val R²: 0.409926
Epoch 290/300
Train Loss: 0.00022733, Train R²: 0.999891, Val Loss: 1.26436031, Val R²: 0.410089
Epoch 291/300
Train Loss: 0.00030677, Train R²: 0.999854, Val Loss: 1.26449919, Val R²: 0.410025
Epoch 292/300
Train Loss: 0.00024975, Train R²: 0.999881, Val Loss: 1.26462066, Val R²: 0.409968
Epoch 293/300
Train Loss: 0.00040748, Train R²: 0.999805, Val Loss: 1.26436079, Val R²: 0.410089
Epoch 294/300
Train Loss: 0.00022178, Train R²: 0.999894, Val Loss: 1.26436090, Val R²: 0.410089
Epoch 295/300
Train Loss: 0.00054634, Train R²: 0.999739, Val Loss: 1.26484811, Val R²: 0.409862
Epoch 296/300
Train Loss: 0.00021576, Train R²: 0.999897, Val Loss: 1.26449919, Val R²: 0.410025
Epoch 297/300
Train Loss: 0.00017999, Train R²: 0.999914, Val Loss: 1.26449931, Val R²: 0.410025
Epoch 298/300
Train Loss: 0.00021581, Train R²: 0.999897, Val Loss: 1.26484835, Val R²: 0.409862
Epoch 299/300
Train Loss: 0.00025506, Train R²: 0.999878, Val Loss: 1.26484847, Val R²: 0.409862
Epoch 300/300
Train Loss: 0.00065194, Train R²: 0.999689, Val Loss: 1.26449919, Val R²: 0.410025
Training Complete. Best Val Loss: 1.1676753759384155
训练时间: 620.16 秒
