Using device: cuda
Total samples: 1000, Training: 800, Validation: 200
Total trainable parameters: 1477889
Epoch 1/200
Train Loss: 5.08601383, Train R²: -1.420139, Val Loss: 2.25944304, Val R²: -0.064250
Saved best model with validation R2 -0.064250 to best_finetuned_model.pth
Epoch 2/200
Train Loss: 2.23322299, Train R²: -0.062661, Val Loss: 2.12895894, Val R²: -0.002789
Saved best model with validation R2 -0.002789 to best_finetuned_model.pth
Epoch 3/200
Train Loss: 1.92281976, Train R²: 0.085042, Val Loss: 1.93102109, Val R²: 0.090444
Saved best model with validation R2 0.090444 to best_finetuned_model.pth
Epoch 4/200
Train Loss: 1.79101435, Train R²: 0.147760, Val Loss: 1.82297635, Val R²: 0.141336
Saved best model with validation R2 0.141336 to best_finetuned_model.pth
Epoch 5/200
Train Loss: 1.58486851, Train R²: 0.245853, Val Loss: 1.77609062, Val R²: 0.163420
Saved best model with validation R2 0.163420 to best_finetuned_model.pth
Epoch 6/200
Train Loss: 1.36565215, Train R²: 0.350165, Val Loss: 1.54782283, Val R²: 0.270939
Saved best model with validation R2 0.270939 to best_finetuned_model.pth
Epoch 7/200
Train Loss: 1.16070771, Train R²: 0.447687, Val Loss: 1.67893183, Val R²: 0.209184
Epoch 8/200
Train Loss: 0.95550239, Train R²: 0.545332, Val Loss: 2.13080311, Val R²: -0.003658
Epoch 9/200
Train Loss: 1.25495532, Train R²: 0.402840, Val Loss: 1.48513913, Val R²: 0.300465
Saved best model with validation R2 0.300465 to best_finetuned_model.pth
Epoch 10/200
Train Loss: 0.92064013, Train R²: 0.561921, Val Loss: 1.32352638, Val R²: 0.376588
Saved best model with validation R2 0.376588 to best_finetuned_model.pth
Epoch 11/200
Train Loss: 0.80157160, Train R²: 0.618579, Val Loss: 1.38723826, Val R²: 0.346578
Epoch 12/200
Train Loss: 0.69839392, Train R²: 0.667675, Val Loss: 1.20547295, Val R²: 0.432194
Saved best model with validation R2 0.432194 to best_finetuned_model.pth
Epoch 13/200
Train Loss: 0.56051292, Train R²: 0.733284, Val Loss: 1.40347803, Val R²: 0.338929
Epoch 14/200
Train Loss: 0.51950378, Train R²: 0.752798, Val Loss: 1.15347552, Val R²: 0.456686
Saved best model with validation R2 0.456686 to best_finetuned_model.pth
Epoch 15/200
Train Loss: 0.37798045, Train R²: 0.820141, Val Loss: 1.15959048, Val R²: 0.453806
Epoch 16/200
Train Loss: 0.31885783, Train R²: 0.848274, Val Loss: 1.13689494, Val R²: 0.464496
Saved best model with validation R2 0.464496 to best_finetuned_model.pth
Epoch 17/200
Train Loss: 0.28834704, Train R²: 0.862792, Val Loss: 1.18692446, Val R²: 0.440931
Epoch 18/200
Train Loss: 0.26722306, Train R²: 0.872844, Val Loss: 1.19412494, Val R²: 0.437539
Epoch 19/200
Train Loss: 0.24778972, Train R²: 0.882091, Val Loss: 1.21663153, Val R²: 0.426938
Epoch 20/200
Train Loss: 0.21954087, Train R²: 0.895533, Val Loss: 1.26705337, Val R²: 0.403188
Epoch 21/200
Train Loss: 0.21688663, Train R²: 0.896796, Val Loss: 1.24401855, Val R²: 0.414038
Epoch 22/200
Train Loss: 0.17129104, Train R²: 0.918493, Val Loss: 1.19626260, Val R²: 0.436532
Epoch 23/200
Train Loss: 0.13838681, Train R²: 0.934150, Val Loss: 1.21712244, Val R²: 0.426707
Epoch 24/200
Train Loss: 0.12916914, Train R²: 0.938536, Val Loss: 1.33541286, Val R²: 0.370989
Epoch 25/200
Train Loss: 0.14912908, Train R²: 0.929038, Val Loss: 1.31455934, Val R²: 0.380812
Epoch 26/200
Train Loss: 0.16732425, Train R²: 0.920380, Val Loss: 1.27431536, Val R²: 0.399768
Epoch 27/200
Train Loss: 0.15557086, Train R²: 0.925973, Val Loss: 1.28317153, Val R²: 0.395596
Epoch 00027: reducing learning rate of group 0 to 5.0000e-04.
Epoch 28/200
Train Loss: 0.13544265, Train R²: 0.935551, Val Loss: 1.25302756, Val R²: 0.409795
Epoch 29/200
Train Loss: 0.11062346, Train R²: 0.947361, Val Loss: 1.24654138, Val R²: 0.412850
Epoch 30/200
Train Loss: 0.09696018, Train R²: 0.953862, Val Loss: 1.23492694, Val R²: 0.418321
Epoch 31/200
Train Loss: 0.07721993, Train R²: 0.963256, Val Loss: 1.22398257, Val R²: 0.423476
Epoch 32/200
Train Loss: 0.06740498, Train R²: 0.967926, Val Loss: 1.20015109, Val R²: 0.434701
Epoch 33/200
Train Loss: 0.05447549, Train R²: 0.974078, Val Loss: 1.18476868, Val R²: 0.441946
Epoch 34/200
Train Loss: 0.04439263, Train R²: 0.978876, Val Loss: 1.18843853, Val R²: 0.440218
Epoch 35/200
Train Loss: 0.04285174, Train R²: 0.979609, Val Loss: 1.16220319, Val R²: 0.452575
Epoch 36/200
Train Loss: 0.03788495, Train R²: 0.981973, Val Loss: 1.15498412, Val R²: 0.455976
Epoch 37/200
Train Loss: 0.02938780, Train R²: 0.986016, Val Loss: 1.17158294, Val R²: 0.448157
Epoch 38/200
Train Loss: 0.02539727, Train R²: 0.987915, Val Loss: 1.17379189, Val R²: 0.447117
Epoch 00038: reducing learning rate of group 0 to 2.5000e-04.
Epoch 39/200
Train Loss: 0.02226781, Train R²: 0.989404, Val Loss: 1.17740798, Val R²: 0.445413
Epoch 40/200
Train Loss: 0.01807190, Train R²: 0.991401, Val Loss: 1.16191840, Val R²: 0.452709
Epoch 41/200
Train Loss: 0.01605950, Train R²: 0.992358, Val Loss: 1.16590905, Val R²: 0.450830
Epoch 42/200
Train Loss: 0.01501387, Train R²: 0.992856, Val Loss: 1.15917587, Val R²: 0.454001
Epoch 43/200
Train Loss: 0.01210161, Train R²: 0.994242, Val Loss: 1.16395831, Val R²: 0.451749
Epoch 44/200
Train Loss: 0.00984423, Train R²: 0.995316, Val Loss: 1.16070879, Val R²: 0.453279
Epoch 45/200
Train Loss: 0.00891535, Train R²: 0.995758, Val Loss: 1.16366458, Val R²: 0.451887
Epoch 46/200
Train Loss: 0.00764436, Train R²: 0.996362, Val Loss: 1.15753829, Val R²: 0.454772
Epoch 47/200
Train Loss: 0.00680375, Train R²: 0.996762, Val Loss: 1.16043687, Val R²: 0.453407
Epoch 48/200
Train Loss: 0.00601362, Train R²: 0.997138, Val Loss: 1.15837073, Val R²: 0.454380
Epoch 49/200
Train Loss: 0.00565135, Train R²: 0.997311, Val Loss: 1.16415775, Val R²: 0.451655
Epoch 00049: reducing learning rate of group 0 to 1.2500e-04.
Epoch 50/200
Train Loss: 0.00458701, Train R²: 0.997817, Val Loss: 1.16212296, Val R²: 0.452613
Epoch 51/200
Train Loss: 0.00437642, Train R²: 0.997918, Val Loss: 1.16168678, Val R²: 0.452818
Epoch 52/200
Train Loss: 0.00372214, Train R²: 0.998229, Val Loss: 1.16278183, Val R²: 0.452303
Epoch 53/200
Train Loss: 0.00338382, Train R²: 0.998390, Val Loss: 1.15955520, Val R²: 0.453822
Epoch 54/200
Train Loss: 0.00304567, Train R²: 0.998551, Val Loss: 1.16093826, Val R²: 0.453171
Epoch 55/200
Train Loss: 0.00271941, Train R²: 0.998706, Val Loss: 1.15919292, Val R²: 0.453993
Epoch 56/200
Train Loss: 0.00262647, Train R²: 0.998750, Val Loss: 1.15750086, Val R²: 0.454790
Epoch 57/200
Train Loss: 0.00230691, Train R²: 0.998902, Val Loss: 1.16019428, Val R²: 0.453522
Epoch 58/200
Train Loss: 0.00222687, Train R²: 0.998940, Val Loss: 1.15926933, Val R²: 0.453957
Epoch 59/200
Train Loss: 0.00199510, Train R²: 0.999051, Val Loss: 1.15953135, Val R²: 0.453834
Epoch 60/200
Train Loss: 0.00188608, Train R²: 0.999103, Val Loss: 1.16096306, Val R²: 0.453159
Epoch 00060: reducing learning rate of group 0 to 6.2500e-05.
Epoch 61/200
Train Loss: 0.00223972, Train R²: 0.998934, Val Loss: 1.16392732, Val R²: 0.451763
Epoch 62/200
Train Loss: 0.00194077, Train R²: 0.999076, Val Loss: 1.16199958, Val R²: 0.452671
Epoch 63/200
Train Loss: 0.00146512, Train R²: 0.999303, Val Loss: 1.16166377, Val R²: 0.452829
Epoch 64/200
Train Loss: 0.00169975, Train R²: 0.999191, Val Loss: 1.16134679, Val R²: 0.452979
Epoch 65/200
Train Loss: 0.00174973, Train R²: 0.999167, Val Loss: 1.16321146, Val R²: 0.452100
Epoch 66/200
Train Loss: 0.00146759, Train R²: 0.999302, Val Loss: 1.16246486, Val R²: 0.452452
Epoch 67/200
Train Loss: 0.00124818, Train R²: 0.999406, Val Loss: 1.16424072, Val R²: 0.451616
Epoch 68/200
Train Loss: 0.00172055, Train R²: 0.999181, Val Loss: 1.16239369, Val R²: 0.452485
Epoch 69/200
Train Loss: 0.00144177, Train R²: 0.999314, Val Loss: 1.16394508, Val R²: 0.451755
Epoch 70/200
Train Loss: 0.00117907, Train R²: 0.999439, Val Loss: 1.16214323, Val R²: 0.452603
Epoch 71/200
Train Loss: 0.00120729, Train R²: 0.999426, Val Loss: 1.16526735, Val R²: 0.451132
Epoch 00071: reducing learning rate of group 0 to 3.1250e-05.
Epoch 72/200
Train Loss: 0.00119997, Train R²: 0.999429, Val Loss: 1.16196191, Val R²: 0.452689
Epoch 73/200
Train Loss: 0.00146127, Train R²: 0.999305, Val Loss: 1.16310370, Val R²: 0.452151
Epoch 74/200
Train Loss: 0.00131861, Train R²: 0.999373, Val Loss: 1.16318691, Val R²: 0.452112
Epoch 75/200
Train Loss: 0.00098708, Train R²: 0.999530, Val Loss: 1.16000772, Val R²: 0.453609
Epoch 76/200
Train Loss: 0.00131477, Train R²: 0.999374, Val Loss: 1.16292357, Val R²: 0.452236
Epoch 77/200
Train Loss: 0.00101372, Train R²: 0.999518, Val Loss: 1.16231048, Val R²: 0.452525
Epoch 78/200
Train Loss: 0.00096306, Train R²: 0.999542, Val Loss: 1.16194010, Val R²: 0.452699
Epoch 79/200
Train Loss: 0.00100327, Train R²: 0.999523, Val Loss: 1.16170561, Val R²: 0.452810
Epoch 80/200
Train Loss: 0.00113644, Train R²: 0.999459, Val Loss: 1.16259789, Val R²: 0.452389
Epoch 81/200
Train Loss: 0.00102058, Train R²: 0.999514, Val Loss: 1.16293848, Val R²: 0.452229
Epoch 82/200
Train Loss: 0.00091629, Train R²: 0.999564, Val Loss: 1.16372085, Val R²: 0.451860
Epoch 00082: reducing learning rate of group 0 to 1.5625e-05.
Epoch 83/200
Train Loss: 0.00110719, Train R²: 0.999473, Val Loss: 1.16366625, Val R²: 0.451886
Epoch 84/200
Train Loss: 0.00081320, Train R²: 0.999613, Val Loss: 1.16177487, Val R²: 0.452777
Epoch 85/200
Train Loss: 0.00110677, Train R²: 0.999473, Val Loss: 1.16331804, Val R²: 0.452050
Epoch 86/200
Train Loss: 0.00090054, Train R²: 0.999571, Val Loss: 1.16439569, Val R²: 0.451543
Epoch 87/200
Train Loss: 0.00082282, Train R²: 0.999608, Val Loss: 1.16508770, Val R²: 0.451217
Epoch 88/200
Train Loss: 0.00097456, Train R²: 0.999536, Val Loss: 1.16351986, Val R²: 0.451955
Epoch 89/200
Train Loss: 0.00113243, Train R²: 0.999461, Val Loss: 1.16288507, Val R²: 0.452254
Epoch 90/200
Train Loss: 0.00079774, Train R²: 0.999620, Val Loss: 1.16236198, Val R²: 0.452500
Epoch 91/200
Train Loss: 0.00071058, Train R²: 0.999662, Val Loss: 1.16292560, Val R²: 0.452235
Epoch 92/200
Train Loss: 0.00090337, Train R²: 0.999570, Val Loss: 1.16190064, Val R²: 0.452718
Epoch 93/200
Train Loss: 0.00083291, Train R²: 0.999604, Val Loss: 1.16379225, Val R²: 0.451827
Epoch 00093: reducing learning rate of group 0 to 7.8125e-06.
Epoch 94/200
Train Loss: 0.00075803, Train R²: 0.999639, Val Loss: 1.16378188, Val R²: 0.451832
Epoch 95/200
Train Loss: 0.00092443, Train R²: 0.999560, Val Loss: 1.16447663, Val R²: 0.451504
Epoch 96/200
Train Loss: 0.00068435, Train R²: 0.999674, Val Loss: 1.16375303, Val R²: 0.451845
Epoch 97/200
Train Loss: 0.00075558, Train R²: 0.999640, Val Loss: 1.16306102, Val R²: 0.452171
Epoch 98/200
Train Loss: 0.00090192, Train R²: 0.999571, Val Loss: 1.16492534, Val R²: 0.451293
Epoch 99/200
Train Loss: 0.00083178, Train R²: 0.999604, Val Loss: 1.16305280, Val R²: 0.452175
Epoch 100/200
Train Loss: 0.00100171, Train R²: 0.999523, Val Loss: 1.16249824, Val R²: 0.452436
Epoch 101/200
Train Loss: 0.00092640, Train R²: 0.999559, Val Loss: 1.16457415, Val R²: 0.451458
Epoch 102/200
Train Loss: 0.00077304, Train R²: 0.999632, Val Loss: 1.16198158, Val R²: 0.452680
Epoch 103/200
Train Loss: 0.00082524, Train R²: 0.999607, Val Loss: 1.16385221, Val R²: 0.451798
Epoch 104/200
Train Loss: 0.00077526, Train R²: 0.999631, Val Loss: 1.16335416, Val R²: 0.452033
Epoch 00104: reducing learning rate of group 0 to 3.9063e-06.
Epoch 105/200
Train Loss: 0.00104860, Train R²: 0.999501, Val Loss: 1.16366768, Val R²: 0.451885
Epoch 106/200
Train Loss: 0.00065144, Train R²: 0.999690, Val Loss: 1.16257417, Val R²: 0.452401
Epoch 107/200
Train Loss: 0.00069363, Train R²: 0.999670, Val Loss: 1.16181421, Val R²: 0.452758
Epoch 108/200
Train Loss: 0.00075654, Train R²: 0.999640, Val Loss: 1.16379499, Val R²: 0.451825
Epoch 109/200
Train Loss: 0.00071103, Train R²: 0.999662, Val Loss: 1.16377103, Val R²: 0.451837
Epoch 110/200
Train Loss: 0.00073626, Train R²: 0.999650, Val Loss: 1.16241169, Val R²: 0.452477
Epoch 111/200
Train Loss: 0.00074385, Train R²: 0.999646, Val Loss: 1.16361809, Val R²: 0.451909
Epoch 112/200
Train Loss: 0.00078312, Train R²: 0.999627, Val Loss: 1.16505098, Val R²: 0.451234
Epoch 113/200
Train Loss: 0.00090303, Train R²: 0.999570, Val Loss: 1.16251028, Val R²: 0.452431
Epoch 114/200
Train Loss: 0.00086419, Train R²: 0.999589, Val Loss: 1.16251278, Val R²: 0.452429
Epoch 115/200
Train Loss: 0.00084759, Train R²: 0.999597, Val Loss: 1.16252375, Val R²: 0.452424
Epoch 00115: reducing learning rate of group 0 to 1.9531e-06.
Epoch 116/200
Train Loss: 0.00076809, Train R²: 0.999635, Val Loss: 1.16375315, Val R²: 0.451845
Epoch 117/200
Train Loss: 0.00084798, Train R²: 0.999596, Val Loss: 1.16390276, Val R²: 0.451775
Epoch 118/200
Train Loss: 0.00083362, Train R²: 0.999603, Val Loss: 1.16509235, Val R²: 0.451214
Epoch 119/200
Train Loss: 0.00076525, Train R²: 0.999636, Val Loss: 1.16243303, Val R²: 0.452467
Epoch 120/200
Train Loss: 0.00060951, Train R²: 0.999710, Val Loss: 1.16490698, Val R²: 0.451302
Epoch 121/200
Train Loss: 0.00057834, Train R²: 0.999725, Val Loss: 1.16380465, Val R²: 0.451821
Epoch 122/200
Train Loss: 0.00069982, Train R²: 0.999667, Val Loss: 1.16380978, Val R²: 0.451818
Epoch 123/200
Train Loss: 0.00071713, Train R²: 0.999659, Val Loss: 1.16526711, Val R²: 0.451132
Epoch 124/200
Train Loss: 0.00066513, Train R²: 0.999684, Val Loss: 1.16380167, Val R²: 0.451822
Epoch 125/200
Train Loss: 0.00059951, Train R²: 0.999715, Val Loss: 1.16378391, Val R²: 0.451831
Epoch 126/200
Train Loss: 0.00059661, Train R²: 0.999716, Val Loss: 1.16253519, Val R²: 0.452419
Epoch 00126: reducing learning rate of group 0 to 9.7656e-07.
Epoch 127/200
Train Loss: 0.00069628, Train R²: 0.999669, Val Loss: 1.16375875, Val R²: 0.451842
Epoch 128/200
Train Loss: 0.00061527, Train R²: 0.999707, Val Loss: 1.16397262, Val R²: 0.451742
Epoch 129/200
Train Loss: 0.00084038, Train R²: 0.999600, Val Loss: 1.16535974, Val R²: 0.451088
Epoch 130/200
Train Loss: 0.00080607, Train R²: 0.999616, Val Loss: 1.16255474, Val R²: 0.452410
Epoch 131/200
Train Loss: 0.00070556, Train R²: 0.999664, Val Loss: 1.16534042, Val R²: 0.451097
Epoch 132/200
Train Loss: 0.00056581, Train R²: 0.999731, Val Loss: 1.16335642, Val R²: 0.452032
Epoch 133/200
Train Loss: 0.00073615, Train R²: 0.999650, Val Loss: 1.16536820, Val R²: 0.451084
Epoch 134/200
Train Loss: 0.00079829, Train R²: 0.999620, Val Loss: 1.16323042, Val R²: 0.452091
Epoch 135/200
Train Loss: 0.00080340, Train R²: 0.999618, Val Loss: 1.16465330, Val R²: 0.451421
Epoch 136/200
Train Loss: 0.00057903, Train R²: 0.999724, Val Loss: 1.16568935, Val R²: 0.450933
Epoch 137/200
Train Loss: 0.00071708, Train R²: 0.999659, Val Loss: 1.16618812, Val R²: 0.450698
Epoch 00137: reducing learning rate of group 0 to 4.8828e-07.
Epoch 138/200
Train Loss: 0.00061179, Train R²: 0.999709, Val Loss: 1.16383982, Val R²: 0.451804
Epoch 139/200
Train Loss: 0.00075170, Train R²: 0.999642, Val Loss: 1.16460752, Val R²: 0.451443
Epoch 140/200
Train Loss: 0.00061167, Train R²: 0.999709, Val Loss: 1.16460836, Val R²: 0.451442
Epoch 141/200
Train Loss: 0.00063295, Train R²: 0.999699, Val Loss: 1.16583276, Val R²: 0.450866
Epoch 142/200
Train Loss: 0.00071035, Train R²: 0.999662, Val Loss: 1.16393101, Val R²: 0.451761
Epoch 143/200
Train Loss: 0.00059030, Train R²: 0.999719, Val Loss: 1.16384315, Val R²: 0.451803
Epoch 144/200
Train Loss: 0.00066022, Train R²: 0.999686, Val Loss: 1.16384566, Val R²: 0.451802
Epoch 145/200
Train Loss: 0.00084161, Train R²: 0.999600, Val Loss: 1.16384363, Val R²: 0.451803
Epoch 146/200
Train Loss: 0.00080846, Train R²: 0.999615, Val Loss: 1.16368783, Val R²: 0.451876
Epoch 147/200
Train Loss: 0.00058552, Train R²: 0.999721, Val Loss: 1.16593730, Val R²: 0.450816
Epoch 148/200
Train Loss: 0.00062131, Train R²: 0.999704, Val Loss: 1.16322458, Val R²: 0.452094
Epoch 00148: reducing learning rate of group 0 to 2.4414e-07.
Epoch 149/200
Train Loss: 0.00071804, Train R²: 0.999658, Val Loss: 1.16262019, Val R²: 0.452379
Epoch 150/200
Train Loss: 0.00067467, Train R²: 0.999679, Val Loss: 1.16385210, Val R²: 0.451799
Epoch 151/200
Train Loss: 0.00060765, Train R²: 0.999711, Val Loss: 1.16369653, Val R²: 0.451872
Epoch 152/200
Train Loss: 0.00063495, Train R²: 0.999698, Val Loss: 1.16369629, Val R²: 0.451872
Epoch 153/200
Train Loss: 0.00059175, Train R²: 0.999718, Val Loss: 1.16483426, Val R²: 0.451336
Epoch 154/200
Train Loss: 0.00057284, Train R²: 0.999727, Val Loss: 1.16462183, Val R²: 0.451436
Epoch 155/200
Train Loss: 0.00083916, Train R²: 0.999601, Val Loss: 1.16200054, Val R²: 0.452671
Epoch 156/200
Train Loss: 0.00076007, Train R²: 0.999638, Val Loss: 1.16385293, Val R²: 0.451798
Epoch 157/200
Train Loss: 0.00059964, Train R²: 0.999715, Val Loss: 1.16369653, Val R²: 0.451872
Epoch 158/200
Train Loss: 0.00060864, Train R²: 0.999710, Val Loss: 1.16462350, Val R²: 0.451435
Epoch 159/200
Train Loss: 0.00084060, Train R²: 0.999600, Val Loss: 1.16395199, Val R²: 0.451751
Epoch 00159: reducing learning rate of group 0 to 1.2207e-07.
Epoch 160/200
Train Loss: 0.00067790, Train R²: 0.999677, Val Loss: 1.16262531, Val R²: 0.452376
Epoch 161/200
Train Loss: 0.00072639, Train R²: 0.999654, Val Loss: 1.16638601, Val R²: 0.450605
Epoch 162/200
Train Loss: 0.00068087, Train R²: 0.999676, Val Loss: 1.16385520, Val R²: 0.451797
Epoch 163/200
Train Loss: 0.00077115, Train R²: 0.999633, Val Loss: 1.16307592, Val R²: 0.452164
Epoch 164/200
Train Loss: 0.00063619, Train R²: 0.999697, Val Loss: 1.16389799, Val R²: 0.451777
Epoch 165/200
Train Loss: 0.00090205, Train R²: 0.999571, Val Loss: 1.16297030, Val R²: 0.452214
Epoch 166/200
Train Loss: 0.00065351, Train R²: 0.999689, Val Loss: 1.16610610, Val R²: 0.450737
Epoch 167/200
Train Loss: 0.00077998, Train R²: 0.999629, Val Loss: 1.16385531, Val R²: 0.451797
Epoch 168/200
Train Loss: 0.00092841, Train R²: 0.999558, Val Loss: 1.16538668, Val R²: 0.451076
Epoch 169/200
Train Loss: 0.00059746, Train R²: 0.999716, Val Loss: 1.16544235, Val R²: 0.451049
Epoch 170/200
Train Loss: 0.00092387, Train R²: 0.999560, Val Loss: 1.16307616, Val R²: 0.452164
Epoch 00170: reducing learning rate of group 0 to 6.1035e-08.
Epoch 171/200
Train Loss: 0.00095674, Train R²: 0.999545, Val Loss: 1.16436327, Val R²: 0.451558
Epoch 172/200
Train Loss: 0.00095630, Train R²: 0.999545, Val Loss: 1.16440582, Val R²: 0.451538
Epoch 173/200
Train Loss: 0.00074360, Train R²: 0.999646, Val Loss: 1.16385615, Val R²: 0.451797
Epoch 174/200
Train Loss: 0.00076851, Train R²: 0.999634, Val Loss: 1.16262722, Val R²: 0.452375
Epoch 175/200
Train Loss: 0.00068301, Train R²: 0.999675, Val Loss: 1.16323376, Val R²: 0.452090
Epoch 176/200
Train Loss: 0.00066233, Train R²: 0.999685, Val Loss: 1.16385663, Val R²: 0.451796
Epoch 177/200
Train Loss: 0.00061293, Train R²: 0.999708, Val Loss: 1.16323376, Val R²: 0.452090
Epoch 178/200
Train Loss: 0.00069814, Train R²: 0.999668, Val Loss: 1.16395438, Val R²: 0.451750
Epoch 179/200
Train Loss: 0.00086881, Train R²: 0.999587, Val Loss: 1.16385591, Val R²: 0.451797
Epoch 180/200
Train Loss: 0.00068683, Train R²: 0.999673, Val Loss: 1.16462648, Val R²: 0.451434
Epoch 181/200
Train Loss: 0.00065699, Train R²: 0.999687, Val Loss: 1.16385567, Val R²: 0.451797
Epoch 00181: reducing learning rate of group 0 to 3.0518e-08.
Epoch 182/200
Train Loss: 0.00094297, Train R²: 0.999551, Val Loss: 1.16385555, Val R²: 0.451797
Epoch 183/200
Train Loss: 0.00077842, Train R²: 0.999630, Val Loss: 1.16638756, Val R²: 0.450604
Epoch 184/200
Train Loss: 0.00087732, Train R²: 0.999583, Val Loss: 1.16523266, Val R²: 0.451148
Epoch 185/200
Train Loss: 0.00071680, Train R²: 0.999659, Val Loss: 1.16471469, Val R²: 0.451392
Epoch 186/200
Train Loss: 0.00063068, Train R²: 0.999700, Val Loss: 1.16538751, Val R²: 0.451075
Epoch 187/200
Train Loss: 0.00078678, Train R²: 0.999626, Val Loss: 1.16385591, Val R²: 0.451797
Epoch 188/200
Train Loss: 0.00055168, Train R²: 0.999737, Val Loss: 1.16278350, Val R²: 0.452302
Epoch 189/200
Train Loss: 0.00067522, Train R²: 0.999679, Val Loss: 1.16405928, Val R²: 0.451701
Epoch 190/200
Train Loss: 0.00084466, Train R²: 0.999598, Val Loss: 1.16323328, Val R²: 0.452090
Epoch 191/200
Train Loss: 0.00063404, Train R²: 0.999698, Val Loss: 1.16262722, Val R²: 0.452375
Epoch 192/200
Train Loss: 0.00066181, Train R²: 0.999685, Val Loss: 1.16595101, Val R²: 0.450810
Epoch 00192: reducing learning rate of group 0 to 1.5259e-08.
Epoch 193/200
Train Loss: 0.00064757, Train R²: 0.999692, Val Loss: 1.16344380, Val R²: 0.451991
Epoch 194/200
Train Loss: 0.00059814, Train R²: 0.999715, Val Loss: 1.16569948, Val R²: 0.450928
Epoch 195/200
Train Loss: 0.00072618, Train R²: 0.999654, Val Loss: 1.16465998, Val R²: 0.451418
Epoch 196/200
Train Loss: 0.00062731, Train R²: 0.999701, Val Loss: 1.16462708, Val R²: 0.451434
Epoch 197/200
Train Loss: 0.00061789, Train R²: 0.999706, Val Loss: 1.16200447, Val R²: 0.452669
Epoch 198/200
Train Loss: 0.00076738, Train R²: 0.999635, Val Loss: 1.16262734, Val R²: 0.452375
Epoch 199/200
Train Loss: 0.00075609, Train R²: 0.999640, Val Loss: 1.16468263, Val R²: 0.451407
Epoch 200/200
Train Loss: 0.00075216, Train R²: 0.999642, Val Loss: 1.16262734, Val R²: 0.452375
Training Complete. Best Val Loss: 1.136894941329956
训练时间: 224.06 秒
