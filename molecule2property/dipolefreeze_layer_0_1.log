Using device: cuda
Total samples: 1000, Training: 800, Validation: 200
Total trainable parameters: 1477889
Epoch 1/200
Train Loss: 5.36925241, Train R²: -1.815101, Val Loss: 2.90788865, Val R²: -0.003193
Saved best model with validation R2 -0.003193 to best_finetuned_model.pth
Epoch 2/200
Train Loss: 1.96152052, Train R²: -0.028426, Val Loss: 2.97315907, Val R²: -0.025711
Epoch 3/200
Train Loss: 1.88693744, Train R²: 0.010678, Val Loss: 2.83692551, Val R²: 0.021289
Saved best model with validation R2 0.021289 to best_finetuned_model.pth
Epoch 4/200
Train Loss: 1.72896498, Train R²: 0.093503, Val Loss: 2.84779596, Val R²: 0.017539
Epoch 5/200
Train Loss: 1.64858504, Train R²: 0.135646, Val Loss: 2.66533875, Val R²: 0.080485
Saved best model with validation R2 0.080485 to best_finetuned_model.pth
Epoch 6/200
Train Loss: 1.44874231, Train R²: 0.240424, Val Loss: 2.48716116, Val R²: 0.141954
Saved best model with validation R2 0.141954 to best_finetuned_model.pth
Epoch 7/200
Train Loss: 1.23723063, Train R²: 0.351320, Val Loss: 2.40393519, Val R²: 0.170666
Saved best model with validation R2 0.170666 to best_finetuned_model.pth
Epoch 8/200
Train Loss: 0.98413380, Train R²: 0.484018, Val Loss: 2.36016774, Val R²: 0.185765
Saved best model with validation R2 0.185765 to best_finetuned_model.pth
Epoch 9/200
Train Loss: 0.82293259, Train R²: 0.568536, Val Loss: 2.50972962, Val R²: 0.134168
Epoch 10/200
Train Loss: 0.71688648, Train R²: 0.624136, Val Loss: 2.25063801, Val R²: 0.223552
Saved best model with validation R2 0.223552 to best_finetuned_model.pth
Epoch 11/200
Train Loss: 0.64688205, Train R²: 0.660840, Val Loss: 2.21483898, Val R²: 0.235902
Saved best model with validation R2 0.235902 to best_finetuned_model.pth
Epoch 12/200
Train Loss: 0.49157444, Train R²: 0.742267, Val Loss: 2.44028306, Val R²: 0.158126
Epoch 13/200
Train Loss: 0.59562704, Train R²: 0.687713, Val Loss: 2.44633245, Val R²: 0.156040
Epoch 14/200
Train Loss: 0.46987618, Train R²: 0.753644, Val Loss: 2.36836791, Val R²: 0.182936
Epoch 15/200
Train Loss: 0.39467688, Train R²: 0.793071, Val Loss: 2.49871755, Val R²: 0.137967
Epoch 16/200
Train Loss: 0.36994518, Train R²: 0.806038, Val Loss: 2.47174454, Val R²: 0.147273
Epoch 17/200
Train Loss: 0.28369241, Train R²: 0.851260, Val Loss: 2.28726196, Val R²: 0.210917
Epoch 18/200
Train Loss: 0.22679491, Train R²: 0.881091, Val Loss: 2.18915033, Val R²: 0.244765
Saved best model with validation R2 0.244765 to best_finetuned_model.pth
Epoch 19/200
Train Loss: 0.19769443, Train R²: 0.896349, Val Loss: 2.19948936, Val R²: 0.241198
Epoch 20/200
Train Loss: 0.17638581, Train R²: 0.907521, Val Loss: 2.17632675, Val R²: 0.249189
Saved best model with validation R2 0.249189 to best_finetuned_model.pth
Epoch 21/200
Train Loss: 0.16706315, Train R²: 0.912409, Val Loss: 2.13211846, Val R²: 0.264440
Saved best model with validation R2 0.264440 to best_finetuned_model.pth
Epoch 22/200
Train Loss: 0.15824562, Train R²: 0.917032, Val Loss: 2.14510393, Val R²: 0.259960
Epoch 23/200
Train Loss: 0.17837287, Train R²: 0.906479, Val Loss: 2.17217898, Val R²: 0.250620
Epoch 24/200
Train Loss: 0.11857226, Train R²: 0.937833, Val Loss: 2.22715473, Val R²: 0.231654
Epoch 25/200
Train Loss: 0.09872264, Train R²: 0.948240, Val Loss: 2.19679260, Val R²: 0.242128
Epoch 26/200
Train Loss: 0.08443643, Train R²: 0.955730, Val Loss: 2.24941897, Val R²: 0.223973
Epoch 27/200
Train Loss: 0.08629828, Train R²: 0.954754, Val Loss: 2.22058392, Val R²: 0.233920
Epoch 28/200
Train Loss: 0.07675196, Train R²: 0.959759, Val Loss: 2.21843719, Val R²: 0.234661
Epoch 29/200
Train Loss: 0.07853540, Train R²: 0.958824, Val Loss: 2.26379657, Val R²: 0.219012
Epoch 30/200
Train Loss: 0.07797157, Train R²: 0.959119, Val Loss: 2.23945761, Val R²: 0.227409
Epoch 31/200
Train Loss: 0.06083201, Train R²: 0.968106, Val Loss: 2.23511934, Val R²: 0.228906
Epoch 32/200
Train Loss: 0.05860296, Train R²: 0.969274, Val Loss: 2.21348119, Val R²: 0.236371
Epoch 00032: reducing learning rate of group 0 to 5.0000e-04.
Epoch 33/200
Train Loss: 0.05393915, Train R²: 0.971720, Val Loss: 2.22924900, Val R²: 0.230931
Epoch 34/200
Train Loss: 0.04754149, Train R²: 0.975074, Val Loss: 2.21691918, Val R²: 0.235185
Epoch 35/200
Train Loss: 0.03266174, Train R²: 0.982875, Val Loss: 2.20944118, Val R²: 0.237765
Epoch 36/200
Train Loss: 0.03780801, Train R²: 0.980177, Val Loss: 2.20548630, Val R²: 0.239129
Epoch 37/200
Train Loss: 0.03771669, Train R²: 0.980225, Val Loss: 2.24999523, Val R²: 0.223774
Epoch 38/200
Train Loss: 0.03362732, Train R²: 0.982369, Val Loss: 2.20433474, Val R²: 0.239526
Epoch 39/200
Train Loss: 0.02852536, Train R²: 0.985044, Val Loss: 2.22217917, Val R²: 0.233370
Epoch 40/200
Train Loss: 0.02239545, Train R²: 0.988258, Val Loss: 2.20151210, Val R²: 0.240500
Epoch 41/200
Train Loss: 0.02082264, Train R²: 0.989083, Val Loss: 2.19355989, Val R²: 0.243243
Epoch 42/200
Train Loss: 0.01600169, Train R²: 0.991610, Val Loss: 2.18703294, Val R²: 0.245495
Epoch 43/200
Train Loss: 0.01454454, Train R²: 0.992374, Val Loss: 2.19220877, Val R²: 0.243710
Epoch 00043: reducing learning rate of group 0 to 2.5000e-04.
Epoch 44/200
Train Loss: 0.01178446, Train R²: 0.993821, Val Loss: 2.19550228, Val R²: 0.242573
Epoch 45/200
Train Loss: 0.01178597, Train R²: 0.993821, Val Loss: 2.19049454, Val R²: 0.244301
Epoch 46/200
Train Loss: 0.00732771, Train R²: 0.996158, Val Loss: 2.18791509, Val R²: 0.245191
Epoch 47/200
Train Loss: 0.00626355, Train R²: 0.996716, Val Loss: 2.18230605, Val R²: 0.247126
Epoch 48/200
Train Loss: 0.00489883, Train R²: 0.997432, Val Loss: 2.18045926, Val R²: 0.247763
Epoch 49/200
Train Loss: 0.00389898, Train R²: 0.997956, Val Loss: 2.18382692, Val R²: 0.246601
Epoch 50/200
Train Loss: 0.00302938, Train R²: 0.998412, Val Loss: 2.18543553, Val R²: 0.246046
Epoch 51/200
Train Loss: 0.00226585, Train R²: 0.998812, Val Loss: 2.18699217, Val R²: 0.245509
Epoch 52/200
Train Loss: 0.00214075, Train R²: 0.998878, Val Loss: 2.18631864, Val R²: 0.245742
Epoch 53/200
Train Loss: 0.00198464, Train R²: 0.998959, Val Loss: 2.18693924, Val R²: 0.245528
Epoch 54/200
Train Loss: 0.00168494, Train R²: 0.999117, Val Loss: 2.18973541, Val R²: 0.244563
Epoch 00054: reducing learning rate of group 0 to 1.2500e-04.
Epoch 55/200
Train Loss: 0.00174611, Train R²: 0.999085, Val Loss: 2.18928289, Val R²: 0.244719
Epoch 56/200
Train Loss: 0.00143057, Train R²: 0.999250, Val Loss: 2.19019222, Val R²: 0.244405
Epoch 57/200
Train Loss: 0.00136000, Train R²: 0.999287, Val Loss: 2.19213796, Val R²: 0.243734
Epoch 58/200
Train Loss: 0.00134015, Train R²: 0.999297, Val Loss: 2.18922472, Val R²: 0.244739
Epoch 59/200
Train Loss: 0.00109688, Train R²: 0.999425, Val Loss: 2.18956161, Val R²: 0.244623
Epoch 60/200
Train Loss: 0.00096296, Train R²: 0.999495, Val Loss: 2.19233966, Val R²: 0.243664
Epoch 61/200
Train Loss: 0.00084738, Train R²: 0.999556, Val Loss: 2.19488406, Val R²: 0.242787
Epoch 62/200
Train Loss: 0.00087857, Train R²: 0.999539, Val Loss: 2.19499540, Val R²: 0.242748
Epoch 63/200
Train Loss: 0.00068759, Train R²: 0.999639, Val Loss: 2.19421649, Val R²: 0.243017
Epoch 64/200
Train Loss: 0.00082061, Train R²: 0.999570, Val Loss: 2.19239020, Val R²: 0.243647
Epoch 65/200
Train Loss: 0.00074759, Train R²: 0.999608, Val Loss: 2.19005871, Val R²: 0.244451
Epoch 00065: reducing learning rate of group 0 to 6.2500e-05.
Epoch 66/200
Train Loss: 0.00085033, Train R²: 0.999554, Val Loss: 2.18986225, Val R²: 0.244519
Epoch 67/200
Train Loss: 0.00089512, Train R²: 0.999531, Val Loss: 2.18888569, Val R²: 0.244856
Epoch 68/200
Train Loss: 0.00079440, Train R²: 0.999583, Val Loss: 2.18937564, Val R²: 0.244687
Epoch 69/200
Train Loss: 0.00092888, Train R²: 0.999513, Val Loss: 2.18975925, Val R²: 0.244555
Epoch 70/200
Train Loss: 0.00067307, Train R²: 0.999647, Val Loss: 2.18925643, Val R²: 0.244728
Epoch 71/200
Train Loss: 0.00073437, Train R²: 0.999615, Val Loss: 2.18889832, Val R²: 0.244852
Epoch 72/200
Train Loss: 0.00051549, Train R²: 0.999730, Val Loss: 2.18833923, Val R²: 0.245045
Epoch 73/200
Train Loss: 0.00061529, Train R²: 0.999677, Val Loss: 2.18805814, Val R²: 0.245142
Epoch 74/200
Train Loss: 0.00075883, Train R²: 0.999602, Val Loss: 2.18802977, Val R²: 0.245151
Epoch 75/200
Train Loss: 0.00048731, Train R²: 0.999745, Val Loss: 2.18827605, Val R²: 0.245066
Epoch 76/200
Train Loss: 0.00050331, Train R²: 0.999736, Val Loss: 2.18853903, Val R²: 0.244976
Epoch 00076: reducing learning rate of group 0 to 3.1250e-05.
Epoch 77/200
Train Loss: 0.00047241, Train R²: 0.999752, Val Loss: 2.18867660, Val R²: 0.244928
Epoch 78/200
Train Loss: 0.00054452, Train R²: 0.999715, Val Loss: 2.18946719, Val R²: 0.244655
Epoch 79/200
Train Loss: 0.00043757, Train R²: 0.999771, Val Loss: 2.18963575, Val R²: 0.244597
Epoch 80/200
Train Loss: 0.00049445, Train R²: 0.999741, Val Loss: 2.18894958, Val R²: 0.244834
Epoch 81/200
Train Loss: 0.00058726, Train R²: 0.999692, Val Loss: 2.18897557, Val R²: 0.244825
Epoch 82/200
Train Loss: 0.00048999, Train R²: 0.999743, Val Loss: 2.18909693, Val R²: 0.244783
Epoch 83/200
Train Loss: 0.00054195, Train R²: 0.999716, Val Loss: 2.18917894, Val R²: 0.244755
Epoch 84/200
Train Loss: 0.00052900, Train R²: 0.999723, Val Loss: 2.18917203, Val R²: 0.244757
Epoch 85/200
Train Loss: 0.00051261, Train R²: 0.999731, Val Loss: 2.18904495, Val R²: 0.244801
Epoch 86/200
Train Loss: 0.00051654, Train R²: 0.999729, Val Loss: 2.18907213, Val R²: 0.244792
Epoch 87/200
Train Loss: 0.00057542, Train R²: 0.999698, Val Loss: 2.18910599, Val R²: 0.244780
Epoch 00087: reducing learning rate of group 0 to 1.5625e-05.
Epoch 88/200
Train Loss: 0.00044323, Train R²: 0.999768, Val Loss: 2.18907762, Val R²: 0.244790
Epoch 89/200
Train Loss: 0.00037834, Train R²: 0.999802, Val Loss: 2.18905020, Val R²: 0.244799
Epoch 90/200
Train Loss: 0.00038136, Train R²: 0.999800, Val Loss: 2.18904638, Val R²: 0.244801
Epoch 91/200
Train Loss: 0.00050651, Train R²: 0.999734, Val Loss: 2.18963647, Val R²: 0.244597
Epoch 92/200
Train Loss: 0.00045642, Train R²: 0.999761, Val Loss: 2.18891263, Val R²: 0.244847
Epoch 93/200
Train Loss: 0.00056190, Train R²: 0.999705, Val Loss: 2.18886828, Val R²: 0.244862
Epoch 94/200
Train Loss: 0.00052068, Train R²: 0.999727, Val Loss: 2.18885827, Val R²: 0.244865
Epoch 95/200
Train Loss: 0.00052349, Train R²: 0.999726, Val Loss: 2.18887568, Val R²: 0.244859
Epoch 96/200
Train Loss: 0.00047173, Train R²: 0.999753, Val Loss: 2.18886876, Val R²: 0.244862
Epoch 97/200
Train Loss: 0.00043284, Train R²: 0.999773, Val Loss: 2.18881941, Val R²: 0.244879
Epoch 98/200
Train Loss: 0.00061926, Train R²: 0.999675, Val Loss: 2.18888688, Val R²: 0.244856
Epoch 00098: reducing learning rate of group 0 to 7.8125e-06.
Epoch 99/200
Train Loss: 0.00057209, Train R²: 0.999700, Val Loss: 2.18892694, Val R²: 0.244842
Epoch 100/200
Train Loss: 0.00048219, Train R²: 0.999747, Val Loss: 2.18898702, Val R²: 0.244821
Epoch 101/200
Train Loss: 0.00037114, Train R²: 0.999805, Val Loss: 2.18901372, Val R²: 0.244812
Epoch 102/200
Train Loss: 0.00039614, Train R²: 0.999792, Val Loss: 2.18898749, Val R²: 0.244821
Epoch 103/200
Train Loss: 0.00049903, Train R²: 0.999738, Val Loss: 2.18904996, Val R²: 0.244799
Epoch 104/200
Train Loss: 0.00040968, Train R²: 0.999785, Val Loss: 2.18904853, Val R²: 0.244800
Epoch 105/200
Train Loss: 0.00043154, Train R²: 0.999774, Val Loss: 2.18969655, Val R²: 0.244576
Epoch 106/200
Train Loss: 0.00042629, Train R²: 0.999776, Val Loss: 2.18903017, Val R²: 0.244806
Epoch 107/200
Train Loss: 0.00065814, Train R²: 0.999655, Val Loss: 2.18901920, Val R²: 0.244810
Epoch 108/200
Train Loss: 0.00041459, Train R²: 0.999783, Val Loss: 2.18896151, Val R²: 0.244830
Epoch 109/200
Train Loss: 0.00057023, Train R²: 0.999701, Val Loss: 2.18894649, Val R²: 0.244835
Epoch 00109: reducing learning rate of group 0 to 3.9063e-06.
Epoch 110/200
Train Loss: 0.00053914, Train R²: 0.999717, Val Loss: 2.18900728, Val R²: 0.244814
Epoch 111/200
Train Loss: 0.00064884, Train R²: 0.999660, Val Loss: 2.18902373, Val R²: 0.244808
Epoch 112/200
Train Loss: 0.00055820, Train R²: 0.999707, Val Loss: 2.18965602, Val R²: 0.244590
Epoch 113/200
Train Loss: 0.00038303, Train R²: 0.999799, Val Loss: 2.18903732, Val R²: 0.244804
Epoch 114/200
Train Loss: 0.00045125, Train R²: 0.999763, Val Loss: 2.18969750, Val R²: 0.244576
Epoch 115/200
Train Loss: 0.00057122, Train R²: 0.999701, Val Loss: 2.18904853, Val R²: 0.244800
Epoch 116/200
Train Loss: 0.00043565, Train R²: 0.999772, Val Loss: 2.18906617, Val R²: 0.244794
Epoch 117/200
Train Loss: 0.00036259, Train R²: 0.999810, Val Loss: 2.18903732, Val R²: 0.244804
Epoch 118/200
Train Loss: 0.00049035, Train R²: 0.999743, Val Loss: 2.18909359, Val R²: 0.244784
Epoch 119/200
Train Loss: 0.00061082, Train R²: 0.999680, Val Loss: 2.18976712, Val R²: 0.244552
Epoch 120/200
Train Loss: 0.00039964, Train R²: 0.999790, Val Loss: 2.18912172, Val R²: 0.244775
Epoch 00120: reducing learning rate of group 0 to 1.9531e-06.
Epoch 121/200
Train Loss: 0.00049684, Train R²: 0.999740, Val Loss: 2.18908310, Val R²: 0.244788
Epoch 122/200
Train Loss: 0.00036228, Train R²: 0.999810, Val Loss: 2.18973780, Val R²: 0.244562
Epoch 123/200
Train Loss: 0.00039083, Train R²: 0.999795, Val Loss: 2.18976855, Val R²: 0.244551
Epoch 124/200
Train Loss: 0.00049837, Train R²: 0.999739, Val Loss: 2.18907213, Val R²: 0.244792
Epoch 125/200
Train Loss: 0.00046838, Train R²: 0.999754, Val Loss: 2.18907857, Val R²: 0.244790
Epoch 126/200
Train Loss: 0.00040729, Train R²: 0.999786, Val Loss: 2.18913317, Val R²: 0.244771
Epoch 127/200
Train Loss: 0.00043876, Train R²: 0.999770, Val Loss: 2.18909883, Val R²: 0.244782
Epoch 128/200
Train Loss: 0.00050414, Train R²: 0.999736, Val Loss: 2.18911028, Val R²: 0.244779
Epoch 129/200
Train Loss: 0.00054502, Train R²: 0.999714, Val Loss: 2.18917561, Val R²: 0.244756
Epoch 130/200
Train Loss: 0.00040629, Train R²: 0.999787, Val Loss: 2.18981266, Val R²: 0.244536
Epoch 131/200
Train Loss: 0.00041184, Train R²: 0.999784, Val Loss: 2.18917060, Val R²: 0.244758
Epoch 00131: reducing learning rate of group 0 to 9.7656e-07.
Epoch 132/200
Train Loss: 0.00037339, Train R²: 0.999804, Val Loss: 2.18983626, Val R²: 0.244528
Epoch 133/200
Train Loss: 0.00053548, Train R²: 0.999719, Val Loss: 2.18918419, Val R²: 0.244753
Epoch 134/200
Train Loss: 0.00060601, Train R²: 0.999682, Val Loss: 2.18923664, Val R²: 0.244735
Epoch 135/200
Train Loss: 0.00036127, Train R²: 0.999811, Val Loss: 2.18924332, Val R²: 0.244733
Epoch 136/200
Train Loss: 0.00037470, Train R²: 0.999804, Val Loss: 2.18925309, Val R²: 0.244729
Epoch 137/200
Train Loss: 0.00055993, Train R²: 0.999706, Val Loss: 2.18987823, Val R²: 0.244514
Epoch 138/200
Train Loss: 0.00040318, Train R²: 0.999789, Val Loss: 2.18926406, Val R²: 0.244725
Epoch 139/200
Train Loss: 0.00039764, Train R²: 0.999792, Val Loss: 2.18926692, Val R²: 0.244725
Epoch 140/200
Train Loss: 0.00045761, Train R²: 0.999760, Val Loss: 2.18926787, Val R²: 0.244724
Epoch 141/200
Train Loss: 0.00039151, Train R²: 0.999795, Val Loss: 2.18988681, Val R²: 0.244511
Epoch 142/200
Train Loss: 0.00052887, Train R²: 0.999723, Val Loss: 2.18922567, Val R²: 0.244739
Epoch 00142: reducing learning rate of group 0 to 4.8828e-07.
Epoch 143/200
Train Loss: 0.00040174, Train R²: 0.999789, Val Loss: 2.18922591, Val R²: 0.244739
Epoch 144/200
Train Loss: 0.00058802, Train R²: 0.999692, Val Loss: 2.18926764, Val R²: 0.244724
Epoch 145/200
Train Loss: 0.00037858, Train R²: 0.999802, Val Loss: 2.18992829, Val R²: 0.244496
Epoch 146/200
Train Loss: 0.00034122, Train R²: 0.999821, Val Loss: 2.18922257, Val R²: 0.244740
Epoch 147/200
Train Loss: 0.00064116, Train R²: 0.999664, Val Loss: 2.18989086, Val R²: 0.244509
Epoch 148/200
Train Loss: 0.00035414, Train R²: 0.999814, Val Loss: 2.18927717, Val R²: 0.244721
Epoch 149/200
Train Loss: 0.00051201, Train R²: 0.999732, Val Loss: 2.18926907, Val R²: 0.244724
Epoch 150/200
Train Loss: 0.00047284, Train R²: 0.999752, Val Loss: 2.18987894, Val R²: 0.244513
Epoch 151/200
Train Loss: 0.00034953, Train R²: 0.999817, Val Loss: 2.18873715, Val R²: 0.244907
Epoch 152/200
Train Loss: 0.00052213, Train R²: 0.999726, Val Loss: 2.18990612, Val R²: 0.244504
Epoch 153/200
Train Loss: 0.00039958, Train R²: 0.999790, Val Loss: 2.18929005, Val R²: 0.244717
Epoch 00153: reducing learning rate of group 0 to 2.4414e-07.
Epoch 154/200
Train Loss: 0.00036080, Train R²: 0.999811, Val Loss: 2.18929005, Val R²: 0.244717
Epoch 155/200
Train Loss: 0.00038448, Train R²: 0.999798, Val Loss: 2.18994260, Val R²: 0.244491
Epoch 156/200
Train Loss: 0.00057924, Train R²: 0.999696, Val Loss: 2.18994236, Val R²: 0.244492
Epoch 157/200
Train Loss: 0.00041986, Train R²: 0.999780, Val Loss: 2.18994951, Val R²: 0.244489
Epoch 158/200
Train Loss: 0.00042913, Train R²: 0.999775, Val Loss: 2.18990874, Val R²: 0.244503
Epoch 159/200
Train Loss: 0.00047113, Train R²: 0.999753, Val Loss: 2.18929291, Val R²: 0.244716
Epoch 160/200
Train Loss: 0.00050462, Train R²: 0.999735, Val Loss: 2.18990445, Val R²: 0.244505
Epoch 161/200
Train Loss: 0.00046820, Train R²: 0.999755, Val Loss: 2.18995428, Val R²: 0.244487
Epoch 162/200
Train Loss: 0.00058802, Train R²: 0.999692, Val Loss: 2.18928242, Val R²: 0.244719
Epoch 163/200
Train Loss: 0.00037278, Train R²: 0.999805, Val Loss: 2.18995404, Val R²: 0.244488
Epoch 164/200
Train Loss: 0.00052980, Train R²: 0.999722, Val Loss: 2.18929505, Val R²: 0.244715
Epoch 00164: reducing learning rate of group 0 to 1.2207e-07.
Epoch 165/200
Train Loss: 0.00040675, Train R²: 0.999787, Val Loss: 2.18929505, Val R²: 0.244715
Epoch 166/200
Train Loss: 0.00041256, Train R²: 0.999784, Val Loss: 2.18995428, Val R²: 0.244487
Epoch 167/200
Train Loss: 0.00042235, Train R²: 0.999779, Val Loss: 2.18995476, Val R²: 0.244487
Epoch 168/200
Train Loss: 0.00036014, Train R²: 0.999811, Val Loss: 2.18929625, Val R²: 0.244714
Epoch 169/200
Train Loss: 0.00041089, Train R²: 0.999785, Val Loss: 2.18925500, Val R²: 0.244729
Epoch 170/200
Train Loss: 0.00037985, Train R²: 0.999801, Val Loss: 2.18929672, Val R²: 0.244714
Epoch 171/200
Train Loss: 0.00036731, Train R²: 0.999807, Val Loss: 2.18929720, Val R²: 0.244714
Epoch 172/200
Train Loss: 0.00053634, Train R²: 0.999719, Val Loss: 2.18925524, Val R²: 0.244729
Epoch 173/200
Train Loss: 0.00039144, Train R²: 0.999795, Val Loss: 2.18995667, Val R²: 0.244487
Epoch 174/200
Train Loss: 0.00040474, Train R²: 0.999788, Val Loss: 2.18929768, Val R²: 0.244714
Epoch 175/200
Train Loss: 0.00039079, Train R²: 0.999795, Val Loss: 2.18925619, Val R²: 0.244728
Epoch 00175: reducing learning rate of group 0 to 6.1035e-08.
Epoch 176/200
Train Loss: 0.00044333, Train R²: 0.999768, Val Loss: 2.18929839, Val R²: 0.244714
Epoch 177/200
Train Loss: 0.00035332, Train R²: 0.999815, Val Loss: 2.18929863, Val R²: 0.244714
Epoch 178/200
Train Loss: 0.00046095, Train R²: 0.999758, Val Loss: 2.18929887, Val R²: 0.244714
Epoch 179/200
Train Loss: 0.00044481, Train R²: 0.999767, Val Loss: 2.18925714, Val R²: 0.244728
Epoch 180/200
Train Loss: 0.00049981, Train R²: 0.999738, Val Loss: 2.18932891, Val R²: 0.244703
Epoch 181/200
Train Loss: 0.00054020, Train R²: 0.999717, Val Loss: 2.18929935, Val R²: 0.244713
Epoch 182/200
Train Loss: 0.00067131, Train R²: 0.999648, Val Loss: 2.18929887, Val R²: 0.244713
Epoch 183/200
Train Loss: 0.00060383, Train R²: 0.999683, Val Loss: 2.18929911, Val R²: 0.244713
Epoch 184/200
Train Loss: 0.00040571, Train R²: 0.999787, Val Loss: 2.18995833, Val R²: 0.244486
Epoch 185/200
Train Loss: 0.00050382, Train R²: 0.999736, Val Loss: 2.18781471, Val R²: 0.245225
Epoch 186/200
Train Loss: 0.00059754, Train R²: 0.999687, Val Loss: 2.18925738, Val R²: 0.244728
Epoch 00186: reducing learning rate of group 0 to 3.0518e-08.
Epoch 187/200
Train Loss: 0.00050702, Train R²: 0.999734, Val Loss: 2.18925738, Val R²: 0.244728
Epoch 188/200
Train Loss: 0.00040555, Train R²: 0.999787, Val Loss: 2.18991637, Val R²: 0.244500
Epoch 189/200
Train Loss: 0.00035495, Train R²: 0.999814, Val Loss: 2.18990970, Val R²: 0.244503
Epoch 190/200
Train Loss: 0.00039291, Train R²: 0.999794, Val Loss: 2.18990993, Val R²: 0.244503
Epoch 191/200
Train Loss: 0.00043732, Train R²: 0.999771, Val Loss: 2.18991685, Val R²: 0.244500
Epoch 192/200
Train Loss: 0.00047212, Train R²: 0.999752, Val Loss: 2.18925738, Val R²: 0.244728
Epoch 193/200
Train Loss: 0.00069362, Train R²: 0.999636, Val Loss: 2.18925738, Val R²: 0.244728
Epoch 194/200
Train Loss: 0.00055382, Train R²: 0.999710, Val Loss: 2.18929982, Val R²: 0.244713
Epoch 195/200
Train Loss: 0.00040707, Train R²: 0.999787, Val Loss: 2.18925786, Val R²: 0.244728
Epoch 196/200
Train Loss: 0.00034711, Train R²: 0.999818, Val Loss: 2.18925786, Val R²: 0.244728
Epoch 197/200
Train Loss: 0.00032725, Train R²: 0.999828, Val Loss: 2.18929982, Val R²: 0.244713
Epoch 00197: reducing learning rate of group 0 to 1.5259e-08.
Epoch 198/200
Train Loss: 0.00043586, Train R²: 0.999771, Val Loss: 2.18932986, Val R²: 0.244703
Epoch 199/200
Train Loss: 0.00034270, Train R²: 0.999820, Val Loss: 2.18929982, Val R²: 0.244713
Epoch 200/200
Train Loss: 0.00043825, Train R²: 0.999770, Val Loss: 2.18925786, Val R²: 0.244728
Training Complete. Best Val Loss: 2.1321184635162354
训练时间: 177.51 秒
