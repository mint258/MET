Using device: cuda
Total samples: 1000, Training: 800, Validation: 200
Total trainable parameters: 1477889
Epoch 1/200
Train Loss: 4.87971698, Train R²: -1.255053, Val Loss: 2.68094420, Val R²: -0.438596
Saved best model with validation R2 -0.438596 to best_finetuned_model.pth
Epoch 2/200
Train Loss: 2.54621265, Train R²: -0.176676, Val Loss: 2.29894471, Val R²: -0.233615
Saved best model with validation R2 -0.233615 to best_finetuned_model.pth
Epoch 3/200
Train Loss: 2.25180558, Train R²: -0.040622, Val Loss: 1.83585906, Val R²: 0.014877
Saved best model with validation R2 0.014877 to best_finetuned_model.pth
Epoch 4/200
Train Loss: 2.04812270, Train R²: 0.053506, Val Loss: 1.83693874, Val R²: 0.014298
Epoch 5/200
Train Loss: 2.03794975, Train R²: 0.058207, Val Loss: 1.72559822, Val R²: 0.074043
Saved best model with validation R2 0.074043 to best_finetuned_model.pth
Epoch 6/200
Train Loss: 1.93179452, Train R²: 0.107264, Val Loss: 1.66792262, Val R²: 0.104992
Saved best model with validation R2 0.104992 to best_finetuned_model.pth
Epoch 7/200
Train Loss: 1.86387950, Train R²: 0.138650, Val Loss: 1.67512262, Val R²: 0.101128
Epoch 8/200
Train Loss: 1.66968771, Train R²: 0.228391, Val Loss: 1.64043069, Val R²: 0.119744
Saved best model with validation R2 0.119744 to best_finetuned_model.pth
Epoch 9/200
Train Loss: 1.42909585, Train R²: 0.339575, Val Loss: 1.57419419, Val R²: 0.155286
Saved best model with validation R2 0.155286 to best_finetuned_model.pth
Epoch 10/200
Train Loss: 1.21249768, Train R²: 0.439671, Val Loss: 1.66116905, Val R²: 0.108616
Epoch 11/200
Train Loss: 0.91554814, Train R²: 0.576900, Val Loss: 1.86839712, Val R²: -0.002583
Epoch 12/200
Train Loss: 0.84516940, Train R²: 0.609424, Val Loss: 1.63543606, Val R²: 0.122424
Epoch 13/200
Train Loss: 0.64113023, Train R²: 0.703716, Val Loss: 1.66541779, Val R²: 0.106336
Epoch 14/200
Train Loss: 0.56807766, Train R²: 0.737476, Val Loss: 1.55739677, Val R²: 0.164300
Saved best model with validation R2 0.164300 to best_finetuned_model.pth
Epoch 15/200
Train Loss: 0.62287663, Train R²: 0.712151, Val Loss: 1.39329684, Val R²: 0.252356
Saved best model with validation R2 0.252356 to best_finetuned_model.pth
Epoch 16/200
Train Loss: 0.55834296, Train R²: 0.741974, Val Loss: 1.42354918, Val R²: 0.236123
Epoch 17/200
Train Loss: 0.42501592, Train R²: 0.803588, Val Loss: 1.58712447, Val R²: 0.148348
Epoch 18/200
Train Loss: 0.40584357, Train R²: 0.812448, Val Loss: 1.65015459, Val R²: 0.114526
Epoch 19/200
Train Loss: 0.35452642, Train R²: 0.836164, Val Loss: 1.76072454, Val R²: 0.055194
Epoch 20/200
Train Loss: 0.30459753, Train R²: 0.859237, Val Loss: 1.53689682, Val R²: 0.175300
Epoch 21/200
Train Loss: 0.28465759, Train R²: 0.868452, Val Loss: 1.40602982, Val R²: 0.245524
Epoch 22/200
Train Loss: 0.23128969, Train R²: 0.893115, Val Loss: 1.76353693, Val R²: 0.053685
Epoch 23/200
Train Loss: 0.33636762, Train R²: 0.844555, Val Loss: 1.39397705, Val R²: 0.251991
Epoch 24/200
Train Loss: 0.24263024, Train R²: 0.887874, Val Loss: 1.29550958, Val R²: 0.304829
Saved best model with validation R2 0.304829 to best_finetuned_model.pth
Epoch 25/200
Train Loss: 0.19435322, Train R²: 0.910184, Val Loss: 1.35866714, Val R²: 0.270938
Epoch 26/200
Train Loss: 0.20493893, Train R²: 0.905292, Val Loss: 1.37263083, Val R²: 0.263445
Epoch 27/200
Train Loss: 0.17377744, Train R²: 0.919693, Val Loss: 1.41167736, Val R²: 0.242493
Epoch 28/200
Train Loss: 0.21214742, Train R²: 0.901961, Val Loss: 1.47310913, Val R²: 0.209529
Epoch 29/200
Train Loss: 0.26619102, Train R²: 0.876986, Val Loss: 1.35026336, Val R²: 0.275448
Epoch 30/200
Train Loss: 0.21291440, Train R²: 0.901606, Val Loss: 1.37781048, Val R²: 0.260666
Epoch 31/200
Train Loss: 0.17880796, Train R²: 0.917368, Val Loss: 1.42930877, Val R²: 0.233032
Epoch 32/200
Train Loss: 0.15152013, Train R²: 0.929978, Val Loss: 1.42122543, Val R²: 0.237370
Epoch 33/200
Train Loss: 0.15020466, Train R²: 0.930586, Val Loss: 1.46501005, Val R²: 0.213875
Epoch 34/200
Train Loss: 0.20288987, Train R²: 0.906239, Val Loss: 1.42596900, Val R²: 0.234824
Epoch 35/200
Train Loss: 0.18610186, Train R²: 0.913997, Val Loss: 1.40071774, Val R²: 0.248374
Epoch 00035: reducing learning rate of group 0 to 5.0000e-04.
Epoch 36/200
Train Loss: 0.16450061, Train R²: 0.923980, Val Loss: 1.35363781, Val R²: 0.273637
Epoch 37/200
Train Loss: 0.11912798, Train R²: 0.944948, Val Loss: 1.41327333, Val R²: 0.241637
Epoch 38/200
Train Loss: 0.10051238, Train R²: 0.953550, Val Loss: 1.39216888, Val R²: 0.252961
Epoch 39/200
Train Loss: 0.08076210, Train R²: 0.962678, Val Loss: 1.32689011, Val R²: 0.287990
Epoch 40/200
Train Loss: 0.08109414, Train R²: 0.962524, Val Loss: 1.31794906, Val R²: 0.292788
Epoch 41/200
Train Loss: 0.07855677, Train R²: 0.963697, Val Loss: 1.30594003, Val R²: 0.299232
Epoch 42/200
Train Loss: 0.05509679, Train R²: 0.974538, Val Loss: 1.33142543, Val R²: 0.285556
Epoch 43/200
Train Loss: 0.04064522, Train R²: 0.981217, Val Loss: 1.30333030, Val R²: 0.300632
Epoch 44/200
Train Loss: 0.03132004, Train R²: 0.985526, Val Loss: 1.32513940, Val R²: 0.288929
Epoch 45/200
Train Loss: 0.02251882, Train R²: 0.989593, Val Loss: 1.32494080, Val R²: 0.289036
Epoch 46/200
Train Loss: 0.01898573, Train R²: 0.991226, Val Loss: 1.31455994, Val R²: 0.294606
Epoch 00046: reducing learning rate of group 0 to 2.5000e-04.
Epoch 47/200
Train Loss: 0.01472739, Train R²: 0.993194, Val Loss: 1.30953336, Val R²: 0.297304
Epoch 48/200
Train Loss: 0.01215127, Train R²: 0.994385, Val Loss: 1.31135643, Val R²: 0.296325
Epoch 49/200
Train Loss: 0.00971199, Train R²: 0.995512, Val Loss: 1.30910826, Val R²: 0.297532
Epoch 50/200
Train Loss: 0.00883217, Train R²: 0.995918, Val Loss: 1.32652712, Val R²: 0.288185
Epoch 51/200
Train Loss: 0.00799295, Train R²: 0.996306, Val Loss: 1.32298434, Val R²: 0.290086
Epoch 52/200
Train Loss: 0.00583343, Train R²: 0.997304, Val Loss: 1.31827021, Val R²: 0.292615
Epoch 53/200
Train Loss: 0.00488683, Train R²: 0.997742, Val Loss: 1.32035732, Val R²: 0.291495
Epoch 54/200
Train Loss: 0.00410720, Train R²: 0.998102, Val Loss: 1.31924617, Val R²: 0.292092
Epoch 55/200
Train Loss: 0.00347529, Train R²: 0.998394, Val Loss: 1.32489443, Val R²: 0.289061
Epoch 56/200
Train Loss: 0.00289485, Train R²: 0.998662, Val Loss: 1.32804000, Val R²: 0.287373
Epoch 57/200
Train Loss: 0.00250539, Train R²: 0.998842, Val Loss: 1.33340955, Val R²: 0.284492
Epoch 00057: reducing learning rate of group 0 to 1.2500e-04.
Epoch 58/200
Train Loss: 0.00216124, Train R²: 0.999001, Val Loss: 1.33012474, Val R²: 0.286254
Epoch 59/200
Train Loss: 0.00204580, Train R²: 0.999055, Val Loss: 1.32677698, Val R²: 0.288051
Epoch 60/200
Train Loss: 0.00171940, Train R²: 0.999205, Val Loss: 1.32703245, Val R²: 0.287914
Epoch 61/200
Train Loss: 0.00135346, Train R²: 0.999375, Val Loss: 1.32690728, Val R²: 0.287981
Epoch 62/200
Train Loss: 0.00136107, Train R²: 0.999371, Val Loss: 1.32182157, Val R²: 0.290710
Epoch 63/200
Train Loss: 0.00115863, Train R²: 0.999465, Val Loss: 1.32127535, Val R²: 0.291003
Epoch 64/200
Train Loss: 0.00121752, Train R²: 0.999437, Val Loss: 1.32444739, Val R²: 0.289301
Epoch 65/200
Train Loss: 0.00134952, Train R²: 0.999376, Val Loss: 1.32391810, Val R²: 0.289585
Epoch 66/200
Train Loss: 0.00120243, Train R²: 0.999444, Val Loss: 1.31975520, Val R²: 0.291819
Epoch 67/200
Train Loss: 0.00095902, Train R²: 0.999557, Val Loss: 1.32210672, Val R²: 0.290557
Epoch 68/200
Train Loss: 0.00122758, Train R²: 0.999433, Val Loss: 1.32200146, Val R²: 0.290613
Epoch 00068: reducing learning rate of group 0 to 6.2500e-05.
Epoch 69/200
Train Loss: 0.00072690, Train R²: 0.999664, Val Loss: 1.32342041, Val R²: 0.289852
Epoch 70/200
Train Loss: 0.00099318, Train R²: 0.999541, Val Loss: 1.32035351, Val R²: 0.291498
Epoch 71/200
Train Loss: 0.00068682, Train R²: 0.999683, Val Loss: 1.32313037, Val R²: 0.290007
Epoch 72/200
Train Loss: 0.00092007, Train R²: 0.999575, Val Loss: 1.32190716, Val R²: 0.290664
Epoch 73/200
Train Loss: 0.00072749, Train R²: 0.999664, Val Loss: 1.32165098, Val R²: 0.290801
Epoch 74/200
Train Loss: 0.00122573, Train R²: 0.999434, Val Loss: 1.32347465, Val R²: 0.289823
Epoch 75/200
Train Loss: 0.00060049, Train R²: 0.999722, Val Loss: 1.32388282, Val R²: 0.289604
Epoch 76/200
Train Loss: 0.00073840, Train R²: 0.999659, Val Loss: 1.32045984, Val R²: 0.291440
Epoch 77/200
Train Loss: 0.00059763, Train R²: 0.999724, Val Loss: 1.32117915, Val R²: 0.291055
Epoch 78/200
Train Loss: 0.00076025, Train R²: 0.999649, Val Loss: 1.32280612, Val R²: 0.290182
Epoch 79/200
Train Loss: 0.00079598, Train R²: 0.999632, Val Loss: 1.32288206, Val R²: 0.290141
Epoch 00079: reducing learning rate of group 0 to 3.1250e-05.
Epoch 80/200
Train Loss: 0.00055832, Train R²: 0.999742, Val Loss: 1.32986248, Val R²: 0.286395
Epoch 81/200
Train Loss: 0.00075761, Train R²: 0.999650, Val Loss: 1.32266259, Val R²: 0.290258
Epoch 82/200
Train Loss: 0.00054461, Train R²: 0.999748, Val Loss: 1.32133377, Val R²: 0.290972
Epoch 83/200
Train Loss: 0.00066402, Train R²: 0.999693, Val Loss: 1.32261658, Val R²: 0.290283
Epoch 84/200
Train Loss: 0.00065477, Train R²: 0.999697, Val Loss: 1.32140231, Val R²: 0.290935
Epoch 85/200
Train Loss: 0.00050240, Train R²: 0.999768, Val Loss: 1.32383919, Val R²: 0.289627
Epoch 86/200
Train Loss: 0.00048540, Train R²: 0.999776, Val Loss: 1.32292092, Val R²: 0.290120
Epoch 87/200
Train Loss: 0.00051392, Train R²: 0.999763, Val Loss: 1.32022977, Val R²: 0.291564
Epoch 88/200
Train Loss: 0.00081978, Train R²: 0.999621, Val Loss: 1.32216489, Val R²: 0.290526
Epoch 89/200
Train Loss: 0.00048973, Train R²: 0.999774, Val Loss: 1.32132959, Val R²: 0.290974
Epoch 90/200
Train Loss: 0.00101203, Train R²: 0.999532, Val Loss: 1.32962060, Val R²: 0.286525
Epoch 00090: reducing learning rate of group 0 to 1.5625e-05.
Epoch 91/200
Train Loss: 0.00084998, Train R²: 0.999607, Val Loss: 1.32415581, Val R²: 0.289457
Epoch 92/200
Train Loss: 0.00053773, Train R²: 0.999752, Val Loss: 1.32169223, Val R²: 0.290779
Epoch 93/200
Train Loss: 0.00074600, Train R²: 0.999655, Val Loss: 1.32172990, Val R²: 0.290759
Epoch 94/200
Train Loss: 0.00056711, Train R²: 0.999738, Val Loss: 1.32428849, Val R²: 0.289386
Epoch 95/200
Train Loss: 0.00053379, Train R²: 0.999753, Val Loss: 1.32173645, Val R²: 0.290755
Epoch 96/200
Train Loss: 0.00055333, Train R²: 0.999744, Val Loss: 1.32151794, Val R²: 0.290873
Epoch 97/200
Train Loss: 0.00046441, Train R²: 0.999785, Val Loss: 1.32254219, Val R²: 0.290323
Epoch 98/200
Train Loss: 0.00048513, Train R²: 0.999776, Val Loss: 1.31720686, Val R²: 0.293186
Epoch 99/200
Train Loss: 0.00057982, Train R²: 0.999732, Val Loss: 1.32422149, Val R²: 0.289422
Epoch 100/200
Train Loss: 0.00075621, Train R²: 0.999651, Val Loss: 1.32281387, Val R²: 0.290177
Epoch 101/200
Train Loss: 0.00048752, Train R²: 0.999775, Val Loss: 1.32267427, Val R²: 0.290252
Epoch 00101: reducing learning rate of group 0 to 7.8125e-06.
Epoch 102/200
Train Loss: 0.00056322, Train R²: 0.999740, Val Loss: 1.32280815, Val R²: 0.290180
Epoch 103/200
Train Loss: 0.00051924, Train R²: 0.999760, Val Loss: 1.32427406, Val R²: 0.289394
Epoch 104/200
Train Loss: 0.00044564, Train R²: 0.999794, Val Loss: 1.32979882, Val R²: 0.286429
Epoch 105/200
Train Loss: 0.00070941, Train R²: 0.999672, Val Loss: 1.32172966, Val R²: 0.290759
Epoch 106/200
Train Loss: 0.00051692, Train R²: 0.999761, Val Loss: 1.32178986, Val R²: 0.290727
Epoch 107/200
Train Loss: 0.00051136, Train R²: 0.999764, Val Loss: 1.32448483, Val R²: 0.289281
Epoch 108/200
Train Loss: 0.00040884, Train R²: 0.999811, Val Loss: 1.32186317, Val R²: 0.290687
Epoch 109/200
Train Loss: 0.00049415, Train R²: 0.999772, Val Loss: 1.32187915, Val R²: 0.290679
Epoch 110/200
Train Loss: 0.00039791, Train R²: 0.999816, Val Loss: 1.32427609, Val R²: 0.289393
Epoch 111/200
Train Loss: 0.00053690, Train R²: 0.999752, Val Loss: 1.32419097, Val R²: 0.289438
Epoch 112/200
Train Loss: 0.00061729, Train R²: 0.999715, Val Loss: 1.32141888, Val R²: 0.290926
Epoch 00112: reducing learning rate of group 0 to 3.9063e-06.
Epoch 113/200
Train Loss: 0.00043655, Train R²: 0.999798, Val Loss: 1.32257950, Val R²: 0.290303
Epoch 114/200
Train Loss: 0.00057118, Train R²: 0.999736, Val Loss: 1.32145357, Val R²: 0.290907
Epoch 115/200
Train Loss: 0.00103401, Train R²: 0.999522, Val Loss: 1.32241786, Val R²: 0.290390
Epoch 116/200
Train Loss: 0.00040573, Train R²: 0.999813, Val Loss: 1.32242966, Val R²: 0.290383
Epoch 117/200
Train Loss: 0.00042977, Train R²: 0.999801, Val Loss: 1.32149899, Val R²: 0.290883
Epoch 118/200
Train Loss: 0.00066310, Train R²: 0.999694, Val Loss: 1.32332122, Val R²: 0.289905
Epoch 119/200
Train Loss: 0.00056312, Train R²: 0.999740, Val Loss: 1.32086205, Val R²: 0.291225
Epoch 120/200
Train Loss: 0.00063232, Train R²: 0.999708, Val Loss: 1.32150352, Val R²: 0.290880
Epoch 121/200
Train Loss: 0.00059937, Train R²: 0.999723, Val Loss: 1.32257724, Val R²: 0.290304
Epoch 122/200
Train Loss: 0.00060433, Train R²: 0.999721, Val Loss: 1.32339323, Val R²: 0.289866
Epoch 123/200
Train Loss: 0.00087200, Train R²: 0.999597, Val Loss: 1.32143557, Val R²: 0.290917
Epoch 00123: reducing learning rate of group 0 to 1.9531e-06.
Epoch 124/200
Train Loss: 0.00043322, Train R²: 0.999800, Val Loss: 1.32190323, Val R²: 0.290666
Epoch 125/200
Train Loss: 0.00046533, Train R²: 0.999785, Val Loss: 1.32409692, Val R²: 0.289489
Epoch 126/200
Train Loss: 0.00049500, Train R²: 0.999771, Val Loss: 1.32147384, Val R²: 0.290896
Epoch 127/200
Train Loss: 0.00046037, Train R²: 0.999787, Val Loss: 1.32161558, Val R²: 0.290820
Epoch 128/200
Train Loss: 0.00044317, Train R²: 0.999795, Val Loss: 1.32419384, Val R²: 0.289437
Epoch 129/200
Train Loss: 0.00060602, Train R²: 0.999720, Val Loss: 1.32858872, Val R²: 0.287079
Epoch 130/200
Train Loss: 0.00063295, Train R²: 0.999707, Val Loss: 1.32260251, Val R²: 0.290291
Epoch 131/200
Train Loss: 0.00078224, Train R²: 0.999639, Val Loss: 1.32259369, Val R²: 0.290295
Epoch 132/200
Train Loss: 0.00076270, Train R²: 0.999648, Val Loss: 1.32418454, Val R²: 0.289442
Epoch 133/200
Train Loss: 0.00053169, Train R²: 0.999754, Val Loss: 1.32271540, Val R²: 0.290230
Epoch 134/200
Train Loss: 0.00045134, Train R²: 0.999791, Val Loss: 1.32271457, Val R²: 0.290231
Epoch 00134: reducing learning rate of group 0 to 9.7656e-07.
Epoch 135/200
Train Loss: 0.00039366, Train R²: 0.999818, Val Loss: 1.32305229, Val R²: 0.290049
Epoch 136/200
Train Loss: 0.00045389, Train R²: 0.999790, Val Loss: 1.32967818, Val R²: 0.286494
Epoch 137/200
Train Loss: 0.00056583, Train R²: 0.999739, Val Loss: 1.32150388, Val R²: 0.290880
Epoch 138/200
Train Loss: 0.00065568, Train R²: 0.999697, Val Loss: 1.32415473, Val R²: 0.289458
Epoch 139/200
Train Loss: 0.00113266, Train R²: 0.999477, Val Loss: 1.32260919, Val R²: 0.290287
Epoch 140/200
Train Loss: 0.00045165, Train R²: 0.999791, Val Loss: 1.32151818, Val R²: 0.290873
Epoch 141/200
Train Loss: 0.00034808, Train R²: 0.999839, Val Loss: 1.32307887, Val R²: 0.290035
Epoch 142/200
Train Loss: 0.00062997, Train R²: 0.999709, Val Loss: 1.32275796, Val R²: 0.290207
Epoch 143/200
Train Loss: 0.00052800, Train R²: 0.999756, Val Loss: 1.32154787, Val R²: 0.290857
Epoch 144/200
Train Loss: 0.00039773, Train R²: 0.999816, Val Loss: 1.32167077, Val R²: 0.290791
Epoch 145/200
Train Loss: 0.00043069, Train R²: 0.999801, Val Loss: 1.32420528, Val R²: 0.289431
Epoch 00145: reducing learning rate of group 0 to 4.8828e-07.
Epoch 146/200
Train Loss: 0.00051764, Train R²: 0.999761, Val Loss: 1.32155573, Val R²: 0.290852
Epoch 147/200
Train Loss: 0.00040203, Train R²: 0.999814, Val Loss: 1.32277524, Val R²: 0.290198
Epoch 148/200
Train Loss: 0.00051986, Train R²: 0.999760, Val Loss: 1.32265627, Val R²: 0.290262
Epoch 149/200
Train Loss: 0.00045924, Train R²: 0.999788, Val Loss: 1.32421172, Val R²: 0.289427
Epoch 150/200
Train Loss: 0.00046707, Train R²: 0.999784, Val Loss: 1.32168210, Val R²: 0.290785
Epoch 151/200
Train Loss: 0.00040660, Train R²: 0.999812, Val Loss: 1.32156324, Val R²: 0.290848
Epoch 152/200
Train Loss: 0.00040298, Train R²: 0.999814, Val Loss: 1.32895815, Val R²: 0.286880
Epoch 153/200
Train Loss: 0.00074125, Train R²: 0.999657, Val Loss: 1.32421815, Val R²: 0.289424
Epoch 154/200
Train Loss: 0.00084508, Train R²: 0.999609, Val Loss: 1.32422066, Val R²: 0.289422
Epoch 155/200
Train Loss: 0.00047403, Train R²: 0.999781, Val Loss: 1.32312667, Val R²: 0.290009
Epoch 156/200
Train Loss: 0.00045579, Train R²: 0.999789, Val Loss: 1.32157838, Val R²: 0.290840
Epoch 00156: reducing learning rate of group 0 to 2.4414e-07.
Epoch 157/200
Train Loss: 0.00047295, Train R²: 0.999781, Val Loss: 1.32423091, Val R²: 0.289417
Epoch 158/200
Train Loss: 0.00055113, Train R²: 0.999745, Val Loss: 1.32158172, Val R²: 0.290839
Epoch 159/200
Train Loss: 0.00038710, Train R²: 0.999821, Val Loss: 1.32158339, Val R²: 0.290838
Epoch 160/200
Train Loss: 0.00049512, Train R²: 0.999771, Val Loss: 1.32427096, Val R²: 0.289395
Epoch 161/200
Train Loss: 0.00043929, Train R²: 0.999797, Val Loss: 1.32239044, Val R²: 0.290405
Epoch 162/200
Train Loss: 0.00048885, Train R²: 0.999774, Val Loss: 1.31729639, Val R²: 0.293138
Epoch 163/200
Train Loss: 0.00042960, Train R²: 0.999801, Val Loss: 1.32314146, Val R²: 0.290002
Epoch 164/200
Train Loss: 0.00042183, Train R²: 0.999805, Val Loss: 1.32158840, Val R²: 0.290835
Epoch 165/200
Train Loss: 0.00054911, Train R²: 0.999746, Val Loss: 1.32268643, Val R²: 0.290246
Epoch 166/200
Train Loss: 0.00039768, Train R²: 0.999816, Val Loss: 1.32268703, Val R²: 0.290245
Epoch 167/200
Train Loss: 0.00061676, Train R²: 0.999715, Val Loss: 1.32204449, Val R²: 0.290590
Epoch 00167: reducing learning rate of group 0 to 1.2207e-07.
Epoch 168/200
Train Loss: 0.00054328, Train R²: 0.999749, Val Loss: 1.32424378, Val R²: 0.289410
Epoch 169/200
Train Loss: 0.00052273, Train R²: 0.999758, Val Loss: 1.32353187, Val R²: 0.289792
Epoch 170/200
Train Loss: 0.00047342, Train R²: 0.999781, Val Loss: 1.32269073, Val R²: 0.290243
Epoch 171/200
Train Loss: 0.00049541, Train R²: 0.999771, Val Loss: 1.32424521, Val R²: 0.289409
Epoch 172/200
Train Loss: 0.00045409, Train R²: 0.999790, Val Loss: 1.32096756, Val R²: 0.291168
Epoch 173/200
Train Loss: 0.00043587, Train R²: 0.999799, Val Loss: 1.32269192, Val R²: 0.290243
Epoch 174/200
Train Loss: 0.00052283, Train R²: 0.999758, Val Loss: 1.32159603, Val R²: 0.290831
Epoch 175/200
Train Loss: 0.00045798, Train R²: 0.999788, Val Loss: 1.32281375, Val R²: 0.290177
Epoch 176/200
Train Loss: 0.00033855, Train R²: 0.999844, Val Loss: 1.32159746, Val R²: 0.290830
Epoch 177/200
Train Loss: 0.00040229, Train R²: 0.999814, Val Loss: 1.32281518, Val R²: 0.290177
Epoch 178/200
Train Loss: 0.00055698, Train R²: 0.999743, Val Loss: 1.32159865, Val R²: 0.290829
Epoch 00178: reducing learning rate of group 0 to 6.1035e-08.
Epoch 179/200
Train Loss: 0.00051107, Train R²: 0.999764, Val Loss: 1.32320559, Val R²: 0.289967
Epoch 180/200
Train Loss: 0.00052349, Train R²: 0.999758, Val Loss: 1.32428527, Val R²: 0.289388
Epoch 181/200
Train Loss: 0.00077925, Train R²: 0.999640, Val Loss: 1.32269561, Val R²: 0.290241
Epoch 182/200
Train Loss: 0.00044368, Train R²: 0.999795, Val Loss: 1.32194734, Val R²: 0.290642
Epoch 183/200
Train Loss: 0.00066472, Train R²: 0.999693, Val Loss: 1.32424998, Val R²: 0.289407
Epoch 184/200
Train Loss: 0.00048098, Train R²: 0.999778, Val Loss: 1.32424998, Val R²: 0.289407
Epoch 185/200
Train Loss: 0.00039572, Train R²: 0.999817, Val Loss: 1.32269561, Val R²: 0.290241
Epoch 186/200
Train Loss: 0.00058579, Train R²: 0.999729, Val Loss: 1.32159913, Val R²: 0.290829
Epoch 187/200
Train Loss: 0.00040775, Train R²: 0.999812, Val Loss: 1.32424998, Val R²: 0.289407
Epoch 188/200
Train Loss: 0.00041818, Train R²: 0.999807, Val Loss: 1.32159925, Val R²: 0.290829
Epoch 189/200
Train Loss: 0.00045696, Train R²: 0.999789, Val Loss: 1.32306635, Val R²: 0.290042
Epoch 00189: reducing learning rate of group 0 to 3.0518e-08.
Epoch 190/200
Train Loss: 0.00077130, Train R²: 0.999644, Val Loss: 1.32315361, Val R²: 0.289995
Epoch 191/200
Train Loss: 0.00047276, Train R²: 0.999782, Val Loss: 1.32425010, Val R²: 0.289407
Epoch 192/200
Train Loss: 0.00065805, Train R²: 0.999696, Val Loss: 1.32159913, Val R²: 0.290829
Epoch 193/200
Train Loss: 0.00034144, Train R²: 0.999842, Val Loss: 1.32425010, Val R²: 0.289407
Epoch 194/200
Train Loss: 0.00054834, Train R²: 0.999747, Val Loss: 1.32973981, Val R²: 0.286461
Epoch 195/200
Train Loss: 0.00051593, Train R²: 0.999762, Val Loss: 1.32269585, Val R²: 0.290241
Epoch 196/200
Train Loss: 0.00060396, Train R²: 0.999721, Val Loss: 1.32425034, Val R²: 0.289407
Epoch 197/200
Train Loss: 0.00055181, Train R²: 0.999745, Val Loss: 1.32269585, Val R²: 0.290241
Epoch 198/200
Train Loss: 0.00039499, Train R²: 0.999817, Val Loss: 1.32425046, Val R²: 0.289407
Epoch 199/200
Train Loss: 0.00046815, Train R²: 0.999784, Val Loss: 1.32425034, Val R²: 0.289407
Epoch 200/200
Train Loss: 0.00057563, Train R²: 0.999734, Val Loss: 1.32159960, Val R²: 0.290829
Epoch 00200: reducing learning rate of group 0 to 1.5259e-08.
Training Complete. Best Val Loss: 1.2955095767974854
训练时间: 244.24 秒
