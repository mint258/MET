Using device: cuda
Total samples: 1000, Training: 800, Validation: 200
Total trainable parameters: 1477889
Epoch 1/200
Train Loss: 0.03835516, Train R²: -34.380263, Val Loss: 0.00807119, Val R²: -5.579464
Saved best model with validation R2 -5.579464 to best_finetuned_model.pth
Epoch 2/200
Train Loss: 0.00528292, Train R²: -3.873166, Val Loss: 0.00380933, Val R²: -2.105283
Saved best model with validation R2 -2.105283 to best_finetuned_model.pth
Epoch 3/200
Train Loss: 0.00285602, Train R²: -1.634501, Val Loss: 0.00311212, Val R²: -1.536932
Saved best model with validation R2 -1.536932 to best_finetuned_model.pth
Epoch 4/200
Train Loss: 0.00214142, Train R²: -0.975327, Val Loss: 0.00226244, Val R²: -0.844293
Saved best model with validation R2 -0.844293 to best_finetuned_model.pth
Epoch 5/200
Train Loss: 0.00140058, Train R²: -0.291952, Val Loss: 0.00189025, Val R²: -0.540895
Saved best model with validation R2 -0.540895 to best_finetuned_model.pth
Epoch 6/200
Train Loss: 0.00104632, Train R²: 0.034838, Val Loss: 0.00171605, Val R²: -0.398889
Saved best model with validation R2 -0.398889 to best_finetuned_model.pth
Epoch 7/200
Train Loss: 0.00087492, Train R²: 0.192939, Val Loss: 0.00159618, Val R²: -0.301171
Saved best model with validation R2 -0.301171 to best_finetuned_model.pth
Epoch 8/200
Train Loss: 0.00071866, Train R²: 0.337077, Val Loss: 0.00140486, Val R²: -0.145214
Saved best model with validation R2 -0.145214 to best_finetuned_model.pth
Epoch 9/200
Train Loss: 0.00047446, Train R²: 0.562343, Val Loss: 0.00132365, Val R²: -0.079008
Saved best model with validation R2 -0.079008 to best_finetuned_model.pth
Epoch 10/200
Train Loss: 0.00029977, Train R²: 0.723483, Val Loss: 0.00126565, Val R²: -0.031734
Saved best model with validation R2 -0.031734 to best_finetuned_model.pth
Epoch 11/200
Train Loss: 0.00023574, Train R²: 0.782541, Val Loss: 0.00124341, Val R²: -0.013601
Saved best model with validation R2 -0.013601 to best_finetuned_model.pth
Epoch 12/200
Train Loss: 0.00019901, Train R²: 0.816427, Val Loss: 0.00134524, Val R²: -0.096613
Epoch 13/200
Train Loss: 0.00017074, Train R²: 0.842503, Val Loss: 0.00120144, Val R²: 0.020610
Saved best model with validation R2 0.020610 to best_finetuned_model.pth
Epoch 14/200
Train Loss: 0.00016757, Train R²: 0.845427, Val Loss: 0.00121785, Val R²: 0.007238
Epoch 15/200
Train Loss: 0.00017880, Train R²: 0.835065, Val Loss: 0.00125531, Val R²: -0.023302
Epoch 16/200
Train Loss: 0.00016356, Train R²: 0.849130, Val Loss: 0.00127400, Val R²: -0.038538
Epoch 17/200
Train Loss: 0.00092399, Train R²: 0.147679, Val Loss: 0.00148863, Val R²: -0.213502
Epoch 18/200
Train Loss: 0.00091634, Train R²: 0.154731, Val Loss: 0.00149380, Val R²: -0.217717
Epoch 19/200
Train Loss: 0.00079670, Train R²: 0.265091, Val Loss: 0.00148278, Val R²: -0.208733
Epoch 20/200
Train Loss: 0.00056958, Train R²: 0.474601, Val Loss: 0.00157096, Val R²: -0.280615
Epoch 21/200
Train Loss: 0.00045937, Train R²: 0.576261, Val Loss: 0.00128290, Val R²: -0.045794
Epoch 22/200
Train Loss: 0.00032325, Train R²: 0.701819, Val Loss: 0.00129727, Val R²: -0.057507
Epoch 23/200
Train Loss: 0.00027133, Train R²: 0.749712, Val Loss: 0.00131700, Val R²: -0.073590
Epoch 24/200
Train Loss: 0.00030645, Train R²: 0.717323, Val Loss: 0.00128522, Val R²: -0.047683
Epoch 00024: reducing learning rate of group 0 to 5.0000e-04.
Epoch 25/200
Train Loss: 0.00050048, Train R²: 0.538340, Val Loss: 0.00110606, Val R²: 0.098366
Saved best model with validation R2 0.098366 to best_finetuned_model.pth
Epoch 26/200
Train Loss: 0.00066093, Train R²: 0.390329, Val Loss: 0.00121159, Val R²: 0.012337
Epoch 27/200
Train Loss: 0.00063026, Train R²: 0.418628, Val Loss: 0.00126626, Val R²: -0.032227
Epoch 28/200
Train Loss: 0.00058349, Train R²: 0.461764, Val Loss: 0.00123082, Val R²: -0.003335
Epoch 29/200
Train Loss: 0.00058447, Train R²: 0.460860, Val Loss: 0.00126251, Val R²: -0.029172
Epoch 30/200
Train Loss: 0.00043166, Train R²: 0.601822, Val Loss: 0.00132438, Val R²: -0.079610
Epoch 31/200
Train Loss: 0.00034686, Train R²: 0.680042, Val Loss: 0.00119393, Val R²: 0.026736
Epoch 32/200
Train Loss: 0.00032006, Train R²: 0.704765, Val Loss: 0.00121936, Val R²: 0.006004
Epoch 33/200
Train Loss: 0.00030196, Train R²: 0.721457, Val Loss: 0.00108203, Val R²: 0.117951
Saved best model with validation R2 0.117951 to best_finetuned_model.pth
Epoch 34/200
Train Loss: 0.00023507, Train R²: 0.783161, Val Loss: 0.00111120, Val R²: 0.094171
Epoch 35/200
Train Loss: 0.00020484, Train R²: 0.811047, Val Loss: 0.00101772, Val R²: 0.170380
Saved best model with validation R2 0.170380 to best_finetuned_model.pth
Epoch 36/200
Train Loss: 0.00013765, Train R²: 0.873025, Val Loss: 0.00100007, Val R²: 0.184762
Saved best model with validation R2 0.184762 to best_finetuned_model.pth
Epoch 37/200
Train Loss: 0.00010657, Train R²: 0.901700, Val Loss: 0.00094406, Val R²: 0.230426
Saved best model with validation R2 0.230426 to best_finetuned_model.pth
Epoch 38/200
Train Loss: 0.00008627, Train R²: 0.920421, Val Loss: 0.00097003, Val R²: 0.209249
Epoch 39/200
Train Loss: 0.00006834, Train R²: 0.936963, Val Loss: 0.00095572, Val R²: 0.220917
Epoch 40/200
Train Loss: 0.00005181, Train R²: 0.952207, Val Loss: 0.00096468, Val R²: 0.213617
Epoch 41/200
Train Loss: 0.00005216, Train R²: 0.951888, Val Loss: 0.00095895, Val R²: 0.218285
Epoch 42/200
Train Loss: 0.00004694, Train R²: 0.956698, Val Loss: 0.00097215, Val R²: 0.207521
Epoch 43/200
Train Loss: 0.00003274, Train R²: 0.969802, Val Loss: 0.00097970, Val R²: 0.201368
Epoch 44/200
Train Loss: 0.00003812, Train R²: 0.964839, Val Loss: 0.00100014, Val R²: 0.184710
Epoch 45/200
Train Loss: 0.00003235, Train R²: 0.970159, Val Loss: 0.00098910, Val R²: 0.193710
Epoch 46/200
Train Loss: 0.00003026, Train R²: 0.972085, Val Loss: 0.00104661, Val R²: 0.146827
Epoch 47/200
Train Loss: 0.00003993, Train R²: 0.963167, Val Loss: 0.00104733, Val R²: 0.146235
Epoch 48/200
Train Loss: 0.00006198, Train R²: 0.942830, Val Loss: 0.00104414, Val R²: 0.148836
Epoch 00048: reducing learning rate of group 0 to 2.5000e-04.
Epoch 49/200
Train Loss: 0.00006107, Train R²: 0.943669, Val Loss: 0.00105958, Val R²: 0.136256
Epoch 50/200
Train Loss: 0.00005423, Train R²: 0.949973, Val Loss: 0.00104303, Val R²: 0.149741
Epoch 51/200
Train Loss: 0.00004500, Train R²: 0.958490, Val Loss: 0.00103209, Val R²: 0.158666
Epoch 52/200
Train Loss: 0.00003693, Train R²: 0.965933, Val Loss: 0.00102019, Val R²: 0.168366
Epoch 53/200
Train Loss: 0.00003262, Train R²: 0.969907, Val Loss: 0.00102135, Val R²: 0.167416
Epoch 54/200
Train Loss: 0.00002317, Train R²: 0.978627, Val Loss: 0.00101881, Val R²: 0.169491
Epoch 55/200
Train Loss: 0.00001779, Train R²: 0.983590, Val Loss: 0.00100764, Val R²: 0.178593
Epoch 56/200
Train Loss: 0.00001353, Train R²: 0.987523, Val Loss: 0.00100897, Val R²: 0.177513
Epoch 57/200
Train Loss: 0.00001172, Train R²: 0.989190, Val Loss: 0.00100567, Val R²: 0.180196
Epoch 58/200
Train Loss: 0.00000913, Train R²: 0.991580, Val Loss: 0.00100553, Val R²: 0.180311
Epoch 59/200
Train Loss: 0.00000648, Train R²: 0.994020, Val Loss: 0.00100560, Val R²: 0.180255
Epoch 00059: reducing learning rate of group 0 to 1.2500e-04.
Epoch 60/200
Train Loss: 0.00000497, Train R²: 0.995420, Val Loss: 0.00100620, Val R²: 0.179767
Epoch 61/200
Train Loss: 0.00000970, Train R²: 0.991055, Val Loss: 0.00100781, Val R²: 0.178453
Epoch 62/200
Train Loss: 0.00000849, Train R²: 0.992173, Val Loss: 0.00100705, Val R²: 0.179073
Epoch 63/200
Train Loss: 0.00000531, Train R²: 0.995104, Val Loss: 0.00100571, Val R²: 0.180165
Epoch 64/200
Train Loss: 0.00000370, Train R²: 0.996583, Val Loss: 0.00100644, Val R²: 0.179569
Epoch 65/200
Train Loss: 0.00000574, Train R²: 0.994706, Val Loss: 0.00100622, Val R²: 0.179752
Epoch 66/200
Train Loss: 0.00000620, Train R²: 0.994278, Val Loss: 0.00100667, Val R²: 0.179380
Epoch 67/200
Train Loss: 0.00000722, Train R²: 0.993341, Val Loss: 0.00100644, Val R²: 0.179568
Epoch 68/200
Train Loss: 0.00000661, Train R²: 0.993902, Val Loss: 0.00100658, Val R²: 0.179458
Epoch 69/200
Train Loss: 0.00000482, Train R²: 0.995553, Val Loss: 0.00100632, Val R²: 0.179666
Epoch 70/200
Train Loss: 0.00000347, Train R²: 0.996796, Val Loss: 0.00100480, Val R²: 0.180912
Epoch 00070: reducing learning rate of group 0 to 6.2500e-05.
Epoch 71/200
Train Loss: 0.00000371, Train R²: 0.996575, Val Loss: 0.00100446, Val R²: 0.181187
Epoch 72/200
Train Loss: 0.00000307, Train R²: 0.997168, Val Loss: 0.00101120, Val R²: 0.175688
Epoch 73/200
Train Loss: 0.00000283, Train R²: 0.997389, Val Loss: 0.00101054, Val R²: 0.176228
Epoch 74/200
Train Loss: 0.00000277, Train R²: 0.997444, Val Loss: 0.00101561, Val R²: 0.172095
Epoch 75/200
Train Loss: 0.00000333, Train R²: 0.996931, Val Loss: 0.00101468, Val R²: 0.172853
Epoch 76/200
Train Loss: 0.00000412, Train R²: 0.996200, Val Loss: 0.00102040, Val R²: 0.168190
Epoch 77/200
Train Loss: 0.00000456, Train R²: 0.995791, Val Loss: 0.00102630, Val R²: 0.163383
Epoch 78/200
Train Loss: 0.00000496, Train R²: 0.995425, Val Loss: 0.00102588, Val R²: 0.163725
Epoch 79/200
Train Loss: 0.00000567, Train R²: 0.994768, Val Loss: 0.00102546, Val R²: 0.164064
Epoch 80/200
Train Loss: 0.00000604, Train R²: 0.994428, Val Loss: 0.00101639, Val R²: 0.171460
Epoch 81/200
Train Loss: 0.00000394, Train R²: 0.996366, Val Loss: 0.00102050, Val R²: 0.168113
Epoch 00081: reducing learning rate of group 0 to 3.1250e-05.
Epoch 82/200
Train Loss: 0.00000355, Train R²: 0.996726, Val Loss: 0.00101424, Val R²: 0.173211
Epoch 83/200
Train Loss: 0.00000420, Train R²: 0.996121, Val Loss: 0.00101479, Val R²: 0.172765
Epoch 84/200
Train Loss: 0.00000380, Train R²: 0.996498, Val Loss: 0.00101974, Val R²: 0.168730
Epoch 85/200
Train Loss: 0.00000332, Train R²: 0.996934, Val Loss: 0.00102262, Val R²: 0.166385
Epoch 86/200
Train Loss: 0.00000330, Train R²: 0.996955, Val Loss: 0.00101651, Val R²: 0.171361
Epoch 87/200
Train Loss: 0.00000243, Train R²: 0.997755, Val Loss: 0.00102134, Val R²: 0.167427
Epoch 88/200
Train Loss: 0.00000278, Train R²: 0.997438, Val Loss: 0.00102043, Val R²: 0.168170
Epoch 89/200
Train Loss: 0.00000223, Train R²: 0.997945, Val Loss: 0.00102330, Val R²: 0.165831
Epoch 90/200
Train Loss: 0.00000218, Train R²: 0.997992, Val Loss: 0.00101457, Val R²: 0.172945
Epoch 91/200
Train Loss: 0.00000190, Train R²: 0.998249, Val Loss: 0.00101829, Val R²: 0.169911
Epoch 92/200
Train Loss: 0.00000237, Train R²: 0.997811, Val Loss: 0.00101511, Val R²: 0.172500
Epoch 00092: reducing learning rate of group 0 to 1.5625e-05.
Epoch 93/200
Train Loss: 0.00000201, Train R²: 0.998144, Val Loss: 0.00102071, Val R²: 0.167940
Epoch 94/200
Train Loss: 0.00000236, Train R²: 0.997825, Val Loss: 0.00101473, Val R²: 0.172813
Epoch 95/200
Train Loss: 0.00000181, Train R²: 0.998331, Val Loss: 0.00102084, Val R²: 0.167829
Epoch 96/200
Train Loss: 0.00000203, Train R²: 0.998131, Val Loss: 0.00101501, Val R²: 0.172586
Epoch 97/200
Train Loss: 0.00000242, Train R²: 0.997767, Val Loss: 0.00101432, Val R²: 0.173145
Epoch 98/200
Train Loss: 0.00000186, Train R²: 0.998289, Val Loss: 0.00101454, Val R²: 0.172968
Epoch 99/200
Train Loss: 0.00000169, Train R²: 0.998445, Val Loss: 0.00102038, Val R²: 0.168207
Epoch 100/200
Train Loss: 0.00000242, Train R²: 0.997766, Val Loss: 0.00101740, Val R²: 0.170638
Epoch 101/200
Train Loss: 0.00000422, Train R²: 0.996109, Val Loss: 0.00102024, Val R²: 0.168325
Epoch 102/200
Train Loss: 0.00000148, Train R²: 0.998639, Val Loss: 0.00101545, Val R²: 0.172228
Epoch 103/200
Train Loss: 0.00000163, Train R²: 0.998500, Val Loss: 0.00101547, Val R²: 0.172208
Epoch 00103: reducing learning rate of group 0 to 7.8125e-06.
Epoch 104/200
Train Loss: 0.00000168, Train R²: 0.998446, Val Loss: 0.00102482, Val R²: 0.164588
Epoch 105/200
Train Loss: 0.00000141, Train R²: 0.998704, Val Loss: 0.00101479, Val R²: 0.172764
Epoch 106/200
Train Loss: 0.00000187, Train R²: 0.998278, Val Loss: 0.00101483, Val R²: 0.172731
Epoch 107/200
Train Loss: 0.00000148, Train R²: 0.998634, Val Loss: 0.00102045, Val R²: 0.168153
Epoch 108/200
Train Loss: 0.00000131, Train R²: 0.998796, Val Loss: 0.00101358, Val R²: 0.173748
Epoch 109/200
Train Loss: 0.00000129, Train R²: 0.998812, Val Loss: 0.00101952, Val R²: 0.168905
Epoch 110/200
Train Loss: 0.00000139, Train R²: 0.998715, Val Loss: 0.00101361, Val R²: 0.173725
Epoch 111/200
Train Loss: 0.00000119, Train R²: 0.998898, Val Loss: 0.00101346, Val R²: 0.173846
Epoch 112/200
Train Loss: 0.00000179, Train R²: 0.998350, Val Loss: 0.00101716, Val R²: 0.170834
Epoch 113/200
Train Loss: 0.00000139, Train R²: 0.998715, Val Loss: 0.00101973, Val R²: 0.168735
Epoch 114/200
Train Loss: 0.00000121, Train R²: 0.998885, Val Loss: 0.00101752, Val R²: 0.170539
Epoch 00114: reducing learning rate of group 0 to 3.9063e-06.
Epoch 115/200
Train Loss: 0.00000147, Train R²: 0.998641, Val Loss: 0.00101378, Val R²: 0.173590
Epoch 116/200
Train Loss: 0.00000174, Train R²: 0.998395, Val Loss: 0.00101383, Val R²: 0.173544
Epoch 117/200
Train Loss: 0.00000117, Train R²: 0.998918, Val Loss: 0.00101342, Val R²: 0.173881
Epoch 118/200
Train Loss: 0.00000117, Train R²: 0.998918, Val Loss: 0.00101975, Val R²: 0.168723
Epoch 119/200
Train Loss: 0.00000203, Train R²: 0.998123, Val Loss: 0.00101393, Val R²: 0.173463
Epoch 120/200
Train Loss: 0.00000118, Train R²: 0.998909, Val Loss: 0.00101380, Val R²: 0.173572
Epoch 121/200
Train Loss: 0.00000136, Train R²: 0.998745, Val Loss: 0.00101384, Val R²: 0.173539
Epoch 122/200
Train Loss: 0.00000222, Train R²: 0.997954, Val Loss: 0.00101403, Val R²: 0.173384
Epoch 123/200
Train Loss: 0.00000110, Train R²: 0.998985, Val Loss: 0.00101964, Val R²: 0.168809
Epoch 124/200
Train Loss: 0.00000119, Train R²: 0.998905, Val Loss: 0.00101310, Val R²: 0.174141
Epoch 125/200
Train Loss: 0.00000109, Train R²: 0.998996, Val Loss: 0.00101399, Val R²: 0.173417
Epoch 00125: reducing learning rate of group 0 to 1.9531e-06.
Epoch 126/200
Train Loss: 0.00000138, Train R²: 0.998725, Val Loss: 0.00101963, Val R²: 0.168819
Epoch 127/200
Train Loss: 0.00000142, Train R²: 0.998687, Val Loss: 0.00101351, Val R²: 0.173806
Epoch 128/200
Train Loss: 0.00000120, Train R²: 0.998897, Val Loss: 0.00101952, Val R²: 0.168906
Epoch 129/200
Train Loss: 0.00000165, Train R²: 0.998474, Val Loss: 0.00101353, Val R²: 0.173792
Epoch 130/200
Train Loss: 0.00000169, Train R²: 0.998437, Val Loss: 0.00101738, Val R²: 0.170657
Epoch 131/200
Train Loss: 0.00000143, Train R²: 0.998678, Val Loss: 0.00101976, Val R²: 0.168709
Epoch 132/200
Train Loss: 0.00000188, Train R²: 0.998261, Val Loss: 0.00101374, Val R²: 0.173624
Epoch 133/200
Train Loss: 0.00000158, Train R²: 0.998541, Val Loss: 0.00102345, Val R²: 0.165709
Epoch 134/200
Train Loss: 0.00000131, Train R²: 0.998788, Val Loss: 0.00101343, Val R²: 0.173875
Epoch 135/200
Train Loss: 0.00000101, Train R²: 0.999064, Val Loss: 0.00101986, Val R²: 0.168631
Epoch 136/200
Train Loss: 0.00000110, Train R²: 0.998983, Val Loss: 0.00101748, Val R²: 0.170572
Epoch 00136: reducing learning rate of group 0 to 9.7656e-07.
Epoch 137/200
Train Loss: 0.00000140, Train R²: 0.998710, Val Loss: 0.00101724, Val R²: 0.170764
Epoch 138/200
Train Loss: 0.00000155, Train R²: 0.998572, Val Loss: 0.00101744, Val R²: 0.170601
Epoch 139/200
Train Loss: 0.00000117, Train R²: 0.998920, Val Loss: 0.00101962, Val R²: 0.168831
Epoch 140/200
Train Loss: 0.00000119, Train R²: 0.998898, Val Loss: 0.00101387, Val R²: 0.173514
Epoch 141/200
Train Loss: 0.00000363, Train R²: 0.996652, Val Loss: 0.00101975, Val R²: 0.168723
Epoch 142/200
Train Loss: 0.00000131, Train R²: 0.998792, Val Loss: 0.00101374, Val R²: 0.173618
Epoch 143/200
Train Loss: 0.00000121, Train R²: 0.998881, Val Loss: 0.00101362, Val R²: 0.173717
Epoch 144/200
Train Loss: 0.00000118, Train R²: 0.998914, Val Loss: 0.00102350, Val R²: 0.165661
Epoch 145/200
Train Loss: 0.00000119, Train R²: 0.998902, Val Loss: 0.00101380, Val R²: 0.173571
Epoch 146/200
Train Loss: 0.00000103, Train R²: 0.999045, Val Loss: 0.00102352, Val R²: 0.165650
Epoch 147/200
Train Loss: 0.00000111, Train R²: 0.998974, Val Loss: 0.00101394, Val R²: 0.173458
Epoch 00147: reducing learning rate of group 0 to 4.8828e-07.
Epoch 148/200
Train Loss: 0.00000206, Train R²: 0.998103, Val Loss: 0.00101361, Val R²: 0.173724
Epoch 149/200
Train Loss: 0.00000147, Train R²: 0.998639, Val Loss: 0.00101381, Val R²: 0.173564
Epoch 150/200
Train Loss: 0.00000137, Train R²: 0.998735, Val Loss: 0.00101369, Val R²: 0.173664
Epoch 151/200
Train Loss: 0.00000125, Train R²: 0.998845, Val Loss: 0.00101751, Val R²: 0.170548
Epoch 152/200
Train Loss: 0.00000092, Train R²: 0.999151, Val Loss: 0.00101355, Val R²: 0.173773
Epoch 153/200
Train Loss: 0.00000148, Train R²: 0.998638, Val Loss: 0.00101314, Val R²: 0.174112
Epoch 154/200
Train Loss: 0.00000106, Train R²: 0.999022, Val Loss: 0.00101421, Val R²: 0.173234
Epoch 155/200
Train Loss: 0.00000116, Train R²: 0.998934, Val Loss: 0.00101363, Val R²: 0.173713
Epoch 156/200
Train Loss: 0.00000110, Train R²: 0.998983, Val Loss: 0.00101363, Val R²: 0.173710
Epoch 157/200
Train Loss: 0.00000135, Train R²: 0.998755, Val Loss: 0.00101395, Val R²: 0.173448
Epoch 158/200
Train Loss: 0.00000170, Train R²: 0.998431, Val Loss: 0.00101401, Val R²: 0.173399
Epoch 00158: reducing learning rate of group 0 to 2.4414e-07.
Epoch 159/200
Train Loss: 0.00000147, Train R²: 0.998640, Val Loss: 0.00101775, Val R²: 0.170355
Epoch 160/200
Train Loss: 0.00000094, Train R²: 0.999133, Val Loss: 0.00101417, Val R²: 0.173270
Epoch 161/200
Train Loss: 0.00000112, Train R²: 0.998969, Val Loss: 0.00101419, Val R²: 0.173252
Epoch 162/200
Train Loss: 0.00000140, Train R²: 0.998707, Val Loss: 0.00102339, Val R²: 0.165755
Epoch 163/200
Train Loss: 0.00000135, Train R²: 0.998758, Val Loss: 0.00101419, Val R²: 0.173257
Epoch 164/200
Train Loss: 0.00000099, Train R²: 0.999084, Val Loss: 0.00101736, Val R²: 0.170666
Epoch 165/200
Train Loss: 0.00000121, Train R²: 0.998881, Val Loss: 0.00101301, Val R²: 0.174214
Epoch 166/200
Train Loss: 0.00000108, Train R²: 0.999005, Val Loss: 0.00102351, Val R²: 0.165660
Epoch 167/200
Train Loss: 0.00000129, Train R²: 0.998814, Val Loss: 0.00102350, Val R²: 0.165662
Epoch 168/200
Train Loss: 0.00000119, Train R²: 0.998898, Val Loss: 0.00101418, Val R²: 0.173265
Epoch 169/200
Train Loss: 0.00000149, Train R²: 0.998623, Val Loss: 0.00101773, Val R²: 0.170368
Epoch 00169: reducing learning rate of group 0 to 1.2207e-07.
Epoch 170/200
Train Loss: 0.00000274, Train R²: 0.997468, Val Loss: 0.00102350, Val R²: 0.165661
Epoch 171/200
Train Loss: 0.00000120, Train R²: 0.998890, Val Loss: 0.00101734, Val R²: 0.170683
Epoch 172/200
Train Loss: 0.00000105, Train R²: 0.999027, Val Loss: 0.00101417, Val R²: 0.173266
Epoch 173/200
Train Loss: 0.00000094, Train R²: 0.999131, Val Loss: 0.00101963, Val R²: 0.168821
Epoch 174/200
Train Loss: 0.00000115, Train R²: 0.998940, Val Loss: 0.00101995, Val R²: 0.168562
Epoch 175/200
Train Loss: 0.00000122, Train R²: 0.998874, Val Loss: 0.00101347, Val R²: 0.173844
Epoch 176/200
Train Loss: 0.00000208, Train R²: 0.998086, Val Loss: 0.00101392, Val R²: 0.173470
Epoch 177/200
Train Loss: 0.00000136, Train R²: 0.998748, Val Loss: 0.00101393, Val R²: 0.173470
Epoch 178/200
Train Loss: 0.00000130, Train R²: 0.998805, Val Loss: 0.00101399, Val R²: 0.173417
Epoch 179/200
Train Loss: 0.00000118, Train R²: 0.998912, Val Loss: 0.00101399, Val R²: 0.173417
Epoch 180/200
Train Loss: 0.00000106, Train R²: 0.999023, Val Loss: 0.00101960, Val R²: 0.168841
Epoch 00180: reducing learning rate of group 0 to 6.1035e-08.
Epoch 181/200
Train Loss: 0.00000139, Train R²: 0.998715, Val Loss: 0.00101392, Val R²: 0.173470
Epoch 182/200
Train Loss: 0.00000116, Train R²: 0.998928, Val Loss: 0.00101379, Val R²: 0.173584
Epoch 183/200
Train Loss: 0.00000124, Train R²: 0.998860, Val Loss: 0.00101392, Val R²: 0.173470
Epoch 184/200
Train Loss: 0.00000117, Train R²: 0.998921, Val Loss: 0.00101390, Val R²: 0.173488
Epoch 185/200
Train Loss: 0.00000123, Train R²: 0.998867, Val Loss: 0.00101994, Val R²: 0.168563
Epoch 186/200
Train Loss: 0.00000272, Train R²: 0.997491, Val Loss: 0.00101374, Val R²: 0.173617
Epoch 187/200
Train Loss: 0.00000108, Train R²: 0.999001, Val Loss: 0.00101367, Val R²: 0.173680
Epoch 188/200
Train Loss: 0.00000151, Train R²: 0.998604, Val Loss: 0.00101399, Val R²: 0.173417
Epoch 189/200
Train Loss: 0.00000105, Train R²: 0.999033, Val Loss: 0.00101748, Val R²: 0.170573
Epoch 190/200
Train Loss: 0.00000119, Train R²: 0.998904, Val Loss: 0.00101367, Val R²: 0.173680
Epoch 191/200
Train Loss: 0.00000180, Train R²: 0.998343, Val Loss: 0.00101390, Val R²: 0.173488
Epoch 00191: reducing learning rate of group 0 to 3.0518e-08.
Epoch 192/200
Train Loss: 0.00000113, Train R²: 0.998959, Val Loss: 0.00101374, Val R²: 0.173617
Epoch 193/200
Train Loss: 0.00000169, Train R²: 0.998437, Val Loss: 0.00102001, Val R²: 0.168510
Epoch 194/200
Train Loss: 0.00000111, Train R²: 0.998980, Val Loss: 0.00102017, Val R²: 0.168381
Epoch 195/200
Train Loss: 0.00000109, Train R²: 0.998995, Val Loss: 0.00101417, Val R²: 0.173271
Epoch 196/200
Train Loss: 0.00000158, Train R²: 0.998545, Val Loss: 0.00102360, Val R²: 0.165580
Epoch 197/200
Train Loss: 0.00000118, Train R²: 0.998915, Val Loss: 0.00102336, Val R²: 0.165781
Epoch 198/200
Train Loss: 0.00000125, Train R²: 0.998845, Val Loss: 0.00101367, Val R²: 0.173681
Epoch 199/200
Train Loss: 0.00000133, Train R²: 0.998777, Val Loss: 0.00101349, Val R²: 0.173828
Epoch 200/200
Train Loss: 0.00000131, Train R²: 0.998794, Val Loss: 0.00101417, Val R²: 0.173271
Training Complete. Best Val Loss: 0.0009440556750632823
训练时间: 230.60 秒
