Using device: cuda
Total samples: 1000, Training: 800, Validation: 200
Total trainable parameters: 1477889
Epoch 1/200
Train Loss: 0.03883686, Train R²: -32.799150, Val Loss: 0.00645803, Val R²: -5.777651
Saved best model with validation R2 -5.777651 to best_finetuned_model.pth
Epoch 2/200
Train Loss: 0.00433524, Train R²: -2.772895, Val Loss: 0.00226250, Val R²: -1.374475
Saved best model with validation R2 -1.374475 to best_finetuned_model.pth
Epoch 3/200
Train Loss: 0.00200850, Train R²: -0.747965, Val Loss: 0.00200505, Val R²: -1.104287
Saved best model with validation R2 -1.104287 to best_finetuned_model.pth
Epoch 4/200
Train Loss: 0.00151385, Train R²: -0.317484, Val Loss: 0.00119833, Val R²: -0.257638
Saved best model with validation R2 -0.257638 to best_finetuned_model.pth
Epoch 5/200
Train Loss: 0.00134653, Train R²: -0.171863, Val Loss: 0.00174779, Val R²: -0.834296
Epoch 6/200
Train Loss: 0.00124059, Train R²: -0.079665, Val Loss: 0.00106848, Val R²: -0.121362
Saved best model with validation R2 -0.121362 to best_finetuned_model.pth
Epoch 7/200
Train Loss: 0.00095886, Train R²: 0.165516, Val Loss: 0.00115247, Val R²: -0.209507
Epoch 8/200
Train Loss: 0.00079704, Train R²: 0.306352, Val Loss: 0.00087115, Val R²: 0.085739
Saved best model with validation R2 0.085739 to best_finetuned_model.pth
Epoch 9/200
Train Loss: 0.00063244, Train R²: 0.449598, Val Loss: 0.00104198, Val R²: -0.093554
Epoch 10/200
Train Loss: 0.00049967, Train R²: 0.565147, Val Loss: 0.00080145, Val R²: 0.158884
Saved best model with validation R2 0.158884 to best_finetuned_model.pth
Epoch 11/200
Train Loss: 0.00044775, Train R²: 0.610327, Val Loss: 0.00104293, Val R²: -0.094548
Epoch 12/200
Train Loss: 0.00034084, Train R²: 0.703373, Val Loss: 0.00079308, Val R²: 0.167665
Saved best model with validation R2 0.167665 to best_finetuned_model.pth
Epoch 13/200
Train Loss: 0.00032592, Train R²: 0.716353, Val Loss: 0.00095768, Val R²: -0.005083
Epoch 14/200
Train Loss: 0.00025838, Train R²: 0.775135, Val Loss: 0.00078288, Val R²: 0.178374
Saved best model with validation R2 0.178374 to best_finetuned_model.pth
Epoch 15/200
Train Loss: 0.00027909, Train R²: 0.757113, Val Loss: 0.00090725, Val R²: 0.047851
Epoch 16/200
Train Loss: 0.00029414, Train R²: 0.744016, Val Loss: 0.00088081, Val R²: 0.075598
Epoch 17/200
Train Loss: 0.00034841, Train R²: 0.696784, Val Loss: 0.00079528, Val R²: 0.165362
Epoch 18/200
Train Loss: 0.00035143, Train R²: 0.694153, Val Loss: 0.00089824, Val R²: 0.057307
Epoch 19/200
Train Loss: 0.00050076, Train R²: 0.564200, Val Loss: 0.00075497, Val R²: 0.207662
Saved best model with validation R2 0.207662 to best_finetuned_model.pth
Epoch 20/200
Train Loss: 0.00069722, Train R²: 0.393215, Val Loss: 0.00111600, Val R²: -0.171234
Epoch 21/200
Train Loss: 0.00063019, Train R²: 0.451554, Val Loss: 0.00110578, Val R²: -0.160510
Epoch 22/200
Train Loss: 0.00056422, Train R²: 0.508969, Val Loss: 0.00080654, Val R²: 0.153538
Epoch 23/200
Train Loss: 0.00047445, Train R²: 0.587092, Val Loss: 0.00078307, Val R²: 0.178169
Epoch 24/200
Train Loss: 0.00037779, Train R²: 0.671218, Val Loss: 0.00099955, Val R²: -0.049018
Epoch 25/200
Train Loss: 0.00038149, Train R²: 0.667999, Val Loss: 0.00084458, Val R²: 0.113618
Epoch 26/200
Train Loss: 0.00032735, Train R²: 0.715109, Val Loss: 0.00073540, Val R²: 0.228206
Saved best model with validation R2 0.228206 to best_finetuned_model.pth
Epoch 27/200
Train Loss: 0.00027479, Train R²: 0.760856, Val Loss: 0.00072543, Val R²: 0.238666
Saved best model with validation R2 0.238666 to best_finetuned_model.pth
Epoch 28/200
Train Loss: 0.00026674, Train R²: 0.767858, Val Loss: 0.00070072, Val R²: 0.264602
Saved best model with validation R2 0.264602 to best_finetuned_model.pth
Epoch 29/200
Train Loss: 0.00021251, Train R²: 0.815060, Val Loss: 0.00075484, Val R²: 0.207804
Epoch 30/200
Train Loss: 0.00017678, Train R²: 0.846152, Val Loss: 0.00087139, Val R²: 0.085478
Epoch 31/200
Train Loss: 0.00018242, Train R²: 0.841241, Val Loss: 0.00071168, Val R²: 0.253096
Epoch 32/200
Train Loss: 0.00013180, Train R²: 0.885299, Val Loss: 0.00066796, Val R²: 0.298979
Saved best model with validation R2 0.298979 to best_finetuned_model.pth
Epoch 33/200
Train Loss: 0.00011912, Train R²: 0.896331, Val Loss: 0.00069859, Val R²: 0.266833
Epoch 34/200
Train Loss: 0.00010096, Train R²: 0.912139, Val Loss: 0.00068164, Val R²: 0.284621
Epoch 35/200
Train Loss: 0.00009933, Train R²: 0.913551, Val Loss: 0.00080461, Val R²: 0.155564
Epoch 36/200
Train Loss: 0.00011266, Train R²: 0.901955, Val Loss: 0.00075411, Val R²: 0.208568
Epoch 37/200
Train Loss: 0.00016184, Train R²: 0.859155, Val Loss: 0.00069467, Val R²: 0.270948
Epoch 38/200
Train Loss: 0.00019074, Train R²: 0.834004, Val Loss: 0.00068658, Val R²: 0.279440
Epoch 39/200
Train Loss: 0.00014085, Train R²: 0.877424, Val Loss: 0.00075737, Val R²: 0.205142
Epoch 40/200
Train Loss: 0.00013932, Train R²: 0.878748, Val Loss: 0.00076516, Val R²: 0.196974
Epoch 41/200
Train Loss: 0.00014348, Train R²: 0.875133, Val Loss: 0.00073113, Val R²: 0.232686
Epoch 42/200
Train Loss: 0.00010294, Train R²: 0.910410, Val Loss: 0.00070066, Val R²: 0.264663
Epoch 43/200
Train Loss: 0.00008439, Train R²: 0.926554, Val Loss: 0.00071150, Val R²: 0.253285
Epoch 00043: reducing learning rate of group 0 to 5.0000e-04.
Epoch 44/200
Train Loss: 0.00007089, Train R²: 0.938304, Val Loss: 0.00076215, Val R²: 0.200131
Epoch 45/200
Train Loss: 0.00006079, Train R²: 0.947098, Val Loss: 0.00071081, Val R²: 0.254014
Epoch 46/200
Train Loss: 0.00004714, Train R²: 0.958971, Val Loss: 0.00079073, Val R²: 0.170137
Epoch 47/200
Train Loss: 0.00004666, Train R²: 0.959393, Val Loss: 0.00072556, Val R²: 0.238533
Epoch 48/200
Train Loss: 0.00005207, Train R²: 0.954687, Val Loss: 0.00073049, Val R²: 0.233354
Epoch 49/200
Train Loss: 0.00005836, Train R²: 0.949213, Val Loss: 0.00073394, Val R²: 0.229734
Epoch 50/200
Train Loss: 0.00005869, Train R²: 0.948924, Val Loss: 0.00068281, Val R²: 0.283395
Epoch 51/200
Train Loss: 0.00005781, Train R²: 0.949689, Val Loss: 0.00074080, Val R²: 0.222539
Epoch 52/200
Train Loss: 0.00004396, Train R²: 0.961744, Val Loss: 0.00071449, Val R²: 0.250153
Epoch 53/200
Train Loss: 0.00004005, Train R²: 0.965143, Val Loss: 0.00075662, Val R²: 0.205933
Epoch 54/200
Train Loss: 0.00003517, Train R²: 0.969388, Val Loss: 0.00070135, Val R²: 0.263944
Epoch 00054: reducing learning rate of group 0 to 2.5000e-04.
Epoch 55/200
Train Loss: 0.00002554, Train R²: 0.977772, Val Loss: 0.00072951, Val R²: 0.234383
Epoch 56/200
Train Loss: 0.00002446, Train R²: 0.978717, Val Loss: 0.00071056, Val R²: 0.254271
Epoch 57/200
Train Loss: 0.00002005, Train R²: 0.982550, Val Loss: 0.00072655, Val R²: 0.237496
Epoch 58/200
Train Loss: 0.00001789, Train R²: 0.984430, Val Loss: 0.00071593, Val R²: 0.248632
Epoch 59/200
Train Loss: 0.00001727, Train R²: 0.984972, Val Loss: 0.00072746, Val R²: 0.236531
Epoch 60/200
Train Loss: 0.00001485, Train R²: 0.987075, Val Loss: 0.00071175, Val R²: 0.253028
Epoch 61/200
Train Loss: 0.00001357, Train R²: 0.988194, Val Loss: 0.00070763, Val R²: 0.257351
Epoch 62/200
Train Loss: 0.00001242, Train R²: 0.989189, Val Loss: 0.00071395, Val R²: 0.250712
Epoch 63/200
Train Loss: 0.00001088, Train R²: 0.990534, Val Loss: 0.00071051, Val R²: 0.254323
Epoch 64/200
Train Loss: 0.00001058, Train R²: 0.990792, Val Loss: 0.00071667, Val R²: 0.247856
Epoch 65/200
Train Loss: 0.00001061, Train R²: 0.990766, Val Loss: 0.00069938, Val R²: 0.266004
Epoch 00065: reducing learning rate of group 0 to 1.2500e-04.
Epoch 66/200
Train Loss: 0.00001199, Train R²: 0.989562, Val Loss: 0.00069699, Val R²: 0.268514
Epoch 67/200
Train Loss: 0.00001086, Train R²: 0.990552, Val Loss: 0.00069965, Val R²: 0.265719
Epoch 68/200
Train Loss: 0.00001244, Train R²: 0.989169, Val Loss: 0.00069429, Val R²: 0.271349
Epoch 69/200
Train Loss: 0.00001187, Train R²: 0.989667, Val Loss: 0.00069184, Val R²: 0.273923
Epoch 70/200
Train Loss: 0.00001024, Train R²: 0.991092, Val Loss: 0.00069729, Val R²: 0.268195
Epoch 71/200
Train Loss: 0.00000998, Train R²: 0.991312, Val Loss: 0.00068784, Val R²: 0.278119
Epoch 72/200
Train Loss: 0.00000958, Train R²: 0.991665, Val Loss: 0.00069780, Val R²: 0.267666
Epoch 73/200
Train Loss: 0.00000865, Train R²: 0.992472, Val Loss: 0.00069654, Val R²: 0.268990
Epoch 74/200
Train Loss: 0.00000984, Train R²: 0.991441, Val Loss: 0.00069513, Val R²: 0.270467
Epoch 75/200
Train Loss: 0.00000821, Train R²: 0.992851, Val Loss: 0.00069717, Val R²: 0.268325
Epoch 76/200
Train Loss: 0.00000879, Train R²: 0.992353, Val Loss: 0.00069893, Val R²: 0.266475
Epoch 00076: reducing learning rate of group 0 to 6.2500e-05.
Epoch 77/200
Train Loss: 0.00000834, Train R²: 0.992739, Val Loss: 0.00069752, Val R²: 0.267958
Epoch 78/200
Train Loss: 0.00000743, Train R²: 0.993538, Val Loss: 0.00069836, Val R²: 0.267075
Epoch 79/200
Train Loss: 0.00000747, Train R²: 0.993500, Val Loss: 0.00069698, Val R²: 0.268524
Epoch 80/200
Train Loss: 0.00000685, Train R²: 0.994041, Val Loss: 0.00069643, Val R²: 0.269103
Epoch 81/200
Train Loss: 0.00000622, Train R²: 0.994583, Val Loss: 0.00069458, Val R²: 0.271045
Epoch 82/200
Train Loss: 0.00000734, Train R²: 0.993610, Val Loss: 0.00069422, Val R²: 0.271425
Epoch 83/200
Train Loss: 0.00000609, Train R²: 0.994700, Val Loss: 0.00069545, Val R²: 0.270126
Epoch 84/200
Train Loss: 0.00000616, Train R²: 0.994639, Val Loss: 0.00069617, Val R²: 0.269377
Epoch 85/200
Train Loss: 0.00000603, Train R²: 0.994753, Val Loss: 0.00069862, Val R²: 0.266800
Epoch 86/200
Train Loss: 0.00000583, Train R²: 0.994928, Val Loss: 0.00069677, Val R²: 0.268750
Epoch 87/200
Train Loss: 0.00000602, Train R²: 0.994759, Val Loss: 0.00069543, Val R²: 0.270148
Epoch 00087: reducing learning rate of group 0 to 3.1250e-05.
Epoch 88/200
Train Loss: 0.00000571, Train R²: 0.995028, Val Loss: 0.00069512, Val R²: 0.270478
Epoch 89/200
Train Loss: 0.00000636, Train R²: 0.994463, Val Loss: 0.00069621, Val R²: 0.269335
Epoch 90/200
Train Loss: 0.00000762, Train R²: 0.993368, Val Loss: 0.00069734, Val R²: 0.268150
Epoch 91/200
Train Loss: 0.00000565, Train R²: 0.995087, Val Loss: 0.00069681, Val R²: 0.268701
Epoch 92/200
Train Loss: 0.00000570, Train R²: 0.995037, Val Loss: 0.00069574, Val R²: 0.269825
Epoch 93/200
Train Loss: 0.00000560, Train R²: 0.995128, Val Loss: 0.00069484, Val R²: 0.270767
Epoch 94/200
Train Loss: 0.00000553, Train R²: 0.995190, Val Loss: 0.00069685, Val R²: 0.268659
Epoch 95/200
Train Loss: 0.00000656, Train R²: 0.994291, Val Loss: 0.00069774, Val R²: 0.267732
Epoch 96/200
Train Loss: 0.00000569, Train R²: 0.995052, Val Loss: 0.00069942, Val R²: 0.265963
Epoch 97/200
Train Loss: 0.00000522, Train R²: 0.995453, Val Loss: 0.00069720, Val R²: 0.268289
Epoch 98/200
Train Loss: 0.00000496, Train R²: 0.995687, Val Loss: 0.00069545, Val R²: 0.270136
Epoch 00098: reducing learning rate of group 0 to 1.5625e-05.
Epoch 99/200
Train Loss: 0.00000543, Train R²: 0.995273, Val Loss: 0.00069524, Val R²: 0.270354
Epoch 100/200
Train Loss: 0.00000545, Train R²: 0.995259, Val Loss: 0.00069588, Val R²: 0.269681
Epoch 101/200
Train Loss: 0.00000565, Train R²: 0.995087, Val Loss: 0.00069708, Val R²: 0.268419
Epoch 102/200
Train Loss: 0.00000503, Train R²: 0.995621, Val Loss: 0.00069794, Val R²: 0.267520
Epoch 103/200
Train Loss: 0.00000538, Train R²: 0.995320, Val Loss: 0.00069719, Val R²: 0.268305
Epoch 104/200
Train Loss: 0.00000475, Train R²: 0.995862, Val Loss: 0.00069861, Val R²: 0.266818
Epoch 105/200
Train Loss: 0.00000552, Train R²: 0.995195, Val Loss: 0.00069818, Val R²: 0.267265
Epoch 106/200
Train Loss: 0.00000488, Train R²: 0.995753, Val Loss: 0.00069669, Val R²: 0.268834
Epoch 107/200
Train Loss: 0.00000536, Train R²: 0.995336, Val Loss: 0.00069612, Val R²: 0.269424
Epoch 108/200
Train Loss: 0.00000538, Train R²: 0.995316, Val Loss: 0.00069496, Val R²: 0.270642
Epoch 109/200
Train Loss: 0.00000506, Train R²: 0.995596, Val Loss: 0.00069522, Val R²: 0.270373
Epoch 00109: reducing learning rate of group 0 to 7.8125e-06.
Epoch 110/200
Train Loss: 0.00000501, Train R²: 0.995641, Val Loss: 0.00069518, Val R²: 0.270410
Epoch 111/200
Train Loss: 0.00000468, Train R²: 0.995929, Val Loss: 0.00069584, Val R²: 0.269718
Epoch 112/200
Train Loss: 0.00000471, Train R²: 0.995905, Val Loss: 0.00069561, Val R²: 0.269963
Epoch 113/200
Train Loss: 0.00000594, Train R²: 0.994834, Val Loss: 0.00069643, Val R²: 0.269101
Epoch 114/200
Train Loss: 0.00000507, Train R²: 0.995590, Val Loss: 0.00069642, Val R²: 0.269110
Epoch 115/200
Train Loss: 0.00000466, Train R²: 0.995941, Val Loss: 0.00069616, Val R²: 0.269390
Epoch 116/200
Train Loss: 0.00000516, Train R²: 0.995509, Val Loss: 0.00069625, Val R²: 0.269293
Epoch 117/200
Train Loss: 0.00000524, Train R²: 0.995441, Val Loss: 0.00069637, Val R²: 0.269169
Epoch 118/200
Train Loss: 0.00000563, Train R²: 0.995097, Val Loss: 0.00069578, Val R²: 0.269781
Epoch 119/200
Train Loss: 0.00000536, Train R²: 0.995334, Val Loss: 0.00069584, Val R²: 0.269721
Epoch 120/200
Train Loss: 0.00000472, Train R²: 0.995895, Val Loss: 0.00069609, Val R²: 0.269461
Epoch 00120: reducing learning rate of group 0 to 3.9063e-06.
Epoch 121/200
Train Loss: 0.00000467, Train R²: 0.995940, Val Loss: 0.00069542, Val R²: 0.270165
Epoch 122/200
Train Loss: 0.00000457, Train R²: 0.996021, Val Loss: 0.00069587, Val R²: 0.269694
Epoch 123/200
Train Loss: 0.00000460, Train R²: 0.995993, Val Loss: 0.00069644, Val R²: 0.269097
Epoch 124/200
Train Loss: 0.00000503, Train R²: 0.995619, Val Loss: 0.00069635, Val R²: 0.269189
Epoch 125/200
Train Loss: 0.00000434, Train R²: 0.996221, Val Loss: 0.00069689, Val R²: 0.268614
Epoch 126/200
Train Loss: 0.00000525, Train R²: 0.995432, Val Loss: 0.00069667, Val R²: 0.268846
Epoch 127/200
Train Loss: 0.00000462, Train R²: 0.995975, Val Loss: 0.00069610, Val R²: 0.269445
Epoch 128/200
Train Loss: 0.00000507, Train R²: 0.995589, Val Loss: 0.00069776, Val R²: 0.267709
Epoch 129/200
Train Loss: 0.00000591, Train R²: 0.994853, Val Loss: 0.00069629, Val R²: 0.269253
Epoch 130/200
Train Loss: 0.00000579, Train R²: 0.994958, Val Loss: 0.00069632, Val R²: 0.269213
Epoch 131/200
Train Loss: 0.00000479, Train R²: 0.995831, Val Loss: 0.00069721, Val R²: 0.268280
Epoch 00131: reducing learning rate of group 0 to 1.9531e-06.
Epoch 132/200
Train Loss: 0.00000477, Train R²: 0.995846, Val Loss: 0.00069678, Val R²: 0.268732
Epoch 133/200
Train Loss: 0.00000468, Train R²: 0.995928, Val Loss: 0.00069703, Val R²: 0.268477
Epoch 134/200
Train Loss: 0.00000468, Train R²: 0.995928, Val Loss: 0.00069674, Val R²: 0.268782
Epoch 135/200
Train Loss: 0.00000610, Train R²: 0.994693, Val Loss: 0.00069602, Val R²: 0.269536
Epoch 136/200
Train Loss: 0.00000461, Train R²: 0.995992, Val Loss: 0.00069565, Val R²: 0.269916
Epoch 137/200
Train Loss: 0.00000427, Train R²: 0.996285, Val Loss: 0.00069637, Val R²: 0.269160
Epoch 138/200
Train Loss: 0.00000580, Train R²: 0.994955, Val Loss: 0.00069651, Val R²: 0.269022
Epoch 139/200
Train Loss: 0.00000492, Train R²: 0.995722, Val Loss: 0.00069610, Val R²: 0.269452
Epoch 140/200
Train Loss: 0.00000506, Train R²: 0.995592, Val Loss: 0.00069631, Val R²: 0.269231
Epoch 141/200
Train Loss: 0.00000492, Train R²: 0.995716, Val Loss: 0.00069633, Val R²: 0.269207
Epoch 142/200
Train Loss: 0.00000458, Train R²: 0.996017, Val Loss: 0.00069633, Val R²: 0.269205
Epoch 00142: reducing learning rate of group 0 to 9.7656e-07.
Epoch 143/200
Train Loss: 0.00000589, Train R²: 0.994874, Val Loss: 0.00069597, Val R²: 0.269586
Epoch 144/200
Train Loss: 0.00000443, Train R²: 0.996145, Val Loss: 0.00069600, Val R²: 0.269558
Epoch 145/200
Train Loss: 0.00000553, Train R²: 0.995183, Val Loss: 0.00069603, Val R²: 0.269525
Epoch 146/200
Train Loss: 0.00000451, Train R²: 0.996077, Val Loss: 0.00069606, Val R²: 0.269485
Epoch 147/200
Train Loss: 0.00000454, Train R²: 0.996047, Val Loss: 0.00069568, Val R²: 0.269886
Epoch 148/200
Train Loss: 0.00000565, Train R²: 0.995083, Val Loss: 0.00069630, Val R²: 0.269237
Epoch 149/200
Train Loss: 0.00000429, Train R²: 0.996264, Val Loss: 0.00069675, Val R²: 0.268762
Epoch 150/200
Train Loss: 0.00000437, Train R²: 0.996194, Val Loss: 0.00069663, Val R²: 0.268895
Epoch 151/200
Train Loss: 0.00000429, Train R²: 0.996265, Val Loss: 0.00069588, Val R²: 0.269683
Epoch 152/200
Train Loss: 0.00000449, Train R²: 0.996090, Val Loss: 0.00069676, Val R²: 0.268754
Epoch 153/200
Train Loss: 0.00000457, Train R²: 0.996025, Val Loss: 0.00069665, Val R²: 0.268866
Epoch 00153: reducing learning rate of group 0 to 4.8828e-07.
Epoch 154/200
Train Loss: 0.00000433, Train R²: 0.996231, Val Loss: 0.00069698, Val R²: 0.268520
Epoch 155/200
Train Loss: 0.00000456, Train R²: 0.996034, Val Loss: 0.00069698, Val R²: 0.268523
Epoch 156/200
Train Loss: 0.00000422, Train R²: 0.996324, Val Loss: 0.00069596, Val R²: 0.269600
Epoch 157/200
Train Loss: 0.00000509, Train R²: 0.995572, Val Loss: 0.00069598, Val R²: 0.269573
Epoch 158/200
Train Loss: 0.00000547, Train R²: 0.995239, Val Loss: 0.00069620, Val R²: 0.269345
Epoch 159/200
Train Loss: 0.00000445, Train R²: 0.996131, Val Loss: 0.00069702, Val R²: 0.268484
Epoch 160/200
Train Loss: 0.00000472, Train R²: 0.995890, Val Loss: 0.00069698, Val R²: 0.268525
Epoch 161/200
Train Loss: 0.00000569, Train R²: 0.995048, Val Loss: 0.00069595, Val R²: 0.269603
Epoch 162/200
Train Loss: 0.00000512, Train R²: 0.995544, Val Loss: 0.00069693, Val R²: 0.268580
Epoch 163/200
Train Loss: 0.00000442, Train R²: 0.996154, Val Loss: 0.00069655, Val R²: 0.268972
Epoch 164/200
Train Loss: 0.00000517, Train R²: 0.995502, Val Loss: 0.00069687, Val R²: 0.268639
Epoch 00164: reducing learning rate of group 0 to 2.4414e-07.
Epoch 165/200
Train Loss: 0.00000554, Train R²: 0.995177, Val Loss: 0.00069743, Val R²: 0.268055
Epoch 166/200
Train Loss: 0.00000443, Train R²: 0.996148, Val Loss: 0.00069686, Val R²: 0.268651
Epoch 167/200
Train Loss: 0.00000569, Train R²: 0.995052, Val Loss: 0.00069663, Val R²: 0.268892
Epoch 168/200
Train Loss: 0.00000542, Train R²: 0.995286, Val Loss: 0.00069664, Val R²: 0.268883
Epoch 169/200
Train Loss: 0.00000591, Train R²: 0.994854, Val Loss: 0.00069689, Val R²: 0.268617
Epoch 170/200
Train Loss: 0.00000543, Train R²: 0.995277, Val Loss: 0.00069682, Val R²: 0.268695
Epoch 171/200
Train Loss: 0.00000511, Train R²: 0.995550, Val Loss: 0.00069598, Val R²: 0.269575
Epoch 172/200
Train Loss: 0.00000492, Train R²: 0.995715, Val Loss: 0.00069602, Val R²: 0.269529
Epoch 173/200
Train Loss: 0.00000455, Train R²: 0.996044, Val Loss: 0.00069655, Val R²: 0.268981
Epoch 174/200
Train Loss: 0.00000541, Train R²: 0.995291, Val Loss: 0.00069657, Val R²: 0.268960
Epoch 175/200
Train Loss: 0.00000469, Train R²: 0.995918, Val Loss: 0.00069678, Val R²: 0.268736
Epoch 00175: reducing learning rate of group 0 to 1.2207e-07.
Epoch 176/200
Train Loss: 0.00000497, Train R²: 0.995679, Val Loss: 0.00069645, Val R²: 0.269083
Epoch 177/200
Train Loss: 0.00000493, Train R²: 0.995711, Val Loss: 0.00069675, Val R²: 0.268770
Epoch 178/200
Train Loss: 0.00000435, Train R²: 0.996210, Val Loss: 0.00069595, Val R²: 0.269605
Epoch 179/200
Train Loss: 0.00000446, Train R²: 0.996119, Val Loss: 0.00069655, Val R²: 0.268974
Epoch 180/200
Train Loss: 0.00000532, Train R²: 0.995370, Val Loss: 0.00069677, Val R²: 0.268749
Epoch 181/200
Train Loss: 0.00000436, Train R²: 0.996206, Val Loss: 0.00069644, Val R²: 0.269096
Epoch 182/200
Train Loss: 0.00000569, Train R²: 0.995048, Val Loss: 0.00069675, Val R²: 0.268761
Epoch 183/200
Train Loss: 0.00000440, Train R²: 0.996170, Val Loss: 0.00069651, Val R²: 0.269021
Epoch 184/200
Train Loss: 0.00000489, Train R²: 0.995741, Val Loss: 0.00069643, Val R²: 0.269107
Epoch 185/200
Train Loss: 0.00000442, Train R²: 0.996154, Val Loss: 0.00069643, Val R²: 0.269103
Epoch 186/200
Train Loss: 0.00000432, Train R²: 0.996236, Val Loss: 0.00069676, Val R²: 0.268752
Epoch 00186: reducing learning rate of group 0 to 6.1035e-08.
Epoch 187/200
Train Loss: 0.00000465, Train R²: 0.995952, Val Loss: 0.00069677, Val R²: 0.268750
Epoch 188/200
Train Loss: 0.00000557, Train R²: 0.995151, Val Loss: 0.00069655, Val R²: 0.268977
Epoch 189/200
Train Loss: 0.00000408, Train R²: 0.996446, Val Loss: 0.00069677, Val R²: 0.268749
Epoch 190/200
Train Loss: 0.00000657, Train R²: 0.994286, Val Loss: 0.00069655, Val R²: 0.268975
Epoch 191/200
Train Loss: 0.00000575, Train R²: 0.994998, Val Loss: 0.00069679, Val R²: 0.268724
Epoch 192/200
Train Loss: 0.00000426, Train R²: 0.996289, Val Loss: 0.00069595, Val R²: 0.269608
Epoch 193/200
Train Loss: 0.00000440, Train R²: 0.996173, Val Loss: 0.00069677, Val R²: 0.268748
Epoch 194/200
Train Loss: 0.00000570, Train R²: 0.995041, Val Loss: 0.00069600, Val R²: 0.269555
Epoch 195/200
Train Loss: 0.00000464, Train R²: 0.995959, Val Loss: 0.00069573, Val R²: 0.269835
Epoch 196/200
Train Loss: 0.00000503, Train R²: 0.995624, Val Loss: 0.00069655, Val R²: 0.268975
Epoch 197/200
Train Loss: 0.00000422, Train R²: 0.996329, Val Loss: 0.00069595, Val R²: 0.269604
Epoch 00197: reducing learning rate of group 0 to 3.0518e-08.
Epoch 198/200
Train Loss: 0.00000468, Train R²: 0.995927, Val Loss: 0.00069677, Val R²: 0.268745
Epoch 199/200
Train Loss: 0.00000576, Train R²: 0.994986, Val Loss: 0.00069595, Val R²: 0.269604
Epoch 200/200
Train Loss: 0.00000428, Train R²: 0.996279, Val Loss: 0.00069595, Val R²: 0.269603
Training Complete. Best Val Loss: 0.0006679613725282252
训练时间: 231.26 秒
