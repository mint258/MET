Using device: cuda
Total samples: 5000, Training: 4000, Validation: 1000
Total trainable parameters: 1477889
Epoch 1/200
Train Loss: 2.93737656, Train R²: -0.298986, Val Loss: 1.98030247, Val R²: 0.112863
Saved best model with validation R2 0.112863 to best_finetuned_model.pth
Epoch 2/200
Train Loss: 1.74414967, Train R²: 0.228691, Val Loss: 1.63159221, Val R²: 0.269078
Saved best model with validation R2 0.269078 to best_finetuned_model.pth
Epoch 3/200
Train Loss: 1.35154271, Train R²: 0.402312, Val Loss: 1.26168479, Val R²: 0.434790
Saved best model with validation R2 0.434790 to best_finetuned_model.pth
Epoch 4/200
Train Loss: 1.00004343, Train R²: 0.557754, Val Loss: 0.94769747, Val R²: 0.575450
Saved best model with validation R2 0.575450 to best_finetuned_model.pth
Epoch 5/200
Train Loss: 0.75589524, Train R²: 0.665723, Val Loss: 0.90900845, Val R²: 0.592782
Saved best model with validation R2 0.592782 to best_finetuned_model.pth
Epoch 6/200
Train Loss: 0.61157361, Train R²: 0.729546, Val Loss: 0.71654635, Val R²: 0.679001
Saved best model with validation R2 0.679001 to best_finetuned_model.pth
Epoch 7/200
Train Loss: 0.42326366, Train R²: 0.812822, Val Loss: 0.64350373, Val R²: 0.711723
Saved best model with validation R2 0.711723 to best_finetuned_model.pth
Epoch 8/200
Train Loss: 0.30626259, Train R²: 0.864563, Val Loss: 0.57706073, Val R²: 0.741488
Saved best model with validation R2 0.741488 to best_finetuned_model.pth
Epoch 9/200
Train Loss: 0.26457146, Train R²: 0.882999, Val Loss: 0.61550834, Val R²: 0.724264
Epoch 10/200
Train Loss: 0.20441389, Train R²: 0.909603, Val Loss: 0.53927777, Val R²: 0.758414
Saved best model with validation R2 0.758414 to best_finetuned_model.pth
Epoch 11/200
Train Loss: 0.13453020, Train R²: 0.940507, Val Loss: 0.49647964, Val R²: 0.777587
Saved best model with validation R2 0.777587 to best_finetuned_model.pth
Epoch 12/200
Train Loss: 0.16585756, Train R²: 0.926653, Val Loss: 0.58433019, Val R²: 0.738231
Epoch 13/200
Train Loss: 0.16622101, Train R²: 0.926493, Val Loss: 0.55888469, Val R²: 0.749631
Epoch 14/200
Train Loss: 0.13905117, Train R²: 0.938508, Val Loss: 0.55094884, Val R²: 0.753186
Epoch 15/200
Train Loss: 0.11728029, Train R²: 0.948136, Val Loss: 0.60556586, Val R²: 0.728718
Epoch 16/200
Train Loss: 0.14321062, Train R²: 0.936668, Val Loss: 0.65182964, Val R²: 0.707993
Epoch 17/200
Train Loss: 0.14589255, Train R²: 0.935482, Val Loss: 0.49021099, Val R²: 0.780395
Saved best model with validation R2 0.780395 to best_finetuned_model.pth
Epoch 18/200
Train Loss: 0.10077801, Train R²: 0.955433, Val Loss: 0.51839695, Val R²: 0.767768
Epoch 19/200
Train Loss: 0.08124612, Train R²: 0.964071, Val Loss: 0.46065455, Val R²: 0.793636
Saved best model with validation R2 0.793636 to best_finetuned_model.pth
Epoch 20/200
Train Loss: 0.07086499, Train R²: 0.968662, Val Loss: 0.49260274, Val R²: 0.779324
Epoch 21/200
Train Loss: 0.04796435, Train R²: 0.978789, Val Loss: 0.45914089, Val R²: 0.794314
Saved best model with validation R2 0.794314 to best_finetuned_model.pth
Epoch 22/200
Train Loss: 0.03261009, Train R²: 0.985579, Val Loss: 0.45183564, Val R²: 0.797586
Saved best model with validation R2 0.797586 to best_finetuned_model.pth
Epoch 23/200
Train Loss: 0.02220375, Train R²: 0.990181, Val Loss: 0.44202943, Val R²: 0.801979
Saved best model with validation R2 0.801979 to best_finetuned_model.pth
Epoch 24/200
Train Loss: 0.01310949, Train R²: 0.994203, Val Loss: 0.43945591, Val R²: 0.803132
Saved best model with validation R2 0.803132 to best_finetuned_model.pth
Epoch 25/200
Train Loss: 0.00841292, Train R²: 0.996280, Val Loss: 0.43350957, Val R²: 0.805796
Saved best model with validation R2 0.805796 to best_finetuned_model.pth
Epoch 26/200
Train Loss: 0.00743523, Train R²: 0.996712, Val Loss: 0.43528849, Val R²: 0.804999
Epoch 27/200
Train Loss: 0.00743967, Train R²: 0.996710, Val Loss: 0.44425926, Val R²: 0.800980
Epoch 28/200
Train Loss: 0.00827191, Train R²: 0.996342, Val Loss: 0.44703157, Val R²: 0.799739
Epoch 29/200
Train Loss: 0.00943978, Train R²: 0.995825, Val Loss: 0.43769013, Val R²: 0.803923
Epoch 30/200
Train Loss: 0.00821059, Train R²: 0.996369, Val Loss: 0.44105124, Val R²: 0.802418
Epoch 31/200
Train Loss: 0.00893053, Train R²: 0.996051, Val Loss: 0.43037431, Val R²: 0.807201
Saved best model with validation R2 0.807201 to best_finetuned_model.pth
Epoch 32/200
Train Loss: 0.00679206, Train R²: 0.996996, Val Loss: 0.43699393, Val R²: 0.804235
Epoch 33/200
Train Loss: 0.00514705, Train R²: 0.997724, Val Loss: 0.42960960, Val R²: 0.807543
Saved best model with validation R2 0.807543 to best_finetuned_model.pth
Epoch 34/200
Train Loss: 0.00509818, Train R²: 0.997745, Val Loss: 0.43492849, Val R²: 0.805160
Epoch 35/200
Train Loss: 0.00406497, Train R²: 0.998202, Val Loss: 0.43121573, Val R²: 0.806824
Epoch 36/200
Train Loss: 0.00309853, Train R²: 0.998630, Val Loss: 0.42894047, Val R²: 0.807843
Saved best model with validation R2 0.807843 to best_finetuned_model.pth
Epoch 37/200
Train Loss: 0.00327356, Train R²: 0.998552, Val Loss: 0.42886746, Val R²: 0.807876
Saved best model with validation R2 0.807876 to best_finetuned_model.pth
Epoch 38/200
Train Loss: 0.00418865, Train R²: 0.998148, Val Loss: 0.42785626, Val R²: 0.808329
Saved best model with validation R2 0.808329 to best_finetuned_model.pth
Epoch 39/200
Train Loss: 0.00719934, Train R²: 0.996816, Val Loss: 0.43802705, Val R²: 0.803772
Epoch 40/200
Train Loss: 0.00672872, Train R²: 0.997024, Val Loss: 0.43037021, Val R²: 0.807203
Epoch 41/200
Train Loss: 0.00691053, Train R²: 0.996944, Val Loss: 0.42431791, Val R²: 0.809914
Saved best model with validation R2 0.809914 to best_finetuned_model.pth
Epoch 42/200
Train Loss: 0.00943796, Train R²: 0.995826, Val Loss: 0.43182872, Val R²: 0.806549
Epoch 43/200
Train Loss: 0.00899732, Train R²: 0.996021, Val Loss: 0.42771699, Val R²: 0.808391
Epoch 44/200
Train Loss: 0.00787419, Train R²: 0.996518, Val Loss: 0.42935242, Val R²: 0.807658
Epoch 45/200
Train Loss: 0.00881281, Train R²: 0.996103, Val Loss: 0.41860990, Val R²: 0.812471
Saved best model with validation R2 0.812471 to best_finetuned_model.pth
Epoch 46/200
Train Loss: 0.00848199, Train R²: 0.996249, Val Loss: 0.43319334, Val R²: 0.805938
Epoch 47/200
Train Loss: 0.00819181, Train R²: 0.996377, Val Loss: 0.43181809, Val R²: 0.806554
Epoch 48/200
Train Loss: 0.01197364, Train R²: 0.994705, Val Loss: 0.43263075, Val R²: 0.806190
Epoch 49/200
Train Loss: 0.01348444, Train R²: 0.994037, Val Loss: 0.42101141, Val R²: 0.811395
Epoch 50/200
Train Loss: 0.01238862, Train R²: 0.994521, Val Loss: 0.43615932, Val R²: 0.804609
Epoch 51/200
Train Loss: 0.01530675, Train R²: 0.993231, Val Loss: 0.43357921, Val R²: 0.805765
Epoch 52/200
Train Loss: 0.01613280, Train R²: 0.992866, Val Loss: 0.43219641, Val R²: 0.806384
Epoch 53/200
Train Loss: 0.01245937, Train R²: 0.994490, Val Loss: 0.42896082, Val R²: 0.807834
Epoch 54/200
Train Loss: 0.00942187, Train R²: 0.995833, Val Loss: 0.43318835, Val R²: 0.805940
Epoch 55/200
Train Loss: 0.00878363, Train R²: 0.996116, Val Loss: 0.42430925, Val R²: 0.809918
Epoch 56/200
Train Loss: 0.00798511, Train R²: 0.996469, Val Loss: 0.42737714, Val R²: 0.808543
Epoch 00056: reducing learning rate of group 0 to 5.0000e-04.
Epoch 57/200
Train Loss: 0.00503537, Train R²: 0.997773, Val Loss: 0.42298767, Val R²: 0.810510
Epoch 58/200
Train Loss: 0.00218882, Train R²: 0.999032, Val Loss: 0.42343366, Val R²: 0.810310
Epoch 59/200
Train Loss: 0.00099260, Train R²: 0.999561, Val Loss: 0.42280140, Val R²: 0.810593
Epoch 60/200
Train Loss: 0.00049389, Train R²: 0.999782, Val Loss: 0.42326026, Val R²: 0.810388
Epoch 61/200
Train Loss: 0.00026880, Train R²: 0.999881, Val Loss: 0.42272045, Val R²: 0.810629
Epoch 62/200
Train Loss: 0.00020295, Train R²: 0.999910, Val Loss: 0.42316533, Val R²: 0.810430
Epoch 63/200
Train Loss: 0.00013741, Train R²: 0.999939, Val Loss: 0.42249348, Val R²: 0.810731
Epoch 64/200
Train Loss: 0.00009578, Train R²: 0.999958, Val Loss: 0.42251169, Val R²: 0.810723
Epoch 65/200
Train Loss: 0.00008884, Train R²: 0.999961, Val Loss: 0.42325340, Val R²: 0.810391
Epoch 66/200
Train Loss: 0.00007610, Train R²: 0.999966, Val Loss: 0.42258631, Val R²: 0.810690
Epoch 67/200
Train Loss: 0.00007163, Train R²: 0.999968, Val Loss: 0.42318987, Val R²: 0.810419
Epoch 00067: reducing learning rate of group 0 to 2.5000e-04.
Epoch 68/200
Train Loss: 0.00005100, Train R²: 0.999977, Val Loss: 0.42274792, Val R²: 0.810617
Epoch 69/200
Train Loss: 0.00005976, Train R²: 0.999974, Val Loss: 0.42311813, Val R²: 0.810451
Epoch 70/200
Train Loss: 0.00004708, Train R²: 0.999979, Val Loss: 0.42281723, Val R²: 0.810586
Epoch 71/200
Train Loss: 0.00003513, Train R²: 0.999984, Val Loss: 0.42275122, Val R²: 0.810616
Epoch 72/200
Train Loss: 0.00003919, Train R²: 0.999983, Val Loss: 0.42317823, Val R²: 0.810424
Epoch 73/200
Train Loss: 0.00005806, Train R²: 0.999974, Val Loss: 0.42323657, Val R²: 0.810398
Epoch 74/200
Train Loss: 0.00006841, Train R²: 0.999970, Val Loss: 0.42314111, Val R²: 0.810441
Epoch 75/200
Train Loss: 0.00005045, Train R²: 0.999978, Val Loss: 0.42296412, Val R²: 0.810520
Epoch 76/200
Train Loss: 0.00004818, Train R²: 0.999979, Val Loss: 0.42295412, Val R²: 0.810525
Epoch 77/200
Train Loss: 0.00004171, Train R²: 0.999982, Val Loss: 0.42282526, Val R²: 0.810583
Epoch 78/200
Train Loss: 0.00004460, Train R²: 0.999980, Val Loss: 0.42296973, Val R²: 0.810518
Epoch 00078: reducing learning rate of group 0 to 1.2500e-04.
Epoch 79/200
Train Loss: 0.00002095, Train R²: 0.999991, Val Loss: 0.42266570, Val R²: 0.810654
Epoch 80/200
Train Loss: 0.00002274, Train R²: 0.999990, Val Loss: 0.42293934, Val R²: 0.810531
Epoch 81/200
Train Loss: 0.00003506, Train R²: 0.999984, Val Loss: 0.42309858, Val R²: 0.810460
Epoch 82/200
Train Loss: 0.00002263, Train R²: 0.999990, Val Loss: 0.42285156, Val R²: 0.810571
Epoch 83/200
Train Loss: 0.00002107, Train R²: 0.999991, Val Loss: 0.42299414, Val R²: 0.810507
Epoch 84/200
Train Loss: 0.00003235, Train R²: 0.999986, Val Loss: 0.42286121, Val R²: 0.810566
Epoch 85/200
Train Loss: 0.00003447, Train R²: 0.999985, Val Loss: 0.42282608, Val R²: 0.810582
Epoch 86/200
Train Loss: 0.00004015, Train R²: 0.999982, Val Loss: 0.42319554, Val R²: 0.810417
Epoch 87/200
Train Loss: 0.00003428, Train R²: 0.999985, Val Loss: 0.42296018, Val R²: 0.810522
Epoch 88/200
Train Loss: 0.00003801, Train R²: 0.999983, Val Loss: 0.42298959, Val R²: 0.810509
Epoch 89/200
Train Loss: 0.00004495, Train R²: 0.999980, Val Loss: 0.42297797, Val R²: 0.810514
Epoch 00089: reducing learning rate of group 0 to 6.2500e-05.
Epoch 90/200
Train Loss: 0.00003959, Train R²: 0.999982, Val Loss: 0.42299509, Val R²: 0.810506
Epoch 91/200
Train Loss: 0.00002190, Train R²: 0.999990, Val Loss: 0.42294595, Val R²: 0.810528
Epoch 92/200
Train Loss: 0.00003875, Train R²: 0.999983, Val Loss: 0.42286820, Val R²: 0.810563
Epoch 93/200
Train Loss: 0.00003133, Train R²: 0.999986, Val Loss: 0.42305753, Val R²: 0.810478
Epoch 94/200
Train Loss: 0.00003064, Train R²: 0.999986, Val Loss: 0.42305422, Val R²: 0.810480
Epoch 95/200
Train Loss: 0.00002763, Train R²: 0.999988, Val Loss: 0.42298288, Val R²: 0.810512
Epoch 96/200
Train Loss: 0.00002461, Train R²: 0.999989, Val Loss: 0.42292378, Val R²: 0.810538
Epoch 97/200
Train Loss: 0.00001765, Train R²: 0.999992, Val Loss: 0.42312478, Val R²: 0.810448
Epoch 98/200
Train Loss: 0.00002033, Train R²: 0.999991, Val Loss: 0.42282660, Val R²: 0.810582
Epoch 99/200
Train Loss: 0.00002355, Train R²: 0.999990, Val Loss: 0.42264019, Val R²: 0.810665
Epoch 100/200
Train Loss: 0.00002495, Train R²: 0.999989, Val Loss: 0.42258461, Val R²: 0.810690
Epoch 00100: reducing learning rate of group 0 to 3.1250e-05.
Epoch 101/200
Train Loss: 0.00003727, Train R²: 0.999984, Val Loss: 0.42289384, Val R²: 0.810552
Epoch 102/200
Train Loss: 0.00001932, Train R²: 0.999991, Val Loss: 0.42261344, Val R²: 0.810677
Epoch 103/200
Train Loss: 0.00002122, Train R²: 0.999991, Val Loss: 0.42276246, Val R²: 0.810611
Epoch 104/200
Train Loss: 0.00004398, Train R²: 0.999981, Val Loss: 0.42287121, Val R²: 0.810562
Epoch 105/200
Train Loss: 0.00004215, Train R²: 0.999981, Val Loss: 0.42307588, Val R²: 0.810470
Epoch 106/200
Train Loss: 0.00001552, Train R²: 0.999993, Val Loss: 0.42280123, Val R²: 0.810593
Epoch 107/200
Train Loss: 0.00002652, Train R²: 0.999988, Val Loss: 0.42295294, Val R²: 0.810525
Epoch 108/200
Train Loss: 0.00001818, Train R²: 0.999992, Val Loss: 0.42291880, Val R²: 0.810541
Epoch 109/200
Train Loss: 0.00003548, Train R²: 0.999984, Val Loss: 0.42295191, Val R²: 0.810526
Epoch 110/200
Train Loss: 0.00003829, Train R²: 0.999983, Val Loss: 0.42309030, Val R²: 0.810464
Epoch 111/200
Train Loss: 0.00004703, Train R²: 0.999979, Val Loss: 0.42296470, Val R²: 0.810520
Epoch 00111: reducing learning rate of group 0 to 1.5625e-05.
Epoch 112/200
Train Loss: 0.00002578, Train R²: 0.999989, Val Loss: 0.42281440, Val R²: 0.810587
Epoch 113/200
Train Loss: 0.00002199, Train R²: 0.999990, Val Loss: 0.42271561, Val R²: 0.810632
Epoch 114/200
Train Loss: 0.00001902, Train R²: 0.999992, Val Loss: 0.42296595, Val R²: 0.810519
Epoch 115/200
Train Loss: 0.00001682, Train R²: 0.999993, Val Loss: 0.42286923, Val R²: 0.810563
Epoch 116/200
Train Loss: 0.00002513, Train R²: 0.999989, Val Loss: 0.42301166, Val R²: 0.810499
Epoch 117/200
Train Loss: 0.00001752, Train R²: 0.999992, Val Loss: 0.42317506, Val R²: 0.810426
Epoch 118/200
Train Loss: 0.00002304, Train R²: 0.999990, Val Loss: 0.42284489, Val R²: 0.810574
Epoch 119/200
Train Loss: 0.00003080, Train R²: 0.999986, Val Loss: 0.42301247, Val R²: 0.810499
Epoch 120/200
Train Loss: 0.00002573, Train R²: 0.999989, Val Loss: 0.42320498, Val R²: 0.810412
Epoch 121/200
Train Loss: 0.00001699, Train R²: 0.999992, Val Loss: 0.42273061, Val R²: 0.810625
Epoch 122/200
Train Loss: 0.00002147, Train R²: 0.999991, Val Loss: 0.42266556, Val R²: 0.810654
Epoch 00122: reducing learning rate of group 0 to 7.8125e-06.
Epoch 123/200
Train Loss: 0.00002585, Train R²: 0.999989, Val Loss: 0.42286470, Val R²: 0.810565
Epoch 124/200
Train Loss: 0.00001869, Train R²: 0.999992, Val Loss: 0.42284996, Val R²: 0.810571
Epoch 125/200
Train Loss: 0.00002574, Train R²: 0.999989, Val Loss: 0.42292832, Val R²: 0.810536
Epoch 126/200
Train Loss: 0.00002309, Train R²: 0.999990, Val Loss: 0.42282903, Val R²: 0.810581
Epoch 127/200
Train Loss: 0.00002975, Train R²: 0.999987, Val Loss: 0.42272492, Val R²: 0.810627
Epoch 128/200
Train Loss: 0.00003285, Train R²: 0.999985, Val Loss: 0.42288572, Val R²: 0.810555
Epoch 129/200
Train Loss: 0.00004109, Train R²: 0.999982, Val Loss: 0.42267453, Val R²: 0.810650
Epoch 130/200
Train Loss: 0.00003347, Train R²: 0.999985, Val Loss: 0.42280141, Val R²: 0.810593
Epoch 131/200
Train Loss: 0.00002293, Train R²: 0.999990, Val Loss: 0.42309439, Val R²: 0.810462
Epoch 132/200
Train Loss: 0.00001952, Train R²: 0.999991, Val Loss: 0.42277755, Val R²: 0.810604
Epoch 133/200
Train Loss: 0.00001966, Train R²: 0.999991, Val Loss: 0.42307130, Val R²: 0.810472
Epoch 00133: reducing learning rate of group 0 to 3.9063e-06.
Epoch 134/200
Train Loss: 0.00001750, Train R²: 0.999992, Val Loss: 0.42303135, Val R²: 0.810490
Epoch 135/200
Train Loss: 0.00001668, Train R²: 0.999993, Val Loss: 0.42285005, Val R²: 0.810571
Epoch 136/200
Train Loss: 0.00003407, Train R²: 0.999985, Val Loss: 0.42300152, Val R²: 0.810504
Epoch 137/200
Train Loss: 0.00002155, Train R²: 0.999990, Val Loss: 0.42300105, Val R²: 0.810504
Epoch 138/200
Train Loss: 0.00001485, Train R²: 0.999993, Val Loss: 0.42280483, Val R²: 0.810592
Epoch 139/200
Train Loss: 0.00002484, Train R²: 0.999989, Val Loss: 0.42296994, Val R²: 0.810518
Epoch 140/200
Train Loss: 0.00002935, Train R²: 0.999987, Val Loss: 0.42280221, Val R²: 0.810593
Epoch 141/200
Train Loss: 0.00001702, Train R²: 0.999992, Val Loss: 0.42288890, Val R²: 0.810554
Epoch 142/200
Train Loss: 0.00001974, Train R²: 0.999991, Val Loss: 0.42264976, Val R²: 0.810661
Epoch 143/200
Train Loss: 0.00003266, Train R²: 0.999986, Val Loss: 0.42278410, Val R²: 0.810601
Epoch 144/200
Train Loss: 0.00001993, Train R²: 0.999991, Val Loss: 0.42321704, Val R²: 0.810407
Epoch 00144: reducing learning rate of group 0 to 1.9531e-06.
Epoch 145/200
Train Loss: 0.00002547, Train R²: 0.999989, Val Loss: 0.42268726, Val R²: 0.810644
Epoch 146/200
Train Loss: 0.00002667, Train R²: 0.999988, Val Loss: 0.42281104, Val R²: 0.810589
Epoch 147/200
Train Loss: 0.00001929, Train R²: 0.999991, Val Loss: 0.42279783, Val R²: 0.810595
Epoch 148/200
Train Loss: 0.00001785, Train R²: 0.999992, Val Loss: 0.42283980, Val R²: 0.810576
Epoch 149/200
Train Loss: 0.00002104, Train R²: 0.999991, Val Loss: 0.42294722, Val R²: 0.810528
Epoch 150/200
Train Loss: 0.00001770, Train R²: 0.999992, Val Loss: 0.42308075, Val R²: 0.810468
Epoch 151/200
Train Loss: 0.00002467, Train R²: 0.999989, Val Loss: 0.42284746, Val R²: 0.810573
Epoch 152/200
Train Loss: 0.00002901, Train R²: 0.999987, Val Loss: 0.42281802, Val R²: 0.810586
Epoch 153/200
Train Loss: 0.00003317, Train R²: 0.999985, Val Loss: 0.42300862, Val R²: 0.810500
Epoch 154/200
Train Loss: 0.00002078, Train R²: 0.999991, Val Loss: 0.42293042, Val R²: 0.810535
Epoch 155/200
Train Loss: 0.00003276, Train R²: 0.999986, Val Loss: 0.42331916, Val R²: 0.810361
Epoch 00155: reducing learning rate of group 0 to 9.7656e-07.
Epoch 156/200
Train Loss: 0.00002678, Train R²: 0.999988, Val Loss: 0.42266550, Val R²: 0.810654
Epoch 157/200
Train Loss: 0.00002073, Train R²: 0.999991, Val Loss: 0.42270336, Val R²: 0.810637
Epoch 158/200
Train Loss: 0.00001415, Train R²: 0.999994, Val Loss: 0.42305946, Val R²: 0.810478
Epoch 159/200
Train Loss: 0.00001939, Train R²: 0.999991, Val Loss: 0.42280421, Val R²: 0.810592
Epoch 160/200
Train Loss: 0.00002791, Train R²: 0.999988, Val Loss: 0.42281556, Val R²: 0.810587
Epoch 161/200
Train Loss: 0.00001933, Train R²: 0.999991, Val Loss: 0.42269793, Val R²: 0.810640
Epoch 162/200
Train Loss: 0.00002813, Train R²: 0.999988, Val Loss: 0.42294110, Val R²: 0.810531
Epoch 163/200
Train Loss: 0.00001558, Train R²: 0.999993, Val Loss: 0.42283798, Val R²: 0.810577
Epoch 164/200
Train Loss: 0.00001302, Train R²: 0.999994, Val Loss: 0.42272762, Val R²: 0.810626
Epoch 165/200
Train Loss: 0.00002858, Train R²: 0.999987, Val Loss: 0.42287140, Val R²: 0.810562
Epoch 166/200
Train Loss: 0.00002696, Train R²: 0.999988, Val Loss: 0.42302588, Val R²: 0.810493
Epoch 00166: reducing learning rate of group 0 to 4.8828e-07.
Epoch 167/200
Train Loss: 0.00002136, Train R²: 0.999991, Val Loss: 0.42301713, Val R²: 0.810497
Epoch 168/200
Train Loss: 0.00001810, Train R²: 0.999992, Val Loss: 0.42288891, Val R²: 0.810554
Epoch 169/200
Train Loss: 0.00003505, Train R²: 0.999984, Val Loss: 0.42282231, Val R²: 0.810584
Epoch 170/200
Train Loss: 0.00001996, Train R²: 0.999991, Val Loss: 0.42285083, Val R²: 0.810571
Epoch 171/200
Train Loss: 0.00002077, Train R²: 0.999991, Val Loss: 0.42281320, Val R²: 0.810588
Epoch 172/200
Train Loss: 0.00002245, Train R²: 0.999990, Val Loss: 0.42272927, Val R²: 0.810626
Epoch 173/200
Train Loss: 0.00003618, Train R²: 0.999984, Val Loss: 0.42267464, Val R²: 0.810650
Epoch 174/200
Train Loss: 0.00002982, Train R²: 0.999987, Val Loss: 0.42301770, Val R²: 0.810496
Epoch 175/200
Train Loss: 0.00002286, Train R²: 0.999990, Val Loss: 0.42282997, Val R²: 0.810580
Epoch 176/200
Train Loss: 0.00003377, Train R²: 0.999985, Val Loss: 0.42262829, Val R²: 0.810671
Epoch 177/200
Train Loss: 0.00002142, Train R²: 0.999991, Val Loss: 0.42270142, Val R²: 0.810638
Epoch 00177: reducing learning rate of group 0 to 2.4414e-07.
Epoch 178/200
Train Loss: 0.00003156, Train R²: 0.999986, Val Loss: 0.42270141, Val R²: 0.810638
Epoch 179/200
Train Loss: 0.00003484, Train R²: 0.999985, Val Loss: 0.42270103, Val R²: 0.810638
Epoch 180/200
Train Loss: 0.00002273, Train R²: 0.999990, Val Loss: 0.42266711, Val R²: 0.810653
Epoch 181/200
Train Loss: 0.00002910, Train R²: 0.999987, Val Loss: 0.42262878, Val R²: 0.810671
Epoch 182/200
Train Loss: 0.00001734, Train R²: 0.999992, Val Loss: 0.42270083, Val R²: 0.810638
Epoch 183/200
Train Loss: 0.00001771, Train R²: 0.999992, Val Loss: 0.42294572, Val R²: 0.810529
Epoch 184/200
Train Loss: 0.00002278, Train R²: 0.999990, Val Loss: 0.42283143, Val R²: 0.810580
Epoch 185/200
Train Loss: 0.00001881, Train R²: 0.999992, Val Loss: 0.42292386, Val R²: 0.810538
Epoch 186/200
Train Loss: 0.00001472, Train R²: 0.999993, Val Loss: 0.42283138, Val R²: 0.810580
Epoch 187/200
Train Loss: 0.00001601, Train R²: 0.999993, Val Loss: 0.42283140, Val R²: 0.810580
Epoch 188/200
Train Loss: 0.00002080, Train R²: 0.999991, Val Loss: 0.42281526, Val R²: 0.810587
Epoch 00188: reducing learning rate of group 0 to 1.2207e-07.
Epoch 189/200
Train Loss: 0.00002278, Train R²: 0.999990, Val Loss: 0.42270969, Val R²: 0.810634
Epoch 190/200
Train Loss: 0.00002050, Train R²: 0.999991, Val Loss: 0.42317212, Val R²: 0.810427
Epoch 191/200
Train Loss: 0.00001897, Train R²: 0.999992, Val Loss: 0.42283510, Val R²: 0.810578
Epoch 192/200
Train Loss: 0.00002902, Train R²: 0.999987, Val Loss: 0.42320055, Val R²: 0.810414
Epoch 193/200
Train Loss: 0.00002253, Train R²: 0.999990, Val Loss: 0.42270069, Val R²: 0.810638
Epoch 194/200
Train Loss: 0.00002179, Train R²: 0.999990, Val Loss: 0.42266220, Val R²: 0.810656
Epoch 195/200
Train Loss: 0.00001503, Train R²: 0.999993, Val Loss: 0.42269737, Val R²: 0.810640
Epoch 196/200
Train Loss: 0.00001537, Train R²: 0.999993, Val Loss: 0.42283133, Val R²: 0.810580
Epoch 197/200
Train Loss: 0.00001505, Train R²: 0.999993, Val Loss: 0.42317193, Val R²: 0.810427
Epoch 198/200
Train Loss: 0.00001291, Train R²: 0.999994, Val Loss: 0.42305773, Val R²: 0.810478
Epoch 199/200
Train Loss: 0.00002242, Train R²: 0.999990, Val Loss: 0.42270092, Val R²: 0.810638
Epoch 00199: reducing learning rate of group 0 to 6.1035e-08.
Epoch 200/200
Train Loss: 0.00002210, Train R²: 0.999990, Val Loss: 0.42292716, Val R²: 0.810537
Training Complete. Best Val Loss: 0.4186098961830139
训练时间: 941.74 秒
