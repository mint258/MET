Using device: cuda
Total samples: 5000, Training: 4000, Validation: 1000
Total trainable parameters: 1477889
Epoch 1/200
Train Loss: 3.38885803, Train R²: -0.499094, Val Loss: 2.10066076, Val R²: 0.060335
Saved best model with validation R2 0.060335 to best_finetuned_model.pth
Epoch 2/200
Train Loss: 1.90804258, Train R²: 0.155959, Val Loss: 1.62363423, Val R²: 0.273718
Saved best model with validation R2 0.273718 to best_finetuned_model.pth
Epoch 3/200
Train Loss: 1.37865111, Train R²: 0.390140, Val Loss: 1.25003946, Val R²: 0.440834
Saved best model with validation R2 0.440834 to best_finetuned_model.pth
Epoch 4/200
Train Loss: 1.00564744, Train R²: 0.555142, Val Loss: 1.18210406, Val R²: 0.471223
Saved best model with validation R2 0.471223 to best_finetuned_model.pth
Epoch 5/200
Train Loss: 0.82711073, Train R²: 0.634120, Val Loss: 0.92136147, Val R²: 0.587858
Saved best model with validation R2 0.587858 to best_finetuned_model.pth
Epoch 6/200
Train Loss: 0.59710833, Train R²: 0.735863, Val Loss: 0.70095494, Val R²: 0.686450
Saved best model with validation R2 0.686450 to best_finetuned_model.pth
Epoch 7/200
Train Loss: 0.39893198, Train R²: 0.823529, Val Loss: 0.66105667, Val R²: 0.704297
Saved best model with validation R2 0.704297 to best_finetuned_model.pth
Epoch 8/200
Train Loss: 0.32022172, Train R²: 0.858347, Val Loss: 0.78663597, Val R²: 0.648123
Epoch 9/200
Train Loss: 0.29163408, Train R²: 0.870993, Val Loss: 0.59609423, Val R²: 0.733356
Saved best model with validation R2 0.733356 to best_finetuned_model.pth
Epoch 10/200
Train Loss: 0.19216050, Train R²: 0.914996, Val Loss: 0.56647454, Val R²: 0.746605
Saved best model with validation R2 0.746605 to best_finetuned_model.pth
Epoch 11/200
Train Loss: 0.19334267, Train R²: 0.914473, Val Loss: 0.62239368, Val R²: 0.721592
Epoch 12/200
Train Loss: 0.19200853, Train R²: 0.915063, Val Loss: 0.62144782, Val R²: 0.722015
Epoch 13/200
Train Loss: 0.15737771, Train R²: 0.930382, Val Loss: 0.56596515, Val R²: 0.746833
Saved best model with validation R2 0.746833 to best_finetuned_model.pth
Epoch 14/200
Train Loss: 0.11685126, Train R²: 0.948310, Val Loss: 0.52230789, Val R²: 0.766362
Saved best model with validation R2 0.766362 to best_finetuned_model.pth
Epoch 15/200
Train Loss: 0.08520852, Train R²: 0.962307, Val Loss: 0.50576845, Val R²: 0.773760
Saved best model with validation R2 0.773760 to best_finetuned_model.pth
Epoch 16/200
Train Loss: 0.05733981, Train R²: 0.974635, Val Loss: 0.49107029, Val R²: 0.780335
Saved best model with validation R2 0.780335 to best_finetuned_model.pth
Epoch 17/200
Train Loss: 0.03555813, Train R²: 0.984271, Val Loss: 0.48687208, Val R²: 0.782213
Saved best model with validation R2 0.782213 to best_finetuned_model.pth
Epoch 18/200
Train Loss: 0.02542410, Train R²: 0.988753, Val Loss: 0.47686497, Val R²: 0.786689
Saved best model with validation R2 0.786689 to best_finetuned_model.pth
Epoch 19/200
Train Loss: 0.02175035, Train R²: 0.990379, Val Loss: 0.48227257, Val R²: 0.784270
Epoch 20/200
Train Loss: 0.01893687, Train R²: 0.991623, Val Loss: 0.49422939, Val R²: 0.778922
Epoch 21/200
Train Loss: 0.01920511, Train R²: 0.991504, Val Loss: 0.47967218, Val R²: 0.785434
Epoch 22/200
Train Loss: 0.01581174, Train R²: 0.993006, Val Loss: 0.49271116, Val R²: 0.779601
Epoch 23/200
Train Loss: 0.01505731, Train R²: 0.993339, Val Loss: 0.46832079, Val R²: 0.790511
Saved best model with validation R2 0.790511 to best_finetuned_model.pth
Epoch 24/200
Train Loss: 0.02110976, Train R²: 0.990662, Val Loss: 0.47771290, Val R²: 0.786310
Epoch 25/200
Train Loss: 0.02038960, Train R²: 0.990980, Val Loss: 0.48131440, Val R²: 0.784699
Epoch 26/200
Train Loss: 0.01563361, Train R²: 0.993084, Val Loss: 0.49481371, Val R²: 0.778660
Epoch 27/200
Train Loss: 0.01623288, Train R²: 0.992819, Val Loss: 0.47938260, Val R²: 0.785563
Epoch 28/200
Train Loss: 0.01496634, Train R²: 0.993379, Val Loss: 0.46869182, Val R²: 0.790345
Epoch 29/200
Train Loss: 0.01226658, Train R²: 0.994574, Val Loss: 0.47400141, Val R²: 0.787970
Epoch 30/200
Train Loss: 0.01008798, Train R²: 0.995537, Val Loss: 0.47983417, Val R²: 0.785361
Epoch 31/200
Train Loss: 0.00911341, Train R²: 0.995969, Val Loss: 0.47367061, Val R²: 0.788118
Epoch 32/200
Train Loss: 0.00783488, Train R²: 0.996534, Val Loss: 0.46243451, Val R²: 0.793144
Saved best model with validation R2 0.793144 to best_finetuned_model.pth
Epoch 33/200
Train Loss: 0.00839354, Train R²: 0.996287, Val Loss: 0.46872241, Val R²: 0.790332
Epoch 34/200
Train Loss: 0.00901949, Train R²: 0.996010, Val Loss: 0.47788646, Val R²: 0.786232
Epoch 35/200
Train Loss: 0.00775203, Train R²: 0.996571, Val Loss: 0.46475768, Val R²: 0.792105
Epoch 36/200
Train Loss: 0.00753004, Train R²: 0.996669, Val Loss: 0.47238114, Val R²: 0.788695
Epoch 37/200
Train Loss: 0.00784536, Train R²: 0.996530, Val Loss: 0.46877649, Val R²: 0.790307
Epoch 38/200
Train Loss: 0.01469647, Train R²: 0.993499, Val Loss: 0.51598649, Val R²: 0.769190
Epoch 39/200
Train Loss: 0.02206069, Train R²: 0.990241, Val Loss: 0.47669149, Val R²: 0.786767
Epoch 40/200
Train Loss: 0.02154050, Train R²: 0.990471, Val Loss: 0.48341835, Val R²: 0.783758
Epoch 41/200
Train Loss: 0.02095335, Train R²: 0.990731, Val Loss: 0.47024544, Val R²: 0.789650
Epoch 42/200
Train Loss: 0.01814125, Train R²: 0.991975, Val Loss: 0.47697379, Val R²: 0.786641
Epoch 43/200
Train Loss: 0.01628863, Train R²: 0.992795, Val Loss: 0.46649677, Val R²: 0.791327
Epoch 00043: reducing learning rate of group 0 to 5.0000e-04.
Epoch 44/200
Train Loss: 0.01131748, Train R²: 0.994994, Val Loss: 0.46825368, Val R²: 0.790541
Epoch 45/200
Train Loss: 0.00486873, Train R²: 0.997846, Val Loss: 0.45975774, Val R²: 0.794342
Saved best model with validation R2 0.794342 to best_finetuned_model.pth
Epoch 46/200
Train Loss: 0.00227709, Train R²: 0.998993, Val Loss: 0.45982589, Val R²: 0.794311
Epoch 47/200
Train Loss: 0.00123042, Train R²: 0.999456, Val Loss: 0.46485359, Val R²: 0.792062
Epoch 48/200
Train Loss: 0.00068996, Train R²: 0.999695, Val Loss: 0.46223283, Val R²: 0.793235
Epoch 49/200
Train Loss: 0.00037613, Train R²: 0.999834, Val Loss: 0.46174190, Val R²: 0.793454
Epoch 50/200
Train Loss: 0.00025596, Train R²: 0.999887, Val Loss: 0.46153516, Val R²: 0.793547
Epoch 51/200
Train Loss: 0.00018148, Train R²: 0.999920, Val Loss: 0.46156253, Val R²: 0.793534
Epoch 52/200
Train Loss: 0.00018736, Train R²: 0.999917, Val Loss: 0.46131229, Val R²: 0.793646
Epoch 53/200
Train Loss: 0.00011392, Train R²: 0.999950, Val Loss: 0.46212897, Val R²: 0.793281
Epoch 54/200
Train Loss: 0.00012453, Train R²: 0.999945, Val Loss: 0.46148708, Val R²: 0.793568
Epoch 55/200
Train Loss: 0.00013426, Train R²: 0.999941, Val Loss: 0.46091080, Val R²: 0.793826
Epoch 56/200
Train Loss: 0.00009838, Train R²: 0.999956, Val Loss: 0.46283181, Val R²: 0.792967
Epoch 00056: reducing learning rate of group 0 to 2.5000e-04.
Epoch 57/200
Train Loss: 0.00010753, Train R²: 0.999952, Val Loss: 0.46142818, Val R²: 0.793594
Epoch 58/200
Train Loss: 0.00007657, Train R²: 0.999966, Val Loss: 0.46221619, Val R²: 0.793242
Epoch 59/200
Train Loss: 0.00006132, Train R²: 0.999973, Val Loss: 0.46167169, Val R²: 0.793486
Epoch 60/200
Train Loss: 0.00006436, Train R²: 0.999972, Val Loss: 0.46188743, Val R²: 0.793389
Epoch 61/200
Train Loss: 0.00004596, Train R²: 0.999980, Val Loss: 0.46189049, Val R²: 0.793388
Epoch 62/200
Train Loss: 0.00003656, Train R²: 0.999984, Val Loss: 0.46203935, Val R²: 0.793321
Epoch 63/200
Train Loss: 0.00007829, Train R²: 0.999965, Val Loss: 0.46213792, Val R²: 0.793277
Epoch 64/200
Train Loss: 0.00009979, Train R²: 0.999956, Val Loss: 0.46262062, Val R²: 0.793061
Epoch 65/200
Train Loss: 0.00005282, Train R²: 0.999977, Val Loss: 0.46213017, Val R²: 0.793280
Epoch 66/200
Train Loss: 0.00004768, Train R²: 0.999979, Val Loss: 0.46163203, Val R²: 0.793503
Epoch 67/200
Train Loss: 0.00004611, Train R²: 0.999980, Val Loss: 0.46229195, Val R²: 0.793208
Epoch 00067: reducing learning rate of group 0 to 1.2500e-04.
Epoch 68/200
Train Loss: 0.00003509, Train R²: 0.999984, Val Loss: 0.46218604, Val R²: 0.793255
Epoch 69/200
Train Loss: 0.00004221, Train R²: 0.999981, Val Loss: 0.46200660, Val R²: 0.793336
Epoch 70/200
Train Loss: 0.00002432, Train R²: 0.999989, Val Loss: 0.46189945, Val R²: 0.793384
Epoch 71/200
Train Loss: 0.00002916, Train R²: 0.999987, Val Loss: 0.46224913, Val R²: 0.793227
Epoch 72/200
Train Loss: 0.00002781, Train R²: 0.999988, Val Loss: 0.46201831, Val R²: 0.793331
Epoch 73/200
Train Loss: 0.00003922, Train R²: 0.999983, Val Loss: 0.46170013, Val R²: 0.793473
Epoch 74/200
Train Loss: 0.00004819, Train R²: 0.999979, Val Loss: 0.46170904, Val R²: 0.793469
Epoch 75/200
Train Loss: 0.00006352, Train R²: 0.999972, Val Loss: 0.46238299, Val R²: 0.793167
Epoch 76/200
Train Loss: 0.00005010, Train R²: 0.999978, Val Loss: 0.46216557, Val R²: 0.793265
Epoch 77/200
Train Loss: 0.00004531, Train R²: 0.999980, Val Loss: 0.46197152, Val R²: 0.793351
Epoch 78/200
Train Loss: 0.00002562, Train R²: 0.999989, Val Loss: 0.46187486, Val R²: 0.793395
Epoch 00078: reducing learning rate of group 0 to 6.2500e-05.
Epoch 79/200
Train Loss: 0.00003723, Train R²: 0.999984, Val Loss: 0.46207905, Val R²: 0.793303
Epoch 80/200
Train Loss: 0.00004301, Train R²: 0.999981, Val Loss: 0.46190364, Val R²: 0.793382
Epoch 81/200
Train Loss: 0.00001897, Train R²: 0.999992, Val Loss: 0.46175156, Val R²: 0.793450
Epoch 82/200
Train Loss: 0.00002992, Train R²: 0.999987, Val Loss: 0.46175543, Val R²: 0.793448
Epoch 83/200
Train Loss: 0.00005295, Train R²: 0.999977, Val Loss: 0.46184992, Val R²: 0.793406
Epoch 84/200
Train Loss: 0.00006944, Train R²: 0.999969, Val Loss: 0.46204191, Val R²: 0.793320
Epoch 85/200
Train Loss: 0.00003551, Train R²: 0.999984, Val Loss: 0.46187034, Val R²: 0.793397
Epoch 86/200
Train Loss: 0.00002700, Train R²: 0.999988, Val Loss: 0.46230800, Val R²: 0.793201
Epoch 87/200
Train Loss: 0.00003940, Train R²: 0.999983, Val Loss: 0.46211188, Val R²: 0.793289
Epoch 88/200
Train Loss: 0.00003002, Train R²: 0.999987, Val Loss: 0.46199505, Val R²: 0.793341
Epoch 89/200
Train Loss: 0.00002420, Train R²: 0.999989, Val Loss: 0.46193349, Val R²: 0.793368
Epoch 00089: reducing learning rate of group 0 to 3.1250e-05.
Epoch 90/200
Train Loss: 0.00001804, Train R²: 0.999992, Val Loss: 0.46185410, Val R²: 0.793404
Epoch 91/200
Train Loss: 0.00006438, Train R²: 0.999972, Val Loss: 0.46201113, Val R²: 0.793334
Epoch 92/200
Train Loss: 0.00003721, Train R²: 0.999984, Val Loss: 0.46208693, Val R²: 0.793300
Epoch 93/200
Train Loss: 0.00003378, Train R²: 0.999985, Val Loss: 0.46205144, Val R²: 0.793316
Epoch 94/200
Train Loss: 0.00002095, Train R²: 0.999991, Val Loss: 0.46208277, Val R²: 0.793302
Epoch 95/200
Train Loss: 0.00003082, Train R²: 0.999986, Val Loss: 0.46222541, Val R²: 0.793238
Epoch 96/200
Train Loss: 0.00003644, Train R²: 0.999984, Val Loss: 0.46178265, Val R²: 0.793436
Epoch 97/200
Train Loss: 0.00001822, Train R²: 0.999992, Val Loss: 0.46209523, Val R²: 0.793296
Epoch 98/200
Train Loss: 0.00003132, Train R²: 0.999986, Val Loss: 0.46200946, Val R²: 0.793334
Epoch 99/200
Train Loss: 0.00002071, Train R²: 0.999991, Val Loss: 0.46186987, Val R²: 0.793397
Epoch 100/200
Train Loss: 0.00003047, Train R²: 0.999987, Val Loss: 0.46217739, Val R²: 0.793259
Epoch 00100: reducing learning rate of group 0 to 1.5625e-05.
Epoch 101/200
Train Loss: 0.00004106, Train R²: 0.999982, Val Loss: 0.46196820, Val R²: 0.793353
Epoch 102/200
Train Loss: 0.00002657, Train R²: 0.999988, Val Loss: 0.46200593, Val R²: 0.793336
Epoch 103/200
Train Loss: 0.00002022, Train R²: 0.999991, Val Loss: 0.46198516, Val R²: 0.793345
Epoch 104/200
Train Loss: 0.00002202, Train R²: 0.999990, Val Loss: 0.46188937, Val R²: 0.793388
Epoch 105/200
Train Loss: 0.00003411, Train R²: 0.999985, Val Loss: 0.46185729, Val R²: 0.793403
Epoch 106/200
Train Loss: 0.00002687, Train R²: 0.999988, Val Loss: 0.46180192, Val R²: 0.793427
Epoch 107/200
Train Loss: 0.00003308, Train R²: 0.999985, Val Loss: 0.46198075, Val R²: 0.793347
Epoch 108/200
Train Loss: 0.00003739, Train R²: 0.999983, Val Loss: 0.46212735, Val R²: 0.793282
Epoch 109/200
Train Loss: 0.00004104, Train R²: 0.999982, Val Loss: 0.46190495, Val R²: 0.793381
Epoch 110/200
Train Loss: 0.00002265, Train R²: 0.999990, Val Loss: 0.46208960, Val R²: 0.793299
Epoch 111/200
Train Loss: 0.00002896, Train R²: 0.999987, Val Loss: 0.46205821, Val R²: 0.793313
Epoch 00111: reducing learning rate of group 0 to 7.8125e-06.
Epoch 112/200
Train Loss: 0.00003056, Train R²: 0.999986, Val Loss: 0.46182472, Val R²: 0.793417
Epoch 113/200
Train Loss: 0.00003458, Train R²: 0.999985, Val Loss: 0.46195149, Val R²: 0.793360
Epoch 114/200
Train Loss: 0.00002760, Train R²: 0.999988, Val Loss: 0.46207087, Val R²: 0.793307
Epoch 115/200
Train Loss: 0.00002691, Train R²: 0.999988, Val Loss: 0.46189513, Val R²: 0.793386
Epoch 116/200
Train Loss: 0.00001979, Train R²: 0.999991, Val Loss: 0.46227908, Val R²: 0.793214
Epoch 117/200
Train Loss: 0.00004727, Train R²: 0.999979, Val Loss: 0.46193436, Val R²: 0.793368
Epoch 118/200
Train Loss: 0.00002033, Train R²: 0.999991, Val Loss: 0.46214678, Val R²: 0.793273
Epoch 119/200
Train Loss: 0.00003055, Train R²: 0.999986, Val Loss: 0.46188434, Val R²: 0.793390
Epoch 120/200
Train Loss: 0.00003337, Train R²: 0.999985, Val Loss: 0.46205502, Val R²: 0.793314
Epoch 121/200
Train Loss: 0.00001878, Train R²: 0.999992, Val Loss: 0.46225897, Val R²: 0.793223
Epoch 122/200
Train Loss: 0.00003241, Train R²: 0.999986, Val Loss: 0.46185970, Val R²: 0.793401
Epoch 00122: reducing learning rate of group 0 to 3.9063e-06.
Epoch 123/200
Train Loss: 0.00001543, Train R²: 0.999993, Val Loss: 0.46207890, Val R²: 0.793303
Epoch 124/200
Train Loss: 0.00002390, Train R²: 0.999989, Val Loss: 0.46184208, Val R²: 0.793409
Epoch 125/200
Train Loss: 0.00003943, Train R²: 0.999983, Val Loss: 0.46195297, Val R²: 0.793360
Epoch 126/200
Train Loss: 0.00003386, Train R²: 0.999985, Val Loss: 0.46195108, Val R²: 0.793361
Epoch 127/200
Train Loss: 0.00002293, Train R²: 0.999990, Val Loss: 0.46192461, Val R²: 0.793372
Epoch 128/200
Train Loss: 0.00003110, Train R²: 0.999986, Val Loss: 0.46186407, Val R²: 0.793399
Epoch 129/200
Train Loss: 0.00003350, Train R²: 0.999985, Val Loss: 0.46195478, Val R²: 0.793359
Epoch 130/200
Train Loss: 0.00002979, Train R²: 0.999987, Val Loss: 0.46200821, Val R²: 0.793335
Epoch 131/200
Train Loss: 0.00002929, Train R²: 0.999987, Val Loss: 0.46182267, Val R²: 0.793418
Epoch 132/200
Train Loss: 0.00002470, Train R²: 0.999989, Val Loss: 0.46227756, Val R²: 0.793215
Epoch 133/200
Train Loss: 0.00002342, Train R²: 0.999990, Val Loss: 0.46212306, Val R²: 0.793284
Epoch 00133: reducing learning rate of group 0 to 1.9531e-06.
Epoch 134/200
Train Loss: 0.00002669, Train R²: 0.999988, Val Loss: 0.46181686, Val R²: 0.793421
Epoch 135/200
Train Loss: 0.00002032, Train R²: 0.999991, Val Loss: 0.46196710, Val R²: 0.793353
Epoch 136/200
Train Loss: 0.00002547, Train R²: 0.999989, Val Loss: 0.46200410, Val R²: 0.793337
Epoch 137/200
Train Loss: 0.00002706, Train R²: 0.999988, Val Loss: 0.46188931, Val R²: 0.793388
Epoch 138/200
Train Loss: 0.00001757, Train R²: 0.999992, Val Loss: 0.46188272, Val R²: 0.793391
Epoch 139/200
Train Loss: 0.00005133, Train R²: 0.999977, Val Loss: 0.46192664, Val R²: 0.793372
Epoch 140/200
Train Loss: 0.00002184, Train R²: 0.999990, Val Loss: 0.46228484, Val R²: 0.793211
Epoch 141/200
Train Loss: 0.00003546, Train R²: 0.999984, Val Loss: 0.46192157, Val R²: 0.793374
Epoch 142/200
Train Loss: 0.00002839, Train R²: 0.999987, Val Loss: 0.46186495, Val R²: 0.793399
Epoch 143/200
Train Loss: 0.00001864, Train R²: 0.999992, Val Loss: 0.46223759, Val R²: 0.793232
Epoch 144/200
Train Loss: 0.00001559, Train R²: 0.999993, Val Loss: 0.46219608, Val R²: 0.793251
Epoch 00144: reducing learning rate of group 0 to 9.7656e-07.
Epoch 145/200
Train Loss: 0.00003350, Train R²: 0.999985, Val Loss: 0.46211377, Val R²: 0.793288
Epoch 146/200
Train Loss: 0.00002149, Train R²: 0.999990, Val Loss: 0.46216104, Val R²: 0.793267
Epoch 147/200
Train Loss: 0.00001931, Train R²: 0.999991, Val Loss: 0.46198417, Val R²: 0.793346
Epoch 148/200
Train Loss: 0.00002361, Train R²: 0.999990, Val Loss: 0.46203879, Val R²: 0.793321
Epoch 149/200
Train Loss: 0.00001920, Train R²: 0.999992, Val Loss: 0.46198585, Val R²: 0.793345
Epoch 150/200
Train Loss: 0.00003454, Train R²: 0.999985, Val Loss: 0.46193806, Val R²: 0.793366
Epoch 151/200
Train Loss: 0.00001807, Train R²: 0.999992, Val Loss: 0.46201027, Val R²: 0.793334
Epoch 152/200
Train Loss: 0.00004720, Train R²: 0.999979, Val Loss: 0.46224893, Val R²: 0.793227
Epoch 153/200
Train Loss: 0.00004344, Train R²: 0.999981, Val Loss: 0.46208541, Val R²: 0.793300
Epoch 154/200
Train Loss: 0.00002645, Train R²: 0.999988, Val Loss: 0.46209513, Val R²: 0.793296
Epoch 155/200
Train Loss: 0.00004816, Train R²: 0.999979, Val Loss: 0.46213384, Val R²: 0.793279
Epoch 00155: reducing learning rate of group 0 to 4.8828e-07.
Epoch 156/200
Train Loss: 0.00002276, Train R²: 0.999990, Val Loss: 0.46194836, Val R²: 0.793362
Epoch 157/200
Train Loss: 0.00006144, Train R²: 0.999973, Val Loss: 0.46210898, Val R²: 0.793290
Epoch 158/200
Train Loss: 0.00003253, Train R²: 0.999986, Val Loss: 0.46216048, Val R²: 0.793267
Epoch 159/200
Train Loss: 0.00002535, Train R²: 0.999989, Val Loss: 0.46189935, Val R²: 0.793384
Epoch 160/200
Train Loss: 0.00002091, Train R²: 0.999991, Val Loss: 0.46214831, Val R²: 0.793272
Epoch 161/200
Train Loss: 0.00003066, Train R²: 0.999986, Val Loss: 0.46186397, Val R²: 0.793400
Epoch 162/200
Train Loss: 0.00002517, Train R²: 0.999989, Val Loss: 0.46203844, Val R²: 0.793322
Epoch 163/200
Train Loss: 0.00002312, Train R²: 0.999990, Val Loss: 0.46209574, Val R²: 0.793296
Epoch 164/200
Train Loss: 0.00002748, Train R²: 0.999988, Val Loss: 0.46199670, Val R²: 0.793340
Epoch 165/200
Train Loss: 0.00002430, Train R²: 0.999989, Val Loss: 0.46221767, Val R²: 0.793241
Epoch 166/200
Train Loss: 0.00003135, Train R²: 0.999986, Val Loss: 0.46215903, Val R²: 0.793268
Epoch 00166: reducing learning rate of group 0 to 2.4414e-07.
Epoch 167/200
Train Loss: 0.00002053, Train R²: 0.999991, Val Loss: 0.46181338, Val R²: 0.793422
Epoch 168/200
Train Loss: 0.00003138, Train R²: 0.999986, Val Loss: 0.46189911, Val R²: 0.793384
Epoch 169/200
Train Loss: 0.00002271, Train R²: 0.999990, Val Loss: 0.46193764, Val R²: 0.793367
Epoch 170/200
Train Loss: 0.00001398, Train R²: 0.999994, Val Loss: 0.46203080, Val R²: 0.793325
Epoch 171/200
Train Loss: 0.00003110, Train R²: 0.999986, Val Loss: 0.46201482, Val R²: 0.793332
Epoch 172/200
Train Loss: 0.00001920, Train R²: 0.999992, Val Loss: 0.46211769, Val R²: 0.793286
Epoch 173/200
Train Loss: 0.00001673, Train R²: 0.999993, Val Loss: 0.46220827, Val R²: 0.793246
Epoch 174/200
Train Loss: 0.00004920, Train R²: 0.999978, Val Loss: 0.46220744, Val R²: 0.793246
Epoch 175/200
Train Loss: 0.00001824, Train R²: 0.999992, Val Loss: 0.46195035, Val R²: 0.793361
Epoch 176/200
Train Loss: 0.00002349, Train R²: 0.999990, Val Loss: 0.46226774, Val R²: 0.793219
Epoch 177/200
Train Loss: 0.00002989, Train R²: 0.999987, Val Loss: 0.46213302, Val R²: 0.793279
Epoch 00177: reducing learning rate of group 0 to 1.2207e-07.
Epoch 178/200
Train Loss: 0.00002367, Train R²: 0.999990, Val Loss: 0.46209329, Val R²: 0.793297
Epoch 179/200
Train Loss: 0.00001904, Train R²: 0.999992, Val Loss: 0.46213352, Val R²: 0.793279
Epoch 180/200
Train Loss: 0.00003202, Train R²: 0.999986, Val Loss: 0.46231306, Val R²: 0.793199
Epoch 181/200
Train Loss: 0.00002845, Train R²: 0.999987, Val Loss: 0.46220840, Val R²: 0.793245
Epoch 182/200
Train Loss: 0.00002698, Train R²: 0.999988, Val Loss: 0.46194907, Val R²: 0.793361
Epoch 183/200
Train Loss: 0.00003113, Train R²: 0.999986, Val Loss: 0.46213220, Val R²: 0.793280
Epoch 184/200
Train Loss: 0.00004475, Train R²: 0.999980, Val Loss: 0.46202986, Val R²: 0.793325
Epoch 185/200
Train Loss: 0.00002335, Train R²: 0.999990, Val Loss: 0.46181083, Val R²: 0.793423
Epoch 186/200
Train Loss: 0.00003153, Train R²: 0.999986, Val Loss: 0.46200608, Val R²: 0.793336
Epoch 187/200
Train Loss: 0.00002035, Train R²: 0.999991, Val Loss: 0.46195309, Val R²: 0.793360
Epoch 188/200
