Using device: cuda
Total samples: 1000, Training: 800, Validation: 200
Total trainable parameters: 4577537
Epoch 1/200
Train Loss: 6.64164814, Train R²: -2.136306, Val Loss: 5.56464958, Val R²: -1.710558
Saved best model with validation R2 -1.710558 to best_finetuned_model.pth
Epoch 2/200
Train Loss: 4.61485449, Train R²: -1.179217, Val Loss: 3.68968678, Val R²: -0.797258
Saved best model with validation R2 -0.797258 to best_finetuned_model.pth
Epoch 3/200
Train Loss: 2.91060194, Train R²: -0.374438, Val Loss: 2.34126496, Val R²: -0.140437
Saved best model with validation R2 -0.140437 to best_finetuned_model.pth
Epoch 4/200
Train Loss: 2.04856417, Train R²: 0.032631, Val Loss: 2.14783072, Val R²: -0.046215
Saved best model with validation R2 -0.046215 to best_finetuned_model.pth
Epoch 5/200
Train Loss: 2.12856936, Train R²: -0.005149, Val Loss: 2.14247942, Val R²: -0.043609
Saved best model with validation R2 -0.043609 to best_finetuned_model.pth
Epoch 6/200
Train Loss: 1.99614228, Train R²: 0.057386, Val Loss: 1.93789148, Val R²: 0.056047
Saved best model with validation R2 0.056047 to best_finetuned_model.pth
Epoch 7/200
Train Loss: 1.78027596, Train R²: 0.159322, Val Loss: 1.95267928, Val R²: 0.048844
Epoch 8/200
Train Loss: 1.75225254, Train R²: 0.172555, Val Loss: 1.92399657, Val R²: 0.062815
Saved best model with validation R2 0.062815 to best_finetuned_model.pth
Epoch 9/200
Train Loss: 1.61620650, Train R²: 0.236798, Val Loss: 1.83342469, Val R²: 0.106933
Saved best model with validation R2 0.106933 to best_finetuned_model.pth
Epoch 10/200
Train Loss: 1.49313742, Train R²: 0.294914, Val Loss: 1.79024184, Val R²: 0.127968
Saved best model with validation R2 0.127968 to best_finetuned_model.pth
Epoch 11/200
Train Loss: 1.36361549, Train R²: 0.356076, Val Loss: 1.75807822, Val R²: 0.143634
Saved best model with validation R2 0.143634 to best_finetuned_model.pth
Epoch 12/200
Train Loss: 1.26609492, Train R²: 0.402127, Val Loss: 1.72114956, Val R²: 0.161623
Saved best model with validation R2 0.161623 to best_finetuned_model.pth
Epoch 13/200
Train Loss: 1.14812833, Train R²: 0.457833, Val Loss: 1.66131651, Val R²: 0.190767
Saved best model with validation R2 0.190767 to best_finetuned_model.pth
Epoch 14/200
Train Loss: 0.99714791, Train R²: 0.529129, Val Loss: 1.63232696, Val R²: 0.204888
Saved best model with validation R2 0.204888 to best_finetuned_model.pth
Epoch 15/200
Train Loss: 0.88605111, Train R²: 0.581591, Val Loss: 1.61172247, Val R²: 0.214925
Saved best model with validation R2 0.214925 to best_finetuned_model.pth
Epoch 16/200
Train Loss: 0.75967169, Train R²: 0.641269, Val Loss: 1.55495644, Val R²: 0.242576
Saved best model with validation R2 0.242576 to best_finetuned_model.pth
Epoch 17/200
Train Loss: 0.61368792, Train R²: 0.710206, Val Loss: 1.48614037, Val R²: 0.276096
Saved best model with validation R2 0.276096 to best_finetuned_model.pth
Epoch 18/200
Train Loss: 0.48928879, Train R²: 0.768949, Val Loss: 1.42312467, Val R²: 0.306791
Saved best model with validation R2 0.306791 to best_finetuned_model.pth
Epoch 19/200
Train Loss: 0.39837191, Train R²: 0.811882, Val Loss: 1.40776086, Val R²: 0.314275
Saved best model with validation R2 0.314275 to best_finetuned_model.pth
Epoch 20/200
Train Loss: 0.30573136, Train R²: 0.855628, Val Loss: 1.39535153, Val R²: 0.320320
Saved best model with validation R2 0.320320 to best_finetuned_model.pth
Epoch 21/200
Train Loss: 0.24359969, Train R²: 0.884968, Val Loss: 1.39738810, Val R²: 0.319328
Epoch 22/200
Train Loss: 0.23671159, Train R²: 0.888221, Val Loss: 1.39755952, Val R²: 0.319244
Epoch 23/200
Train Loss: 0.23493288, Train R²: 0.889060, Val Loss: 1.38635063, Val R²: 0.324704
Saved best model with validation R2 0.324704 to best_finetuned_model.pth
Epoch 24/200
Train Loss: 0.20041078, Train R²: 0.905362, Val Loss: 1.38531029, Val R²: 0.325211
Saved best model with validation R2 0.325211 to best_finetuned_model.pth
Epoch 25/200
Train Loss: 0.16022338, Train R²: 0.924340, Val Loss: 1.36624539, Val R²: 0.334498
Saved best model with validation R2 0.334498 to best_finetuned_model.pth
Epoch 26/200
Train Loss: 0.14256472, Train R²: 0.932678, Val Loss: 1.49062562, Val R²: 0.273911
Epoch 27/200
Train Loss: 0.18957442, Train R²: 0.910480, Val Loss: 1.52553129, Val R²: 0.256909
Epoch 28/200
Train Loss: 0.21763950, Train R²: 0.897227, Val Loss: 1.53807235, Val R²: 0.250800
Epoch 29/200
Train Loss: 0.19315025, Train R²: 0.908791, Val Loss: 1.63699389, Val R²: 0.202615
Epoch 30/200
Train Loss: 0.17798379, Train R²: 0.915953, Val Loss: 1.52500045, Val R²: 0.257167
Epoch 31/200
Train Loss: 0.22342425, Train R²: 0.894495, Val Loss: 1.50828743, Val R²: 0.265308
Epoch 32/200
Train Loss: 0.23904179, Train R²: 0.887120, Val Loss: 1.60691679, Val R²: 0.217266
Epoch 33/200
Train Loss: 0.20674990, Train R²: 0.902369, Val Loss: 1.51946592, Val R²: 0.259863
Epoch 34/200
Train Loss: 0.16442775, Train R²: 0.922354, Val Loss: 1.49288845, Val R²: 0.272809
Epoch 35/200
Train Loss: 0.13620994, Train R²: 0.935679, Val Loss: 1.53293741, Val R²: 0.253301
Epoch 36/200
Train Loss: 0.12203278, Train R²: 0.942374, Val Loss: 1.50235868, Val R²: 0.268196
Epoch 00036: reducing learning rate of group 0 to 5.0000e-05.
Epoch 37/200
Train Loss: 0.11103897, Train R²: 0.947565, Val Loss: 1.51081169, Val R²: 0.264079
Epoch 38/200
Train Loss: 0.09786667, Train R²: 0.953786, Val Loss: 1.47854066, Val R²: 0.279798
Epoch 39/200
Train Loss: 0.07900388, Train R²: 0.962693, Val Loss: 1.49652493, Val R²: 0.271038
Epoch 40/200
Train Loss: 0.07027522, Train R²: 0.966815, Val Loss: 1.46620953, Val R²: 0.285805
Epoch 41/200
Train Loss: 0.05835414, Train R²: 0.972444, Val Loss: 1.49977016, Val R²: 0.269457
Epoch 42/200
Train Loss: 0.05064592, Train R²: 0.976084, Val Loss: 1.48636079, Val R²: 0.275989
Epoch 43/200
Train Loss: 0.04349075, Train R²: 0.979463, Val Loss: 1.51726913, Val R²: 0.260933
Epoch 44/200
Train Loss: 0.03928021, Train R²: 0.981451, Val Loss: 1.50269711, Val R²: 0.268031
Epoch 45/200
Train Loss: 0.03446769, Train R²: 0.983724, Val Loss: 1.51379514, Val R²: 0.262626
Epoch 46/200
Train Loss: 0.03104040, Train R²: 0.985342, Val Loss: 1.49563050, Val R²: 0.271474
Epoch 47/200
Train Loss: 0.02908330, Train R²: 0.986266, Val Loss: 1.50702393, Val R²: 0.265924
Epoch 00047: reducing learning rate of group 0 to 2.5000e-05.
Epoch 48/200
Train Loss: 0.02751831, Train R²: 0.987005, Val Loss: 1.48773825, Val R²: 0.275318
Epoch 49/200
Train Loss: 0.02531894, Train R²: 0.988044, Val Loss: 1.50097620, Val R²: 0.268870
Epoch 50/200
Train Loss: 0.02433464, Train R²: 0.988509, Val Loss: 1.49954057, Val R²: 0.269569
Epoch 51/200
Train Loss: 0.02355546, Train R²: 0.988877, Val Loss: 1.49329615, Val R²: 0.272611
Epoch 52/200
Train Loss: 0.02274559, Train R²: 0.989259, Val Loss: 1.50710845, Val R²: 0.265883
Epoch 53/200
Train Loss: 0.02229282, Train R²: 0.989473, Val Loss: 1.51519787, Val R²: 0.261942
Epoch 54/200
Train Loss: 0.02164533, Train R²: 0.989779, Val Loss: 1.51227653, Val R²: 0.263365
Epoch 55/200
Train Loss: 0.02062256, Train R²: 0.990262, Val Loss: 1.51603186, Val R²: 0.261536
Epoch 56/200
Train Loss: 0.02057006, Train R²: 0.990286, Val Loss: 1.51734662, Val R²: 0.260896
Epoch 57/200
Train Loss: 0.02040887, Train R²: 0.990363, Val Loss: 1.50349617, Val R²: 0.267642
Epoch 58/200
Train Loss: 0.01912484, Train R²: 0.990969, Val Loss: 1.51961470, Val R²: 0.259791
Epoch 00058: reducing learning rate of group 0 to 1.2500e-05.
Epoch 59/200
Train Loss: 0.01940568, Train R²: 0.990836, Val Loss: 1.52008605, Val R²: 0.259561
Epoch 60/200
Train Loss: 0.01830848, Train R²: 0.991354, Val Loss: 1.50484860, Val R²: 0.266983
Epoch 61/200
Train Loss: 0.01835003, Train R²: 0.991335, Val Loss: 1.51939225, Val R²: 0.259899
Epoch 62/200
Train Loss: 0.01850956, Train R²: 0.991259, Val Loss: 1.52051663, Val R²: 0.259351
Epoch 63/200
Train Loss: 0.01839542, Train R²: 0.991313, Val Loss: 1.52057612, Val R²: 0.259323
Epoch 64/200
Train Loss: 0.01854388, Train R²: 0.991243, Val Loss: 1.50709593, Val R²: 0.265889
Epoch 65/200
Train Loss: 0.01838844, Train R²: 0.991317, Val Loss: 1.52126038, Val R²: 0.258989
Epoch 66/200
Train Loss: 0.01743494, Train R²: 0.991767, Val Loss: 1.52172601, Val R²: 0.258762
Epoch 67/200
Train Loss: 0.01786015, Train R²: 0.991566, Val Loss: 1.51019633, Val R²: 0.264379
Epoch 68/200
Train Loss: 0.01697894, Train R²: 0.991982, Val Loss: 1.52240479, Val R²: 0.258432
Epoch 69/200
Train Loss: 0.01713468, Train R²: 0.991909, Val Loss: 1.52027702, Val R²: 0.259468
Epoch 00069: reducing learning rate of group 0 to 6.2500e-06.
Epoch 70/200
Train Loss: 0.01630591, Train R²: 0.992300, Val Loss: 1.51035523, Val R²: 0.264301
Epoch 71/200
Train Loss: 0.01652692, Train R²: 0.992196, Val Loss: 1.52457392, Val R²: 0.257375
Epoch 72/200
Train Loss: 0.01604884, Train R²: 0.992421, Val Loss: 1.51236677, Val R²: 0.263321
Epoch 73/200
Train Loss: 0.01580378, Train R²: 0.992537, Val Loss: 1.52740836, Val R²: 0.255995
Epoch 74/200
Train Loss: 0.01528174, Train R²: 0.992784, Val Loss: 1.51417875, Val R²: 0.262439
Epoch 75/200
Train Loss: 0.01527213, Train R²: 0.992788, Val Loss: 1.52690458, Val R²: 0.256240
Epoch 76/200
Train Loss: 0.01488653, Train R²: 0.992970, Val Loss: 1.52907884, Val R²: 0.255181
Epoch 77/200
Train Loss: 0.01526124, Train R²: 0.992793, Val Loss: 1.51679134, Val R²: 0.261166
Epoch 78/200
Train Loss: 0.01509015, Train R²: 0.992874, Val Loss: 1.52806723, Val R²: 0.255673
Epoch 79/200
Train Loss: 0.01499658, Train R²: 0.992918, Val Loss: 1.51581573, Val R²: 0.261641
Epoch 80/200
Train Loss: 0.01462825, Train R²: 0.993092, Val Loss: 1.52856255, Val R²: 0.255432
Epoch 00080: reducing learning rate of group 0 to 3.1250e-06.
Epoch 81/200
Train Loss: 0.01494238, Train R²: 0.992944, Val Loss: 1.52953660, Val R²: 0.254958
Epoch 82/200
Train Loss: 0.01459902, Train R²: 0.993106, Val Loss: 1.53062868, Val R²: 0.254426
Epoch 83/200
Train Loss: 0.01511491, Train R²: 0.992862, Val Loss: 1.52928519, Val R²: 0.255080
Epoch 84/200
Train Loss: 0.01470120, Train R²: 0.993058, Val Loss: 1.51649523, Val R²: 0.261310
Epoch 85/200
Train Loss: 0.01510507, Train R²: 0.992867, Val Loss: 1.51551056, Val R²: 0.261790
Epoch 86/200
Train Loss: 0.01468723, Train R²: 0.993064, Val Loss: 1.53048015, Val R²: 0.254498
Epoch 87/200
Train Loss: 0.01463393, Train R²: 0.993090, Val Loss: 1.51657224, Val R²: 0.261273
Epoch 88/200
Train Loss: 0.01431305, Train R²: 0.993241, Val Loss: 1.52999008, Val R²: 0.254737
Epoch 89/200
Train Loss: 0.01436828, Train R²: 0.993215, Val Loss: 1.51699400, Val R²: 0.261067
Epoch 90/200
Train Loss: 0.01429279, Train R²: 0.993251, Val Loss: 1.53109062, Val R²: 0.254201
Epoch 91/200
Train Loss: 0.01414148, Train R²: 0.993322, Val Loss: 1.53000498, Val R²: 0.254730
Epoch 00091: reducing learning rate of group 0 to 1.5625e-06.
Epoch 92/200
Train Loss: 0.01479455, Train R²: 0.993014, Val Loss: 1.53059566, Val R²: 0.254442
Epoch 93/200
Train Loss: 0.01408244, Train R²: 0.993350, Val Loss: 1.53170133, Val R²: 0.253903
Epoch 94/200
Train Loss: 0.01390297, Train R²: 0.993435, Val Loss: 1.53134823, Val R²: 0.254075
Epoch 95/200
Train Loss: 0.01415061, Train R²: 0.993318, Val Loss: 1.53198051, Val R²: 0.253767
Epoch 96/200
Train Loss: 0.01405653, Train R²: 0.993362, Val Loss: 1.53106165, Val R²: 0.254215
Epoch 97/200
Train Loss: 0.01389242, Train R²: 0.993440, Val Loss: 1.53153503, Val R²: 0.253984
Epoch 98/200
Train Loss: 0.01394266, Train R²: 0.993416, Val Loss: 1.51929200, Val R²: 0.259948
Epoch 99/200
Train Loss: 0.01405660, Train R²: 0.993362, Val Loss: 1.53209627, Val R²: 0.253711
Epoch 100/200
Train Loss: 0.01402151, Train R²: 0.993379, Val Loss: 1.53036308, Val R²: 0.254555
Epoch 101/200
Train Loss: 0.01418550, Train R²: 0.993301, Val Loss: 1.53163874, Val R²: 0.253934
Epoch 102/200
Train Loss: 0.01395516, Train R²: 0.993410, Val Loss: 1.53103268, Val R²: 0.254229
Epoch 00102: reducing learning rate of group 0 to 7.8125e-07.
Epoch 103/200
Train Loss: 0.01399036, Train R²: 0.993393, Val Loss: 1.53023970, Val R²: 0.254615
Epoch 104/200
Train Loss: 0.01406968, Train R²: 0.993356, Val Loss: 1.51911330, Val R²: 0.260035
Epoch 105/200
Train Loss: 0.01374318, Train R²: 0.993510, Val Loss: 1.53200305, Val R²: 0.253756
Epoch 106/200
Train Loss: 0.01350967, Train R²: 0.993621, Val Loss: 1.53150666, Val R²: 0.253998
Epoch 107/200
Train Loss: 0.01354878, Train R²: 0.993602, Val Loss: 1.51946056, Val R²: 0.259866
Epoch 108/200
Train Loss: 0.01378853, Train R²: 0.993489, Val Loss: 1.53157544, Val R²: 0.253965
Epoch 109/200
Train Loss: 0.01361162, Train R²: 0.993572, Val Loss: 1.53306854, Val R²: 0.253237
Epoch 110/200
Train Loss: 0.01352315, Train R²: 0.993614, Val Loss: 1.51999712, Val R²: 0.259605
Epoch 111/200
Train Loss: 0.01383177, Train R²: 0.993468, Val Loss: 1.51947069, Val R²: 0.259861
Epoch 112/200
Train Loss: 0.01339809, Train R²: 0.993673, Val Loss: 1.53205192, Val R²: 0.253733
Epoch 113/200
Train Loss: 0.01339440, Train R²: 0.993675, Val Loss: 1.51997733, Val R²: 0.259614
Epoch 00113: reducing learning rate of group 0 to 3.9063e-07.
Epoch 114/200
Train Loss: 0.01358927, Train R²: 0.993583, Val Loss: 1.53165865, Val R²: 0.253924
Epoch 115/200
Train Loss: 0.01343823, Train R²: 0.993654, Val Loss: 1.51998687, Val R²: 0.259610
Epoch 116/200
Train Loss: 0.01338171, Train R²: 0.993681, Val Loss: 1.53157234, Val R²: 0.253966
Epoch 117/200
Train Loss: 0.01424580, Train R²: 0.993273, Val Loss: 1.51909411, Val R²: 0.260044
Epoch 118/200
Train Loss: 0.01362495, Train R²: 0.993566, Val Loss: 1.53163886, Val R²: 0.253934
Epoch 119/200
Train Loss: 0.01351228, Train R²: 0.993619, Val Loss: 1.53265452, Val R²: 0.253439
Epoch 120/200
Train Loss: 0.01374330, Train R²: 0.993510, Val Loss: 1.53165972, Val R²: 0.253924
Epoch 121/200
Train Loss: 0.01381912, Train R²: 0.993474, Val Loss: 1.53161633, Val R²: 0.253945
Epoch 122/200
Train Loss: 0.01342688, Train R²: 0.993660, Val Loss: 1.51857877, Val R²: 0.260295
Epoch 123/200
Train Loss: 0.01329855, Train R²: 0.993720, Val Loss: 1.53183413, Val R²: 0.253839
Epoch 124/200
Train Loss: 0.01361502, Train R²: 0.993571, Val Loss: 1.51864600, Val R²: 0.260263
Epoch 00124: reducing learning rate of group 0 to 1.9531e-07.
Epoch 125/200
Train Loss: 0.01375529, Train R²: 0.993505, Val Loss: 1.53270495, Val R²: 0.253415
Epoch 126/200
Train Loss: 0.01353592, Train R²: 0.993608, Val Loss: 1.51868522, Val R²: 0.260244
Epoch 127/200
Train Loss: 0.01336791, Train R²: 0.993687, Val Loss: 1.51965630, Val R²: 0.259771
Epoch 128/200
Train Loss: 0.01417239, Train R²: 0.993308, Val Loss: 1.53172517, Val R²: 0.253892
Epoch 129/200
Train Loss: 0.01368323, Train R²: 0.993539, Val Loss: 1.51970589, Val R²: 0.259746
Epoch 130/200
Train Loss: 0.01347158, Train R²: 0.993638, Val Loss: 1.53280580, Val R²: 0.253365
Epoch 131/200
Train Loss: 0.01347125, Train R²: 0.993639, Val Loss: 1.51979065, Val R²: 0.259705
Epoch 132/200
Train Loss: 0.01341908, Train R²: 0.993663, Val Loss: 1.53185391, Val R²: 0.253829
Epoch 133/200
Train Loss: 0.01348043, Train R²: 0.993634, Val Loss: 1.51879501, Val R²: 0.260190
Epoch 134/200
Train Loss: 0.01377458, Train R²: 0.993495, Val Loss: 1.51976144, Val R²: 0.259719
Epoch 135/200
Train Loss: 0.01386814, Train R²: 0.993451, Val Loss: 1.53173673, Val R²: 0.253886
Epoch 00135: reducing learning rate of group 0 to 9.7656e-08.
Epoch 136/200
Train Loss: 0.01340883, Train R²: 0.993668, Val Loss: 1.51963270, Val R²: 0.259782
Epoch 137/200
Train Loss: 0.01369346, Train R²: 0.993534, Val Loss: 1.53184175, Val R²: 0.253835
Epoch 138/200
Train Loss: 0.01349185, Train R²: 0.993629, Val Loss: 1.53262937, Val R²: 0.253451
Epoch 139/200
Train Loss: 0.01340593, Train R²: 0.993669, Val Loss: 1.53236055, Val R²: 0.253582
Epoch 140/200
Train Loss: 0.01379030, Train R²: 0.993488, Val Loss: 1.53164077, Val R²: 0.253933
Epoch 141/200
Train Loss: 0.01340541, Train R²: 0.993670, Val Loss: 1.53166103, Val R²: 0.253923
Epoch 142/200
Train Loss: 0.01324097, Train R²: 0.993747, Val Loss: 1.51962888, Val R²: 0.259784
Epoch 143/200
Train Loss: 0.01327714, Train R²: 0.993730, Val Loss: 1.53270829, Val R²: 0.253413
Epoch 144/200
Train Loss: 0.01332969, Train R²: 0.993705, Val Loss: 1.53329098, Val R²: 0.253129
Epoch 145/200
Train Loss: 0.01345512, Train R²: 0.993646, Val Loss: 1.53180504, Val R²: 0.253853
Epoch 146/200
Train Loss: 0.01352713, Train R²: 0.993612, Val Loss: 1.51878810, Val R²: 0.260193
Epoch 00146: reducing learning rate of group 0 to 4.8828e-08.
Epoch 147/200
Train Loss: 0.01323152, Train R²: 0.993752, Val Loss: 1.51880038, Val R²: 0.260188
Epoch 148/200
Train Loss: 0.01427114, Train R²: 0.993261, Val Loss: 1.51980042, Val R²: 0.259700
Epoch 149/200
Train Loss: 0.01323377, Train R²: 0.993751, Val Loss: 1.51981223, Val R²: 0.259695
Epoch 150/200
Train Loss: 0.01388467, Train R²: 0.993443, Val Loss: 1.53340507, Val R²: 0.253074
Epoch 151/200
Train Loss: 0.01342487, Train R²: 0.993661, Val Loss: 1.53287625, Val R²: 0.253331
Epoch 152/200
Train Loss: 0.01362159, Train R²: 0.993568, Val Loss: 1.52037716, Val R²: 0.259419
Epoch 153/200
Train Loss: 0.01351922, Train R²: 0.993616, Val Loss: 1.53208828, Val R²: 0.253715
Epoch 154/200
Train Loss: 0.01327936, Train R²: 0.993729, Val Loss: 1.51886654, Val R²: 0.260155
Epoch 155/200
Train Loss: 0.01342176, Train R²: 0.993662, Val Loss: 1.53290474, Val R²: 0.253317
Epoch 156/200
Train Loss: 0.01371985, Train R²: 0.993521, Val Loss: 1.51941264, Val R²: 0.259889
Epoch 157/200
Train Loss: 0.01376927, Train R²: 0.993498, Val Loss: 1.53193295, Val R²: 0.253791
Epoch 00157: reducing learning rate of group 0 to 2.4414e-08.
Epoch 158/200
Train Loss: 0.01379999, Train R²: 0.993483, Val Loss: 1.52041161, Val R²: 0.259403
Epoch 159/200
Train Loss: 0.01324236, Train R²: 0.993747, Val Loss: 1.53292489, Val R²: 0.253307
Epoch 160/200
Train Loss: 0.01337434, Train R²: 0.993684, Val Loss: 1.53292739, Val R²: 0.253306
Epoch 161/200
Train Loss: 0.01347638, Train R²: 0.993636, Val Loss: 1.52041709, Val R²: 0.259400
Epoch 162/200
Train Loss: 0.01375830, Train R²: 0.993503, Val Loss: 1.53292871, Val R²: 0.253306
Epoch 163/200
Train Loss: 0.01334213, Train R²: 0.993700, Val Loss: 1.53292596, Val R²: 0.253307
Epoch 164/200
Train Loss: 0.01337828, Train R²: 0.993683, Val Loss: 1.53193033, Val R²: 0.253792
Epoch 165/200
Train Loss: 0.01356263, Train R²: 0.993595, Val Loss: 1.53192425, Val R²: 0.253795
Epoch 166/200
Train Loss: 0.01361171, Train R²: 0.993572, Val Loss: 1.53344691, Val R²: 0.253053
Epoch 167/200
Train Loss: 0.01341394, Train R²: 0.993666, Val Loss: 1.53344750, Val R²: 0.253053
Epoch 168/200
Train Loss: 0.01389953, Train R²: 0.993436, Val Loss: 1.51887476, Val R²: 0.260151
Epoch 00168: reducing learning rate of group 0 to 1.2207e-08.
Epoch 169/200
Train Loss: 0.01337612, Train R²: 0.993684, Val Loss: 1.51887572, Val R²: 0.260151
Epoch 170/200
Train Loss: 0.01383821, Train R²: 0.993465, Val Loss: 1.53363037, Val R²: 0.252964
Epoch 171/200
Train Loss: 0.01320375, Train R²: 0.993765, Val Loss: 1.53291225, Val R²: 0.253314
Epoch 172/200
Train Loss: 0.01413097, Train R²: 0.993327, Val Loss: 1.53309321, Val R²: 0.253225
Epoch 173/200
Train Loss: 0.01368631, Train R²: 0.993537, Val Loss: 1.53291476, Val R²: 0.253312
Epoch 174/200
Train Loss: 0.01394420, Train R²: 0.993415, Val Loss: 1.53192854, Val R²: 0.253793
Epoch 175/200
Train Loss: 0.01344800, Train R²: 0.993650, Val Loss: 1.52040553, Val R²: 0.259406
Epoch 176/200
Train Loss: 0.01344544, Train R²: 0.993651, Val Loss: 1.53291810, Val R²: 0.253311
Epoch 177/200
Train Loss: 0.01349695, Train R²: 0.993626, Val Loss: 1.53291833, Val R²: 0.253311
Epoch 178/200
Train Loss: 0.01324722, Train R²: 0.993744, Val Loss: 1.51888216, Val R²: 0.260148
Epoch 179/200
Train Loss: 0.01320068, Train R²: 0.993766, Val Loss: 1.53345644, Val R²: 0.253049
Epoch 180/200
Train Loss: 0.01350426, Train R²: 0.993623, Val Loss: 1.53291857, Val R²: 0.253310
Epoch 181/200
Train Loss: 0.01319179, Train R²: 0.993771, Val Loss: 1.53309953, Val R²: 0.253222
Epoch 182/200
Train Loss: 0.01328956, Train R²: 0.993724, Val Loss: 1.53345764, Val R²: 0.253048
Epoch 183/200
Train Loss: 0.01330438, Train R²: 0.993717, Val Loss: 1.53193295, Val R²: 0.253791
Epoch 184/200
Train Loss: 0.01332944, Train R²: 0.993706, Val Loss: 1.53193343, Val R²: 0.253790
Epoch 185/200
Train Loss: 0.01390304, Train R²: 0.993435, Val Loss: 1.53265119, Val R²: 0.253441
Epoch 186/200
Train Loss: 0.01327654, Train R²: 0.993731, Val Loss: 1.53193367, Val R²: 0.253790
Epoch 187/200
Train Loss: 0.01336352, Train R²: 0.993689, Val Loss: 1.53345978, Val R²: 0.253047
Epoch 188/200
Train Loss: 0.01387688, Train R²: 0.993447, Val Loss: 1.53211606, Val R²: 0.253701
Epoch 189/200
Train Loss: 0.01331928, Train R²: 0.993710, Val Loss: 1.51906872, Val R²: 0.260057
Epoch 190/200
Train Loss: 0.01343094, Train R²: 0.993658, Val Loss: 1.53292429, Val R²: 0.253308
Epoch 191/200
Train Loss: 0.01348127, Train R²: 0.993634, Val Loss: 1.53346324, Val R²: 0.253045
Epoch 192/200
Train Loss: 0.01339291, Train R²: 0.993676, Val Loss: 1.51888990, Val R²: 0.260144
Epoch 193/200
Train Loss: 0.01330423, Train R²: 0.993717, Val Loss: 1.51888943, Val R²: 0.260144
Epoch 194/200
Train Loss: 0.01325375, Train R²: 0.993741, Val Loss: 1.53193879, Val R²: 0.253788
Epoch 195/200
Train Loss: 0.01360608, Train R²: 0.993575, Val Loss: 1.53292632, Val R²: 0.253307
Epoch 196/200
Train Loss: 0.01330673, Train R²: 0.993716, Val Loss: 1.53346491, Val R²: 0.253044
Epoch 197/200
Train Loss: 0.01339750, Train R²: 0.993673, Val Loss: 1.51889110, Val R²: 0.260143
Epoch 198/200
Train Loss: 0.01324708, Train R²: 0.993744, Val Loss: 1.53292811, Val R²: 0.253306
Epoch 199/200
Train Loss: 0.01337467, Train R²: 0.993684, Val Loss: 1.53292906, Val R²: 0.253305
Epoch 200/200
Train Loss: 0.01427899, Train R²: 0.993257, Val Loss: 1.53248012, Val R²: 0.253524
Training Complete. Best Val Loss: 1.3662453889846802
训练时间: 196.90 秒
