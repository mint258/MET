Using device: cuda
Total samples: 5000, Training: 4000, Validation: 1000
Total trainable parameters: 4577537
Epoch 1/200
Train Loss: 4.17723019, Train R²: -0.848963, Val Loss: 2.39025730, Val R²: -0.066512
Saved best model with validation R2 -0.066512 to best_finetuned_model.pth
Epoch 2/200
Train Loss: 2.12433871, Train R²: 0.059706, Val Loss: 1.90404254, Val R²: 0.150433
Saved best model with validation R2 0.150433 to best_finetuned_model.pth
Epoch 3/200
Train Loss: 1.76579158, Train R²: 0.218410, Val Loss: 1.62825705, Val R²: 0.273486
Saved best model with validation R2 0.273486 to best_finetuned_model.pth
Epoch 4/200
Train Loss: 1.42895804, Train R²: 0.367502, Val Loss: 1.34725567, Val R²: 0.398866
Saved best model with validation R2 0.398866 to best_finetuned_model.pth
Epoch 5/200
Train Loss: 1.10540427, Train R²: 0.510716, Val Loss: 1.09648502, Val R²: 0.510758
Saved best model with validation R2 0.510758 to best_finetuned_model.pth
Epoch 6/200
Train Loss: 0.75111939, Train R²: 0.667533, Val Loss: 0.88424769, Val R²: 0.605456
Saved best model with validation R2 0.605456 to best_finetuned_model.pth
Epoch 7/200
Train Loss: 0.45297529, Train R²: 0.799500, Val Loss: 0.77381107, Val R²: 0.654732
Saved best model with validation R2 0.654732 to best_finetuned_model.pth
Epoch 8/200
Train Loss: 0.26998622, Train R²: 0.880496, Val Loss: 0.73767329, Val R²: 0.670857
Saved best model with validation R2 0.670857 to best_finetuned_model.pth
Epoch 9/200
Train Loss: 0.17678531, Train R²: 0.921750, Val Loss: 0.66873430, Val R²: 0.701617
Saved best model with validation R2 0.701617 to best_finetuned_model.pth
Epoch 10/200
Train Loss: 0.13279225, Train R²: 0.941222, Val Loss: 0.65313624, Val R²: 0.708576
Saved best model with validation R2 0.708576 to best_finetuned_model.pth
Epoch 11/200
Train Loss: 0.08756488, Train R²: 0.961241, Val Loss: 0.65806817, Val R²: 0.706376
Epoch 12/200
Train Loss: 0.05706780, Train R²: 0.974740, Val Loss: 0.63548642, Val R²: 0.716452
Saved best model with validation R2 0.716452 to best_finetuned_model.pth
Epoch 13/200
Train Loss: 0.05845514, Train R²: 0.974126, Val Loss: 0.67210291, Val R²: 0.700114
Epoch 14/200
Train Loss: 0.05969946, Train R²: 0.973575, Val Loss: 0.68429249, Val R²: 0.694675
Epoch 15/200
Train Loss: 0.10763403, Train R²: 0.952358, Val Loss: 0.68028467, Val R²: 0.696463
Epoch 16/200
Train Loss: 0.07513935, Train R²: 0.966741, Val Loss: 0.68235504, Val R²: 0.695539
Epoch 17/200
Train Loss: 0.05171568, Train R²: 0.977109, Val Loss: 0.67570162, Val R²: 0.698508
Epoch 18/200
Train Loss: 0.03384762, Train R²: 0.985018, Val Loss: 0.65521910, Val R²: 0.707647
Epoch 19/200
Train Loss: 0.02959056, Train R²: 0.986902, Val Loss: 0.64489202, Val R²: 0.712255
Epoch 20/200
Train Loss: 0.02475430, Train R²: 0.989043, Val Loss: 0.63290636, Val R²: 0.717603
Saved best model with validation R2 0.717603 to best_finetuned_model.pth
Epoch 21/200
Train Loss: 0.01723722, Train R²: 0.992370, Val Loss: 0.63559543, Val R²: 0.716403
Epoch 22/200
Train Loss: 0.01184577, Train R²: 0.994757, Val Loss: 0.63550007, Val R²: 0.716445
Epoch 23/200
Train Loss: 0.01098571, Train R²: 0.995137, Val Loss: 0.63651788, Val R²: 0.715991
Epoch 24/200
Train Loss: 0.01020080, Train R²: 0.995485, Val Loss: 0.63534016, Val R²: 0.716517
Epoch 25/200
Train Loss: 0.01090801, Train R²: 0.995172, Val Loss: 0.64058943, Val R²: 0.714175
Epoch 26/200
Train Loss: 0.01115488, Train R²: 0.995063, Val Loss: 0.63420468, Val R²: 0.717023
Epoch 27/200
Train Loss: 0.00902151, Train R²: 0.996007, Val Loss: 0.63879033, Val R²: 0.714977
Epoch 28/200
Train Loss: 0.00708308, Train R²: 0.996865, Val Loss: 0.63660584, Val R²: 0.715952
Epoch 29/200
Train Loss: 0.00613385, Train R²: 0.997285, Val Loss: 0.63942431, Val R²: 0.714694
Epoch 30/200
Train Loss: 0.02075513, Train R²: 0.990813, Val Loss: 0.64475034, Val R²: 0.712318
Epoch 31/200
Train Loss: 0.03215740, Train R²: 0.985766, Val Loss: 0.64817985, Val R²: 0.710788
Epoch 00031: reducing learning rate of group 0 to 5.0000e-05.
Epoch 32/200
Train Loss: 0.02111127, Train R²: 0.990656, Val Loss: 0.63242972, Val R²: 0.717815
Saved best model with validation R2 0.717815 to best_finetuned_model.pth
Epoch 33/200
Train Loss: 0.01099806, Train R²: 0.995132, Val Loss: 0.63041940, Val R²: 0.718712
Saved best model with validation R2 0.718712 to best_finetuned_model.pth
Epoch 34/200
Train Loss: 0.00713159, Train R²: 0.996843, Val Loss: 0.62925370, Val R²: 0.719232
Saved best model with validation R2 0.719232 to best_finetuned_model.pth
Epoch 35/200
Train Loss: 0.00582883, Train R²: 0.997420, Val Loss: 0.62897274, Val R²: 0.719358
Saved best model with validation R2 0.719358 to best_finetuned_model.pth
Epoch 36/200
Train Loss: 0.00417876, Train R²: 0.998150, Val Loss: 0.62978118, Val R²: 0.718997
Epoch 37/200
Train Loss: 0.00391257, Train R²: 0.998268, Val Loss: 0.62739921, Val R²: 0.720060
Saved best model with validation R2 0.720060 to best_finetuned_model.pth
Epoch 38/200
Train Loss: 0.00386909, Train R²: 0.998287, Val Loss: 0.62821010, Val R²: 0.719698
Epoch 39/200
Train Loss: 0.00364767, Train R²: 0.998385, Val Loss: 0.62803057, Val R²: 0.719778
Epoch 40/200
Train Loss: 0.00321844, Train R²: 0.998575, Val Loss: 0.62854516, Val R²: 0.719549
Epoch 41/200
Train Loss: 0.00279332, Train R²: 0.998764, Val Loss: 0.62954406, Val R²: 0.719103
Epoch 42/200
Train Loss: 0.00264046, Train R²: 0.998831, Val Loss: 0.62999429, Val R²: 0.718902
Epoch 43/200
Train Loss: 0.00236173, Train R²: 0.998955, Val Loss: 0.63063146, Val R²: 0.718618
Epoch 44/200
Train Loss: 0.00227244, Train R²: 0.998994, Val Loss: 0.63085770, Val R²: 0.718517
Epoch 45/200
Train Loss: 0.00208143, Train R²: 0.999079, Val Loss: 0.62981329, Val R²: 0.718983
Epoch 46/200
Train Loss: 0.00192986, Train R²: 0.999146, Val Loss: 0.63057582, Val R²: 0.718643
Epoch 47/200
Train Loss: 0.00177641, Train R²: 0.999214, Val Loss: 0.63114197, Val R²: 0.718390
Epoch 48/200
Train Loss: 0.00163659, Train R²: 0.999276, Val Loss: 0.63147326, Val R²: 0.718242
Epoch 00048: reducing learning rate of group 0 to 2.5000e-05.
Epoch 49/200
Train Loss: 0.00158591, Train R²: 0.999298, Val Loss: 0.63156657, Val R²: 0.718201
Epoch 50/200
Train Loss: 0.00161253, Train R²: 0.999286, Val Loss: 0.63100810, Val R²: 0.718450
Epoch 51/200
Train Loss: 0.00155988, Train R²: 0.999310, Val Loss: 0.63134530, Val R²: 0.718299
Epoch 52/200
Train Loss: 0.00150679, Train R²: 0.999333, Val Loss: 0.63224146, Val R²: 0.717899
Epoch 53/200
Train Loss: 0.00139048, Train R²: 0.999385, Val Loss: 0.63187713, Val R²: 0.718062
Epoch 54/200
Train Loss: 0.00141330, Train R²: 0.999374, Val Loss: 0.63229564, Val R²: 0.717875
Epoch 55/200
Train Loss: 0.00130079, Train R²: 0.999424, Val Loss: 0.63237364, Val R²: 0.717840
Epoch 56/200
Train Loss: 0.00131713, Train R²: 0.999417, Val Loss: 0.63290390, Val R²: 0.717604
Epoch 57/200
Train Loss: 0.00122814, Train R²: 0.999456, Val Loss: 0.63277003, Val R²: 0.717664
Epoch 58/200
Train Loss: 0.00117784, Train R²: 0.999479, Val Loss: 0.63286147, Val R²: 0.717623
Epoch 59/200
Train Loss: 0.00113336, Train R²: 0.999498, Val Loss: 0.63298071, Val R²: 0.717570
Epoch 00059: reducing learning rate of group 0 to 1.2500e-05.
Epoch 60/200
Train Loss: 0.00115177, Train R²: 0.999490, Val Loss: 0.63302071, Val R²: 0.717552
Epoch 61/200
Train Loss: 0.00113481, Train R²: 0.999498, Val Loss: 0.63333525, Val R²: 0.717411
Epoch 62/200
Train Loss: 0.00110055, Train R²: 0.999513, Val Loss: 0.63327414, Val R²: 0.717439
Epoch 63/200
Train Loss: 0.00106271, Train R²: 0.999530, Val Loss: 0.63353540, Val R²: 0.717322
Epoch 64/200
Train Loss: 0.00105033, Train R²: 0.999535, Val Loss: 0.63362349, Val R²: 0.717283
Epoch 65/200
Train Loss: 0.00108535, Train R²: 0.999520, Val Loss: 0.63323854, Val R²: 0.717454
Epoch 66/200
Train Loss: 0.00100755, Train R²: 0.999554, Val Loss: 0.63366241, Val R²: 0.717265
Epoch 67/200
Train Loss: 0.00099823, Train R²: 0.999558, Val Loss: 0.63367688, Val R²: 0.717259
Epoch 68/200
Train Loss: 0.00101532, Train R²: 0.999551, Val Loss: 0.63362004, Val R²: 0.717284
Epoch 69/200
Train Loss: 0.00104712, Train R²: 0.999537, Val Loss: 0.63361073, Val R²: 0.717288
Epoch 70/200
Train Loss: 0.00097091, Train R²: 0.999570, Val Loss: 0.63386366, Val R²: 0.717176
Epoch 00070: reducing learning rate of group 0 to 6.2500e-06.
Epoch 71/200
Train Loss: 0.00094543, Train R²: 0.999582, Val Loss: 0.63388830, Val R²: 0.717165
Epoch 72/200
Train Loss: 0.00093793, Train R²: 0.999585, Val Loss: 0.63390516, Val R²: 0.717157
Epoch 73/200
Train Loss: 0.00093792, Train R²: 0.999585, Val Loss: 0.63399335, Val R²: 0.717118
Epoch 74/200
Train Loss: 0.00093901, Train R²: 0.999584, Val Loss: 0.63381493, Val R²: 0.717197
Epoch 75/200
Train Loss: 0.00094258, Train R²: 0.999583, Val Loss: 0.63405938, Val R²: 0.717088
Epoch 76/200
Train Loss: 0.00090647, Train R²: 0.999599, Val Loss: 0.63413327, Val R²: 0.717055
Epoch 77/200
Train Loss: 0.00087839, Train R²: 0.999611, Val Loss: 0.63401915, Val R²: 0.717106
Epoch 78/200
Train Loss: 0.00088187, Train R²: 0.999610, Val Loss: 0.63424732, Val R²: 0.717004
Epoch 79/200
Train Loss: 0.00085341, Train R²: 0.999622, Val Loss: 0.63389750, Val R²: 0.717160
Epoch 80/200
Train Loss: 0.00086567, Train R²: 0.999617, Val Loss: 0.63389526, Val R²: 0.717161
Epoch 81/200
Train Loss: 0.00086734, Train R²: 0.999616, Val Loss: 0.63443520, Val R²: 0.716921
Epoch 00081: reducing learning rate of group 0 to 3.1250e-06.
Epoch 82/200
Train Loss: 0.00090589, Train R²: 0.999599, Val Loss: 0.63386812, Val R²: 0.717174
Epoch 83/200
Train Loss: 0.00082183, Train R²: 0.999636, Val Loss: 0.63387583, Val R²: 0.717170
Epoch 84/200
Train Loss: 0.00083245, Train R²: 0.999632, Val Loss: 0.63439891, Val R²: 0.716937
Epoch 85/200
Train Loss: 0.00085695, Train R²: 0.999621, Val Loss: 0.63441808, Val R²: 0.716928
Epoch 86/200
Train Loss: 0.00086528, Train R²: 0.999617, Val Loss: 0.63451584, Val R²: 0.716885
Epoch 87/200
Train Loss: 0.00087111, Train R²: 0.999614, Val Loss: 0.63404212, Val R²: 0.717096
Epoch 88/200
Train Loss: 0.00089198, Train R²: 0.999605, Val Loss: 0.63454643, Val R²: 0.716871
Epoch 89/200
Train Loss: 0.00081753, Train R²: 0.999638, Val Loss: 0.63454318, Val R²: 0.716872
Epoch 90/200
Train Loss: 0.00081262, Train R²: 0.999640, Val Loss: 0.63411290, Val R²: 0.717064
Epoch 91/200
Train Loss: 0.00081675, Train R²: 0.999638, Val Loss: 0.63466423, Val R²: 0.716818
Epoch 92/200
Train Loss: 0.00080430, Train R²: 0.999644, Val Loss: 0.63461956, Val R²: 0.716838
Epoch 00092: reducing learning rate of group 0 to 1.5625e-06.
Epoch 93/200
Train Loss: 0.00078052, Train R²: 0.999655, Val Loss: 0.63460378, Val R²: 0.716845
Epoch 94/200
Train Loss: 0.00083258, Train R²: 0.999631, Val Loss: 0.63467815, Val R²: 0.716812
Epoch 95/200
Train Loss: 0.00079186, Train R²: 0.999650, Val Loss: 0.63469164, Val R²: 0.716806
Epoch 96/200
Train Loss: 0.00077716, Train R²: 0.999656, Val Loss: 0.63467140, Val R²: 0.716815
Epoch 97/200
Train Loss: 0.00078962, Train R²: 0.999650, Val Loss: 0.63420052, Val R²: 0.717025
Epoch 98/200
Train Loss: 0.00079147, Train R²: 0.999650, Val Loss: 0.63473507, Val R²: 0.716787
Epoch 99/200
Train Loss: 0.00081050, Train R²: 0.999641, Val Loss: 0.63416533, Val R²: 0.717041
Epoch 100/200
Train Loss: 0.00081258, Train R²: 0.999640, Val Loss: 0.63402451, Val R²: 0.717104
Epoch 101/200
Train Loss: 0.00081190, Train R²: 0.999641, Val Loss: 0.63424581, Val R²: 0.717005
Epoch 102/200
Train Loss: 0.00080161, Train R²: 0.999645, Val Loss: 0.63403797, Val R²: 0.717098
Epoch 103/200
Train Loss: 0.00077902, Train R²: 0.999655, Val Loss: 0.63418675, Val R²: 0.717031
Epoch 00103: reducing learning rate of group 0 to 7.8125e-07.
Epoch 104/200
Train Loss: 0.00078367, Train R²: 0.999653, Val Loss: 0.63475621, Val R²: 0.716777
Epoch 105/200
Train Loss: 0.00076394, Train R²: 0.999662, Val Loss: 0.63473731, Val R²: 0.716786
Epoch 106/200
Train Loss: 0.00081391, Train R²: 0.999640, Val Loss: 0.63476916, Val R²: 0.716772
Epoch 107/200
Train Loss: 0.00079987, Train R²: 0.999646, Val Loss: 0.63429403, Val R²: 0.716984
Epoch 108/200
Train Loss: 0.00075314, Train R²: 0.999667, Val Loss: 0.63420704, Val R²: 0.717022
Epoch 109/200
Train Loss: 0.00076254, Train R²: 0.999662, Val Loss: 0.63482662, Val R²: 0.716746
Epoch 110/200
Train Loss: 0.00079897, Train R²: 0.999646, Val Loss: 0.63479252, Val R²: 0.716761
Epoch 111/200
Train Loss: 0.00076106, Train R²: 0.999663, Val Loss: 0.63480698, Val R²: 0.716755
Epoch 112/200
Train Loss: 0.00080967, Train R²: 0.999642, Val Loss: 0.63430126, Val R²: 0.716980
Epoch 113/200
Train Loss: 0.00078514, Train R²: 0.999652, Val Loss: 0.63484085, Val R²: 0.716740
Epoch 114/200
Train Loss: 0.00076564, Train R²: 0.999661, Val Loss: 0.63487722, Val R²: 0.716723
Epoch 00114: reducing learning rate of group 0 to 3.9063e-07.
Epoch 115/200
Train Loss: 0.00073442, Train R²: 0.999675, Val Loss: 0.63485834, Val R²: 0.716732
Epoch 116/200
Train Loss: 0.00074461, Train R²: 0.999670, Val Loss: 0.63488032, Val R²: 0.716722
Epoch 117/200
Train Loss: 0.00076837, Train R²: 0.999660, Val Loss: 0.63486588, Val R²: 0.716728
Epoch 118/200
Train Loss: 0.00076993, Train R²: 0.999659, Val Loss: 0.63416498, Val R²: 0.717041
Epoch 119/200
Train Loss: 0.00076562, Train R²: 0.999661, Val Loss: 0.63487072, Val R²: 0.716726
Epoch 120/200
Train Loss: 0.00075252, Train R²: 0.999667, Val Loss: 0.63487318, Val R²: 0.716725
Epoch 121/200
Train Loss: 0.00073741, Train R²: 0.999674, Val Loss: 0.63436283, Val R²: 0.716953
Epoch 122/200
Train Loss: 0.00075659, Train R²: 0.999665, Val Loss: 0.63486643, Val R²: 0.716728
Epoch 123/200
Train Loss: 0.00075264, Train R²: 0.999667, Val Loss: 0.63483349, Val R²: 0.716743
Epoch 124/200
Train Loss: 0.00074002, Train R²: 0.999672, Val Loss: 0.63433977, Val R²: 0.716963
Epoch 125/200
Train Loss: 0.00073590, Train R²: 0.999674, Val Loss: 0.63490737, Val R²: 0.716710
Epoch 00125: reducing learning rate of group 0 to 1.9531e-07.
Epoch 126/200
Train Loss: 0.00078763, Train R²: 0.999651, Val Loss: 0.63437285, Val R²: 0.716948
Epoch 127/200
Train Loss: 0.00075050, Train R²: 0.999668, Val Loss: 0.63488990, Val R²: 0.716718
Epoch 128/200
Train Loss: 0.00073557, Train R²: 0.999674, Val Loss: 0.63484218, Val R²: 0.716739
Epoch 129/200
Train Loss: 0.00074226, Train R²: 0.999671, Val Loss: 0.63490363, Val R²: 0.716712
Epoch 130/200
Train Loss: 0.00076619, Train R²: 0.999661, Val Loss: 0.63439447, Val R²: 0.716939
Epoch 131/200
Train Loss: 0.00075334, Train R²: 0.999667, Val Loss: 0.63489140, Val R²: 0.716717
Epoch 132/200
Train Loss: 0.00077091, Train R²: 0.999659, Val Loss: 0.63490119, Val R²: 0.716713
Epoch 133/200
Train Loss: 0.00073141, Train R²: 0.999676, Val Loss: 0.63490001, Val R²: 0.716713
Epoch 134/200
Train Loss: 0.00079614, Train R²: 0.999648, Val Loss: 0.63432048, Val R²: 0.716972
Epoch 135/200
Train Loss: 0.00073203, Train R²: 0.999676, Val Loss: 0.63440949, Val R²: 0.716932
Epoch 136/200
Train Loss: 0.00073810, Train R²: 0.999673, Val Loss: 0.63433021, Val R²: 0.716967
Epoch 00136: reducing learning rate of group 0 to 9.7656e-08.
Epoch 137/200
Train Loss: 0.00077741, Train R²: 0.999656, Val Loss: 0.63489521, Val R²: 0.716715
Epoch 138/200
Train Loss: 0.00076872, Train R²: 0.999660, Val Loss: 0.63415587, Val R²: 0.717045
Epoch 139/200
Train Loss: 0.00073695, Train R²: 0.999674, Val Loss: 0.63482707, Val R²: 0.716746
Epoch 140/200
Train Loss: 0.00073492, Train R²: 0.999675, Val Loss: 0.63440669, Val R²: 0.716933
Epoch 141/200
Train Loss: 0.00075276, Train R²: 0.999667, Val Loss: 0.63490037, Val R²: 0.716713
Epoch 142/200
Train Loss: 0.00076638, Train R²: 0.999661, Val Loss: 0.63491964, Val R²: 0.716704
Epoch 143/200
Train Loss: 0.00071366, Train R²: 0.999684, Val Loss: 0.63492167, Val R²: 0.716704
Epoch 144/200
Train Loss: 0.00074229, Train R²: 0.999671, Val Loss: 0.63496931, Val R²: 0.716682
Epoch 145/200
Train Loss: 0.00075823, Train R²: 0.999664, Val Loss: 0.63491279, Val R²: 0.716707
Epoch 146/200
Train Loss: 0.00075892, Train R²: 0.999664, Val Loss: 0.63416278, Val R²: 0.717042
Epoch 147/200
Train Loss: 0.00076005, Train R²: 0.999664, Val Loss: 0.63491993, Val R²: 0.716704
Epoch 00147: reducing learning rate of group 0 to 4.8828e-08.
Epoch 148/200
Train Loss: 0.00072879, Train R²: 0.999677, Val Loss: 0.63493908, Val R²: 0.716696
Epoch 149/200
Train Loss: 0.00075712, Train R²: 0.999665, Val Loss: 0.63440495, Val R²: 0.716934
Epoch 150/200
Train Loss: 0.00076066, Train R²: 0.999663, Val Loss: 0.63433930, Val R²: 0.716963
Epoch 151/200
Train Loss: 0.00077747, Train R²: 0.999656, Val Loss: 0.63492437, Val R²: 0.716702
Epoch 152/200
Train Loss: 0.00076313, Train R²: 0.999662, Val Loss: 0.63476268, Val R²: 0.716774
Epoch 153/200
Train Loss: 0.00075311, Train R²: 0.999667, Val Loss: 0.63493724, Val R²: 0.716697
Epoch 154/200
Train Loss: 0.00080006, Train R²: 0.999646, Val Loss: 0.63492866, Val R²: 0.716700
Epoch 155/200
Train Loss: 0.00079369, Train R²: 0.999649, Val Loss: 0.63490199, Val R²: 0.716712
Epoch 156/200
Train Loss: 0.00077976, Train R²: 0.999655, Val Loss: 0.63488371, Val R²: 0.716720
Epoch 157/200
Train Loss: 0.00080813, Train R²: 0.999642, Val Loss: 0.63433907, Val R²: 0.716963
Epoch 158/200
Train Loss: 0.00077427, Train R²: 0.999657, Val Loss: 0.63485875, Val R²: 0.716732
Epoch 00158: reducing learning rate of group 0 to 2.4414e-08.
Epoch 159/200
Train Loss: 0.00074573, Train R²: 0.999670, Val Loss: 0.63492651, Val R²: 0.716701
Epoch 160/200
Train Loss: 0.00074481, Train R²: 0.999670, Val Loss: 0.63491715, Val R²: 0.716706
Epoch 161/200
Train Loss: 0.00079018, Train R²: 0.999650, Val Loss: 0.63440822, Val R²: 0.716933
Epoch 162/200
Train Loss: 0.00074347, Train R²: 0.999671, Val Loss: 0.63491152, Val R²: 0.716708
Epoch 163/200
Train Loss: 0.00076108, Train R²: 0.999663, Val Loss: 0.63488462, Val R²: 0.716720
Epoch 164/200
Train Loss: 0.00078413, Train R²: 0.999653, Val Loss: 0.63493488, Val R²: 0.716698
Epoch 165/200
Train Loss: 0.00073854, Train R²: 0.999673, Val Loss: 0.63485381, Val R²: 0.716734
Epoch 166/200
Train Loss: 0.00074789, Train R²: 0.999669, Val Loss: 0.63488133, Val R²: 0.716722
Epoch 167/200
Train Loss: 0.00074316, Train R²: 0.999671, Val Loss: 0.63488244, Val R²: 0.716721
Epoch 168/200
Train Loss: 0.00073777, Train R²: 0.999673, Val Loss: 0.63492425, Val R²: 0.716702
Epoch 169/200
Train Loss: 0.00074118, Train R²: 0.999672, Val Loss: 0.63439918, Val R²: 0.716937
Epoch 00169: reducing learning rate of group 0 to 1.2207e-08.
Epoch 170/200
Train Loss: 0.00075673, Train R²: 0.999665, Val Loss: 0.63491351, Val R²: 0.716707
Epoch 171/200
Train Loss: 0.00074290, Train R²: 0.999671, Val Loss: 0.63492381, Val R²: 0.716703
Epoch 172/200
Train Loss: 0.00082595, Train R²: 0.999634, Val Loss: 0.63488095, Val R²: 0.716722
Epoch 173/200
Train Loss: 0.00074514, Train R²: 0.999670, Val Loss: 0.63441801, Val R²: 0.716928
Epoch 174/200
Train Loss: 0.00077325, Train R²: 0.999658, Val Loss: 0.63472520, Val R²: 0.716791
Epoch 175/200
Train Loss: 0.00078028, Train R²: 0.999655, Val Loss: 0.63442188, Val R²: 0.716927
Epoch 176/200
Train Loss: 0.00076025, Train R²: 0.999663, Val Loss: 0.63414310, Val R²: 0.717051
Epoch 177/200
Train Loss: 0.00075621, Train R²: 0.999665, Val Loss: 0.63488083, Val R²: 0.716722
Epoch 178/200
Train Loss: 0.00075809, Train R²: 0.999664, Val Loss: 0.63489512, Val R²: 0.716715
Epoch 179/200
Train Loss: 0.00072201, Train R²: 0.999680, Val Loss: 0.63492720, Val R²: 0.716701
Epoch 180/200
Train Loss: 0.00077343, Train R²: 0.999658, Val Loss: 0.63468632, Val R²: 0.716808
Epoch 181/200
Train Loss: 0.00072166, Train R²: 0.999681, Val Loss: 0.63491249, Val R²: 0.716708
Epoch 182/200
Train Loss: 0.00074634, Train R²: 0.999670, Val Loss: 0.63480338, Val R²: 0.716756
Epoch 183/200
Train Loss: 0.00071558, Train R²: 0.999683, Val Loss: 0.63440585, Val R²: 0.716934
Epoch 184/200
Train Loss: 0.00074620, Train R²: 0.999670, Val Loss: 0.63421945, Val R²: 0.717017
Epoch 185/200
Train Loss: 0.00078931, Train R²: 0.999651, Val Loss: 0.63440411, Val R²: 0.716934
Epoch 186/200
Train Loss: 0.00078212, Train R²: 0.999654, Val Loss: 0.63492742, Val R²: 0.716701
Epoch 187/200
Train Loss: 0.00073526, Train R²: 0.999675, Val Loss: 0.63493178, Val R²: 0.716699
Epoch 188/200
Train Loss: 0.00071543, Train R²: 0.999683, Val Loss: 0.63472992, Val R²: 0.716789
Epoch 189/200
Train Loss: 0.00075463, Train R²: 0.999666, Val Loss: 0.63440139, Val R²: 0.716936
Epoch 190/200
Train Loss: 0.00074441, Train R²: 0.999671, Val Loss: 0.63486211, Val R²: 0.716730
Epoch 191/200
Train Loss: 0.00080998, Train R²: 0.999641, Val Loss: 0.63483565, Val R²: 0.716742
Epoch 192/200
Train Loss: 0.00073620, Train R²: 0.999674, Val Loss: 0.63488195, Val R²: 0.716721
Epoch 193/200
Train Loss: 0.00075256, Train R²: 0.999667, Val Loss: 0.63492297, Val R²: 0.716703
Epoch 194/200
Train Loss: 0.00075263, Train R²: 0.999667, Val Loss: 0.63491657, Val R²: 0.716706
Epoch 195/200
Train Loss: 0.00073443, Train R²: 0.999675, Val Loss: 0.63500943, Val R²: 0.716664
Epoch 196/200
Train Loss: 0.00071732, Train R²: 0.999682, Val Loss: 0.63466431, Val R²: 0.716818
Epoch 197/200
Train Loss: 0.00075281, Train R²: 0.999667, Val Loss: 0.63492284, Val R²: 0.716703
Epoch 198/200
Train Loss: 0.00072765, Train R²: 0.999678, Val Loss: 0.63485893, Val R²: 0.716732
Epoch 199/200
Train Loss: 0.00076841, Train R²: 0.999660, Val Loss: 0.63465700, Val R²: 0.716822
Epoch 200/200
Train Loss: 0.00074602, Train R²: 0.999670, Val Loss: 0.63488187, Val R²: 0.716721
Training Complete. Best Val Loss: 0.6273992128372192
训练时间: 777.15 秒
